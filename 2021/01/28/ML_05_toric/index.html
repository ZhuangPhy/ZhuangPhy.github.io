<!DOCTYPE html>
<html lang="en">





<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="JPZhuang">
  <meta name="keywords" content="">
  <title>强化学习对toric code量子纠错 - JPZ</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
<meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Physics</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner intro-2" id="background" parallax=true
         style="background: url('/img/tag-bg.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fade-in-up">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2021-01-28 13:23">
      January 28, 2021 pm
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      2.8k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      55
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <p>[TOC]</p>
<h2 id="section"></h2>
<h2 id="参考文献">参考文献</h2>
<ol type="1">
<li>《<a href="https://arxiv.org/pdf/1811.12338v3.pdf" target="_blank" rel="noopener">Quantum error correction for the toric code using deep reinforcement learning</a>》</li>
</ol>
<p>We implement a quantum error correction algorithm for bit-flip errors on the topological toric code using deep reinforcement learning. An action-value Q-function encodes the discounted value of moving a defect to a neighboring site on the square grid (the action) depending on the full set of defects on the torus (the syndrome or state). The Q-function is represented by a deep convolutional neural network. Using the translational invariance on the torus allows for viewing each defect from a central perspective which significantly simplifies the state space representation independently of the number of defect pairs. The training is done using experience replay, where data from the algorithm being played out is stored and used for mini-batch upgrade of the Q-network. We find performance which is close to, and for small error rates asymptotically equivalent to, that achieved by the Minimum Weight Perfect Matching algorithm for code distances up to <span class="math inline">\(d=7 .\)</span> Our results show that it is possible for a self-trained agent without supervision or support algorithms to find a decoding scheme that performs on par with hand-made algorithms, opening up for future machine engineered decoders for more general error models and error correcting codes.</p>
<h2 id="介绍">介绍</h2>
<p>Much of the spectacular advances in machine learning using artificial neural networks has been in the domain of supervised learning were deep convolutional networks excel at categorizing objects when trained with big annotated data sets <span class="math inline">\([1-3] .\)</span> A different but also more challenging type of problem is when there is no a priori solution key, but rather a dynamic environment through which we want to learn to navigate for an optimal outcome. For these types of problems reinforcement learning (RL) [4] combined with deep learning has had great success recently when applied to problems such as computer and board games <span class="math inline">\([5-8]\)</span>. The super-human performance achieved by deep reinforcement learning has revolutionized the field of artificial intelligence and opens up for applications in many areas of science and technology.</p>
<p>In physics the use of machine learning has seen a great deal of interest lately <span class="math inline">\([9-13] .\)</span> The most natural type of application of neural networks is in the form of supervised learning where the deep network can capture correlations or subtle information in real or artificial data. The use of deep reinforcement learning may be less obvious in general as the type of topics addressed by RL typically involve some sort of "intelligent" best strategy search, contrary to the deterministic or statistical models used in physics.</p>
<p>In this paper we study a type of problem where artificial intelligence is applicable, namely the task of finding a best strategy for error correction of a topological quantum code; the potential basic building blocks of a quantum computer. In the field of quantum computing, smart algorithms are needed for error correction of fragile quantum bits <span class="math inline">\([14-17] .\)</span> Reinforcement learning has been suggested recently as a tool for quantum error correction and quantum control <span class="math inline">\([18-20]\)</span>, where an agent learns to manipulate a quantum device that functions in an imperfect environment and with incomplete information. Under the umbrella term "Quantum Machine Learning" there are also interesting prospects of utilizing the natural parallelization of quantum computers for machine learning itself [21] , but we will be dealing here with the task of putting (classical) deep learning and AI at the service of quantum computing.</p>
<p>Due to the inherently fragile nature of quantum information a future universal quantum computer will require quantum error correction.[14-17] Perhaps the most promising framework is to use topological error correcting codes. [22-26] Here, logical qubits consisting of a large set of entangled physical qubits are protected against local disturbances from phase or bit flip errors as logical operations require global changes. Local stabilizer operators, in the form of parity checks on a group of physical qubits, provide a quantum non-demolition diagnosis of the logical qubit in terms of violated stabilizers; the so-called syndrome. In order for errors not to proliferate and cause logical failure, a decoder, that provides a set of recovery operations for correction of errors given a particular syndrome, is required. As the syndrome does not uniquely determine the physical errors, the decoder has to incorporate the statistics of errors corresponding to any given syndrome. In addition the syndrome itself may be imperfect, due to stabilizer measurement errors, in which case the decoder must also take that into account.</p>
<p>In the present work we consider Kitaev's toric <span class="math inline">\(\operatorname{code}[22,23,25]\)</span> which is a stabilizer code formulated on a square lattice with periodic boundary conditions (see Figure 1 and Section 2.1 ). We will only consider bit-flip errors which correspond to syndromes with one type of violated stabilizer that can be represented as plaquette defects on the lattice (see Figure 2). The standard decoder for the toric code is the Minimum Weight Perfect Matching (MWPM) or Blossom algorithm <span class="math inline">\([27-29]\)</span> that works by finding the pairwise matching of syndrome defects with shortest total distance, corresponding to the minimal number of errors consistent with the syndrome. The decoder problem is also conceptually well suited for reinforcement learning, similar in spirit to a board game; the state of the system is given by the syndrome, actions correspond to moving defects of the syndrome, and with reward given depending on the outcome of the game. By playing the game, the agent improves its error correcting strategies and the decoder is the trained agent that provides step by step error correction. As in any RL problem the reward scheme is crucial for good performance. The size of the state-action space is also a challenge, to provide the best action for each of a myriad syndromes, but this is exactly the problem addressed by recent deep learning approaches to RL. <span class="math inline">\([5-8]\)</span></p>
<p>We find that by setting up a reward scheme that encourages the elimination of the syndrome in as few operations as possible within the deep Q-learning (or deep Q-network, DQN) [6,7] formalism we are able to train a decoder that is comparable in performance to MWPM. Although the present algorithm does not outperform the latter we expect that it has the potential to be more versatile when addressing depolarizing noise (with correlated bit and phase flip errors), measurement noise giving imperfect syndromes, or varying code geometries. Compared to the MWPM algorithm the RL algorithm also has the possible advantage that it provides step by step correction whereas the MWPM algorithm only provides information on which defects should be paired, making the former more adaptable to the introduction of additional errors.</p>
<p>In concurrent work by Sweke et al. [30] an application of reinforcement learning to error correction of the toric code was implemented. That work focuses on the important issue of imperfect syndromes as well as depolarizing noise and used an auxiliary "referee decoder" to assist the RL decoder. In the present work we consider the simpler but conceptually more direct problem of error correction on a perfect syndrome and with only bit flip error. Also in contrast to [30] we study the actual "toric" code, rather than the code with boundaries. Clearly the toric code will be harder to implement experimentally but nevertheless provides a well understood standard model. It also provides a simplification from the fact that on a torus only the relative positions of syndrome defects are relevant which reduces the state space complexity that decoder agent has to master. By focusing on this minimal problem we find that we can make a rigorous benchmark on the RL decoder showing near optimal performance.</p>
<p>Finding better performing decoders has been the topic of many studies, using methods such as renormalization group <span class="math inline">\([31,32],\)</span> cellular automata <span class="math inline">\([33,34],\)</span> and a number of neural network based decoders <span class="math inline">\([19,35-43]\)</span>. The decoder presented in this paper does not outperform state of the art decoders, it's value lies in showing that it is possible to use reinforcement learning to achieve excellent performance on a minimal model. Given that deep reinforcement learning is arguably the most promising AI framework it holds prospect for future versatile self-trained decoders that can adapt to different error scenarios and code architectures.</p>
<p>The outline of the paper is the following. In the Background section we give a brief but self-contained summary of the main features of the toric code including the basic structure of the error correction and a similar summary of one-step Q-learning and deep Q-learning. The following section, <span class="math inline">\(R L\)</span> Algorithm, describes the formulation and training of the error correcting agent. In the Results section we shows that we have trained the RL agent up to code distance <span class="math inline">\(d=7\)</span> with performance which is very close to the MWPM algorithm. We finally conclude and append details of the asymptotic fail rate for small error rates as well as the neural network ar-chitecture and the RL and network hyperparameters.</p>
<h2 id="背景1toric-code">背景1：Toric code</h2>
<p>The basic construction of the toric code is a square lattice with a spin- <span class="math inline">\(\frac{1}{2}\)</span> degree of freedom on every bond, the physical qubits, and with periodic boundary conditions, as seen in Figure <span class="math inline">\(1 .[22,23]^{1}\)</span> (An alternative rotated lattice representation with the qubits on sites is also com-mon in the literature.) The model is given in terms of a Hamiltonian <span class="math display">\[
H=-\sum_{\alpha} \hat{P}_{\alpha}-\sum_{\nu} \hat{V}_{\nu}
\]</span> where <span class="math inline">\(\alpha\)</span> runs over all plaquettes and <span class="math inline">\(\nu\)</span> over all vertices (sites). The stabilizers are the plaquette operators <span class="math inline">\(\hat{P}_{\alpha}=\prod_{i \in \alpha} \sigma_{i}^{z}\)</span> and the vertex operators <span class="math inline">\(\hat{V}_{\nu}=\prod_{i \in \nu} \sigma_{i}^{x}\)</span> where <span class="math inline">\(\sigma^{z}\)</span> and <span class="math inline">\(\sigma^{x}\)</span> are the Pauli matrices. (Where, in the <span class="math inline">\(\sigma^{z}\)</span> basis, <span class="math inline">\(\sigma^{z}|\uparrow / \downarrow\rangle=\pm 1|\uparrow / \downarrow\rangle\)</span> and <span class="math inline">\(\sigma^{x} \mid \uparrow / \downarrow\)</span> <span class="math inline">\(=|\downarrow / \uparrow\rangle .)\)</span> The stabilizers commute with each other and the Hamiltonian thus block diagonalizing the latter. On a <span class="math inline">\(d \times d\)</span> lattice of plaquettes <span class="math inline">\(d^{2}-1\)</span> plaquette operators are linearly independent (e.g. it is not possible to have a single -1 eigenvalue with all other +1<span class="math inline">\()\)</span> and correspondingly for the vertex operators. With <span class="math inline">\(2 d^{2}\)</span> physical qubits and <span class="math inline">\(2 d^{2}-2\)</span> stabilizers the size of each block is <span class="math inline">\(2^{2 d^{2}} / 2^{2 d^{2}-2}=4,\)</span> corresponding in particular to a ground state which is 4 -fold degenerate. These are the states that will serve as the logical qubits. (More precisely, given the 4 -fold degeneracy it is a qudit or base- 4 qubit.)</p>
<p>To derive the ground state consider first the plaquette operator in the <span class="math inline">\(\sigma^{z}\)</span> -basis; clearly a ground state must have an even number of each spin-up and spin-down on every plaquette to be a +1 eigenstate of each plaque-tte operator. Let's consider the state with all spin-up <span class="math inline">\(|\uparrow \uparrow \uparrow \cdots\rangle\)</span>; acting with a vertex operator on this flips all the spins around the vertex (see Fig. 1b) giving a state still in ground state sector of the plaquette operators as an even number of spins are flipped on the plaquettes surrounding the vertex. (As is also clear from the fact that all the stabilizer operators commute.) The +1 eigenstate of that particular vertex operator is thus the symmetric superposition of the two states. A convenient way to express the operation of one or several adjacent vertex operators is in turns of loop traversing the flipped spins. Such loops (fig. 1b-c) generated from products of vertex operators will always be topologically trivial loops on the surface of the torus since they are just constructed by merging the local loop corresponding to a single vertex operator. Successively acting with vertex operators on the states generated from the original <span class="math inline">\(|\uparrow \uparrow \uparrow \cdots\rangle\)</span> we realize that the ground state is simply the symmetric superposition of all states that are generated from this by acting with (trivial) loops <span class="math display">\[
\left| GS _{0}\right\rangle=\sum_{i \in \text { all trivial loops }} \operatorname{loop}_{i}|\uparrow \uparrow \uparrow \cdots\rangle
\]</span> To generate the other ground states we consider the operators <span class="math inline">\(\bar{X}_{1}\)</span> and <span class="math inline">\(\bar{X}_{2}\)</span> (Fig. 1d) which are products of <span class="math inline">\(\sigma^{x}\)</span> corresponding to the two non-trivial loops that wind the torus. (Deformations of these loops just correspond to multiplication by trivial loops and is thus inconsequential.) Correspondingly there are non-trivial loops of <span class="math inline">\(\sigma^{z}\)</span> operators <span class="math inline">\(\bar{Z}_{1}\)</span> and <span class="math inline">\(\bar{Z}_{2} .\)</span> The four ground states are thus the topologically distinct states <span class="math display">\[
\left\{\left| GS _{0}\right\rangle, \bar{X}_{1}\left| GS _{0}\right\rangle, \bar{X}_{2}\left| GS _{0}\right\rangle, \bar{X}_{2} \bar{X}_{1}\left| GS _{0}\right\rangle\right\} \quad
\]</span> distinguished by their eigenvalues of <span class="math inline">\(\bar{Z}_{1}\)</span> and <span class="math inline">\(\bar{Z}_{2}\)</span> being ±1 . For a torus with <span class="math inline">\(d \times d\)</span> plaquettes there are <span class="math inline">\(2 d^{2}\)</span> physical qubits and the code distance, i.e. minimum length of any logical operator <span class="math inline">\(\left(\bar{X}_{i}\right.\)</span> or <span class="math inline">\(\left.\bar{Z}_{i}\right),\)</span> is <span class="math inline">\(d\)</span>.</p>
<h3 id="纠错">纠错</h3>
<h2 id="背景2q-learning">背景2：Q-learning</h2>
<p>Reinforcement learning is a method to solve the problem of finding an optimal policy of an agent acting in a system where the actions of the agent causes transitions between states of the system. [4] The policy <span class="math inline">\(\pi(s, a)\)</span> of an agent describes (probabilistically perhaps) the action <span class="math inline">\(a\)</span> to be taken by the agent when the system is in state s. In our case the state will correspond to a syndrome, and an action to moving a defect one step. The optimal policy is the one that gives the agent maximal return (cumulative discounted reward) over the course of its interaction with the system. Reward <span class="math inline">\(r_{t+1}\)</span> is given when the system transitions from state <span class="math inline">\(s_{t} \rightarrow s_{t+1}\)</span> such that the return starting at time <span class="math inline">\(t\)</span> is given by <span class="math inline">\(R_{t}=r_{t+1}+\)</span> <span class="math inline">\(\gamma r_{t+2}+\gamma^{2} r_{t+3}+\cdots\)</span>. Here <span class="math inline">\(\gamma \leq 1\)</span> is the discounting factor that quantifies how we want to value immediate versus subsequent reward. As will be discussed in more detail, in the work presented in this paper a constant reward <span class="math inline">\(r=-1\)</span> will be given for each step taken, so that in practice the optimal policy will be the one that minimizes the number of actions, irrespectively of the value of <span class="math inline">\(\gamma\)</span>. (Although in practice, even here the value of <span class="math inline">\(\gamma\)</span> can be important for the convergence of the training.)</p>
<p>One way to represent the cumulative reward depending on a set of actions and corresponding transitions is by means of an action-value function, or Q-function. This function <span class="math inline">\(Q(s, a)\)</span> quantifies the expected return when in state <span class="math inline">\(s\)</span> taking the action <span class="math inline">\(a\)</span>, and subsequently following some policy <span class="math inline">\(\pi\)</span>. In one-step Q-learning we quantify <span class="math inline">\(Q\)</span> according to <span class="math inline">\(Q(s, a)=r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)\)</span> with <span class="math inline">\(s \stackrel{a}{\rightarrow} s^{\prime}\)</span>, which corresponds to following the optimal policy according to our current estimate of <span class="math inline">\(Q .\)</span> In order to learn the value of the Q-function for all states and actions we should explore the full state-action space, with the policy given by taken action <span class="math inline">\(a\)</span> according to <span class="math inline">\(\max _{a} Q(s, a)\)</span> eventually guaranteed to converge to the optimal policy. However, an unbiased exploration gets prohibitively expensive and it is therefore in general efficient to follow an <span class="math inline">\(\epsilon\)</span> -greedy policy which with probability <span class="math inline">\((1-\epsilon)\)</span> takes the optimal action based on our current estimate of <span class="math inline">\(Q(s, a)\)</span> but with probability <span class="math inline">\(\epsilon\)</span> takes a random action. From what we have learned by this action we would update our estimate for <span class="math inline">\(Q\)</span> according to <span class="math display">\[
Q(s, a) \leftarrow Q(s, a)+\alpha\left[\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)\right)-Q(s, a)\right]
\]</span> where <span class="math inline">\(\alpha&lt;1\)</span> is a learning rate. This procedure is then a trade-off between using our current knowledge of the <span class="math inline">\(Q\)</span> function as a guide for the best move to avoid spending extensive time on expensive moves but also exploring to avoid missing out on rewarding parts of the state-action space.</p>
<h3 id="深度q-learning">深度Q-learning</h3>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Machine-learning/">Machine learning</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Machine-learning/">Machine learning</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2021/01/29/topology02_review02_CN/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">凝聚体中的拓扑量子态</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2021/01/28/CMP_17_TN02/">
                        <span class="hidden-mobile">张量网络态与几何</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div id="tocbot"></div>
</div>


      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>







  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 1,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "强化学习对toric code量子纠错&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js" ></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  
















</body>
</html>
