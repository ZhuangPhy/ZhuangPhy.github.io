<!DOCTYPE html>
<html lang="en">





<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="JPZhuang">
  <meta name="keywords" content="">
  <title>矩阵乘积态与机器学习 - JPZ</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
<meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Physics</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner intro-2" id="background" parallax=true
         style="background: url('/img/tag-bg.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fade-in-up">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2021-01-25 16:23">
      January 25, 2021 pm
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      4.4k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      56
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <p>[TOC]</p>
<h2 id="参考文献">参考文献</h2>
<ol type="1">
<li>《基于张量网络的机器学习模型》第四章</li>
<li>张潘写的导览 <a href="https://github.com/QuantumBFS/SSSS/blob/master/2_tensor_network/mps_tutorial.ipynb" target="_blank" rel="noopener">A simple implementation of the MPS Born Machine</a></li>
</ol>
<h2 id="概览">1 概览</h2>
<h4 id="卷积神经网络">卷积神经网络</h4>
<p>深度卷积神经网络 (CNN) [158, 159] 在计算机视觉任务中取得了巨大的戌功，例如著名的 ImageNet 大规类视觉识别挑战 (ILSVRC) [160], 已经成为至少在视觉类式识别领域占主导地位的机器学习方法。从 LeNet5 [161]，虽然结构经过了若干修改 [162-166]，但是除了一些特殊情况 [167]，CNN 的基本构建块几乎总是一致的，即，</p>
<ul>
<li>卷积层、</li>
<li>全连接层( FC )</li>
<li>池化层</li>
<li>非线性的元素激活操作</li>
</ul>
<p>全连接层起到全局线性变换的作用，将输入信号映射到输出信号。在数学上，全连接层的操作可以用以下形式表示： <span class="math display">\[
y =W^{T} x + b
\]</span> 其中</p>
<ul>
<li><span class="math inline">\(x\)</span> 和 <span class="math inline">\(y\)</span> 是层内的输入和输出</li>
<li>通常它们分别是位于 <span class="math inline">\(N_{\text {in }}\)</span> 和 <span class="math inline">\(N_{\text {out }}\)</span> 维空间的实向量。</li>
<li>W 表示权重矩阵，它包含每个输入神经元和输出神经元之间的<strong>关联强度</strong>。</li>
<li><span class="math inline">\(b\)</span> 是与 <span class="math inline">\(y\)</span> 具有相同维度的偏置矢量。</li>
</ul>
<p>用线性代数的语言来说，全连接层表示 <span class="math inline">\(N_{\text {in }}\)</span> 维空间中的 <span class="math inline">\(x\)</span> 到 <span class="math inline">\(N_{\text {out }}\)</span> 维空间中的 <span class="math inline">\(y\)</span> 的一个线性<strong>映射</strong>。 <span class="math inline">\(W^{T}\)</span> 表示线性变换矩阵。在绝大多数情况中，如果 <span class="math inline">\(N_{\text {in }}\)</span> 和 <span class="math inline">\(N_{\text {out }}\)</span> 都很大，</p>
<ul>
<li>由于密集的结构，全连接层将占据很大一部分参数</li>
<li>相比之下卷积层可以看作是局部的线性变换, 参数通常并不会太多。</li>
</ul>
<p>然而，CNN 类型几乎总是必不可少的保留全连接层以实现计算机视觉任务的高精度。一个全连接层的典型例子是分类器，它将学到的图片特征映射为已识别的标签，这在神经网络中非常常见。</p>
<h2 id="矩阵乘积态">2 矩阵乘积态</h2>
<p>在量子物理的背景下，一维 (1D) 局域且有能隙的哈密顿量的基态可以有效地表示为矩阵乘积态 (MPS) [168]，应用于量子态的算符可以相应地表示为矩阵乘积算符 <span class="math inline">\(( MPO )[169,170]_{\circ}\)</span> 这种表示方法可以将波函数的参数数目从随粒子数目指数增加的标度行为减少到多项式增加，对于数值类拟具有很大的优势。</p>
<p class="note note-success">
给定一组<strong>基</strong>，任何物理算子都可以表示为一个<strong>矩阵</strong>，<strong>就像</strong>神经网络中全连接层的权重矩阵W一样。
</p>
<p>在本章中，针对 CNN 的局部卷积取得的巨大成功，</p>
<ol type="1">
<li>以及通过全连接层权重矩阵的秩来成功减掉参数个数的例子 [171-173]，</li>
<li>假定隐藏在图片中的信息总是局域关联占主导 [34]，我们提出用 MPO <strong>来表示</strong>全连接层权重矩阵 <span class="math inline">\(W\)</span> 的方案。这种方案可以<strong>利用</strong>数据集的局域性来<strong>减少</strong>参数的<strong>冗余</strong>。</li>
<li>数值实验.表明，在一系列神经网络中，例如双层全连接网络 (FC2), LeNet5 [161], VGG [174], ResNet [164], 以及 DenseNet [166]，这种表示都可以高效地完成工作。详细结果我们在接下来的小节中展示。</li>
</ol>
<h4 id="矩阵乘积算符简介">矩阵乘积算符简介</h4>
<p>首先，让我们明确我们在本文中使用的一些基本的符号。一个张量可以被看作是一个<strong>多维数组</strong>，其拥有的独立指标的个数被称为张量的阶。例如标量，矢量和矩阵分别是 0 阶、 1 阶和 2 阶张量。如此, 一个 <span class="math inline">\(N\)</span> 阶张量即是 <span class="math display">\[
A \in R ^{I_{1} \times I_{2} \times \cdots \times I_{N}}
\]</span></p>
<ul>
<li>这里 <span class="math inline">\(I_{i}(i=1, \ldots, N)\)</span> 是第 <span class="math inline">\(i\)</span> 个维度的长度，</li>
<li>这个张量总共用 <span class="math inline">\(\prod_{i=1}^{N} I_{i}\)</span> 个独立元素。</li>
<li>它的元素可以表示为 <span class="math inline">\(A_{i_{1} \cdots i_{n} \cdots i_{N}},\)</span> 对所有的 <span class="math inline">\(n,\)</span> 这里的指标 <span class="math inline">\(i_{n}\)</span> 从 1 取到 <span class="math inline">\(I_{n}\)</span> 完备的基矢展开。</li>
</ul>
<p>如此，一个一般的 <span class="math inline">\(N\)</span> 体波函数 <span class="math inline">\(|\Psi\rangle\)</span> 可以如下被表示为 <span class="math display">\[
|\Psi\rangle=\sum_{i_{1} i_{2} \ldots i_{N}} C_{i_{1} i_{2} i_{3} \ldots i_{N}}\left|i_{1} i_{2} \ldots i_{N}\right\rangle
\]</span> 这里 <span class="math inline">\(\left\{\left|i_{1} i_{2} \ldots i_{N}\right\rangle\right\}\)</span> 表示一个<strong>完备集</strong>。这样一来，波函数就可以被写作一组<strong>基矢的叠加</strong>，叠加的加权系数为对应的张量元素 <span class="math inline">\(C_{i_{1} i_{2} \ldots i_{N}}\)</span> 。自然地，在这个完备集上 定义的一般物理<strong>算子</strong> <span class="math inline">\(\hat{O}\)</span> 可以用类似的方式表示 <span class="math display">\[
\hat{O}=\sum_{i_{1} i_{2} \ldots i_{N}, j_{1} j_{2} \ldots j_{N}} O_{i_{1} i_{2} i_{3} \ldots i_{N} j_{1} j_{2} \ldots j_{N}}\left|j_{1} j_{2} \ldots j_{N}\right\rangle\left\langle i_{1} i_{2} \ldots i_{N}\right|
\]</span> 假设每个粒子的局部自由度为 <span class="math inline">\(d,\)</span> 也就是说，对所有 <span class="math inline">\(n, i_{n}\)</span> 从 1 取到 <span class="math inline">\(d,\)</span> 于是权重张量 <span class="math inline">\(C\)</span> 和物理算符 <span class="math inline">\(O\)</span> 分别具有总计 <span class="math inline">\(d^{N}\)</span> 和 <span class="math inline">\(d^{2 N}\)</span> 个元素。当 <span class="math inline">\(N\)</span> 很大的时候, 即便是仅仅存下这个权重张量也是不可实现的任务，这就是在高维数值分析和量子多体物理的希尔伯特空间中的出现的所谓的<strong>维度灾变</strong>。</p>
<p>幸运的是，如果在实空间中定义了基矢，那么对于大多数与物理相关的状态， <span class="math inline">\(C\)</span> 和 <span class="math inline">\(O\)</span> 中的绝大多数元素都将为零，只有很少的基矢会真正对 <span class="math inline">\(|\Psi\rangle\)</span> 有贡献。 这种断言来自于这样一个观察：我们感兴趣的状态几乎总是只有局域相互作用的系统的低能状态，相互作用的局域性导致了这些系统的状态有着很强的约束, 他们的基态纠缠熵满足面积定律 [16]。这个面积定律能够极大地简化这些对应态的表示，例如，描述这些低纠缠态的合适语言正是张量网络态 [175]。它在量子信息和凝聚态物理学中取得了巨大的成功。</p>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210125211314830.png" srcset="/img/loading.gif" /></p>
<blockquote>
<p>图 4.1 量子态和量子算符的矩阵乘积表示。</p>
<ol type="a">
<li><p>权重张量 <span class="math inline">\(C\)</span> 以及它所对应的矩阵乘积态表示，如公式 (4.4) 所示。对任意给定的 <span class="math inline">\(i_{n}, A^{(n)}\left[i_{n}\right]\)</span> 是一个矩阵。</p></li>
<li><p>物理算符 <span class="math inline">\(O\)</span> 及它所对应的 <span class="math inline">\(MPO\)</span> 表示。</p></li>
</ol>
<p>同样的，对任意给定的 <span class="math inline">\(i_{n}\)</span> 和 <span class="math inline">\(j_{n}, B^{(n)}\left[i_{n} j_{n}\right]\)</span> 是一个矩阵。在上述两个例子中, 一个实心圆均表示一个局域长量，它对应一个与之关联的单独粒子。</p>
</blockquote>
<p>在一维情况下，张量网络状态被简化为矩阵积状态 (MPS)。如图 4.1(a)所示，MPS 将权重矩阵 <span class="math inline">\(C\)</span> 表示为 <span class="math inline">\(N\)</span> 个局域张量 <span class="math inline">\(A ^{(i)}\)</span> 的连乘。在开边界条件下 <span class="math display">\[
C_{i_{1} i_{2} \ldots i_{N}}=\sum_{k_{1} k_{2} \ldots k_{(N-1)}} A_{k_{1}}^{(1)}\left[i_{1}\right] A_{k_{1} k_{2}}^{(2)}\left[i_{2}\right] \ldots A_{k_{(N-1)}}^{(N)}\left[i_{N}\right]
\]</span> 这里 <span class="math inline">\(A ^{(i)}\)</span> 是定义在第 <span class="math inline">\(i\)</span> 个格点上的局域张量，这里的这些 <span class="math inline">\(k\)</span> 口做虚拟指标，我们把 <span class="math inline">\(A_{k_{(n-1)} k_{n} i_{n}}\)</span> 写作 <span class="math inline">\(A_{k_{(n-1)} k_{n}}\left[i_{n}\right]\)</span> 以强调在给定 <span class="math inline">\(i_{n}\)</span> 的情况下, <span class="math inline">\(A_{k_{n-1}, k_{n}}\left[i_{n}\right]\)</span> 是一个矩阵。相应的，MPO 将权重系数 <span class="math inline">\(O\)</span> 表示为相似的一组矩阵张量 <span class="math inline">\(B ^{(i)}\)</span> 的连乘， <span class="math display">\[
O_{i_{1} \ldots i_{N} j_{1} \ldots j_{N}}=\sum_{k_{1} k_{2} \ldots k_{(N-1)}} B_{k_{1}}^{(1)}\left[i_{1} j_{1}\right] B_{k_{1} k_{2}}^{(2)}\left[i_{2} j_{2}\right] \ldots B_{k_{(N-1)}}^{(N)}\left[i_{N} j_{N}\right]
\]</span> 如图 <span class="math inline">\(4.1( b )\)</span> 所示。在量子物理中，公式 <span class="math inline">\(C_{i_{1} i_{2} \ldots i_{N}}\)</span> 和 <span class="math inline">\(O_{i_{1} \ldots i_{N} j_{1} \ldots j_{N}}\)</span> 被作为一种变分假设来近似求解基态波函数 <span class="math inline">\(|\Psi\rangle\)</span> 和物理算符 <span class="math inline">\(\hat{O}\)</span> ，事实上，它们成为了许多成功的量子多体算法的基础 [21,29]</p>
<p class="note note-info">
这种特殊的矩阵乘积结构使得参数随系统尺寸的标度行为由指数缩减为多项式，这是 MPS 和 MPO 表示法的一大优点。
</p>
<p>具体地说，在公式 <span class="math inline">\(C_{i_{1} i_{2} \ldots i_{N}}\)</span> 中，参数数目从 <span class="math inline">\(d^{N}\)</span> 减少到了 <span class="math display">\[
D^{2} d(N-2)+2 D d
\]</span> 在公式 <span class="math inline">\(O_{i_{1} \ldots i_{N} j_{1} \ldots j_{N}}\)</span> 中参数数目从 <span class="math inline">\(d^{2 N}\)</span> 减小到了 <span class="math display">\[
D^{2} d^{2}(N-2)+2 D d^{2}
\]</span> 这里 <span class="math inline">\(D\)</span> 是虚拟变量 <span class="math inline">\(\{k\}\)</span> 可能的值的数量，通常被称为<strong>键维度</strong>。在实践中，D 可以被看作是一个用来控制表示的<strong>准确性可调参数</strong>。</p>
<ul>
<li><span class="math inline">\(D\)</span> 越大，对应表示越准确。</li>
</ul>
<p>在本节中，我们主要研究用 MPO 来表示公式 <span class="math inline">\(y =W^{T} x + b\)</span> 中的全连接层的权重矩阵 <span class="math inline">\(W\)</span> 。同时保持神经网络中的其他元素，比如卷积，池化, ReLU 或 Softmax 不变。</p>
<h4 id="全连接层权重矩阵的-mpo-表示">全连接层权重矩阵的 MPO 表示</h4>
<p>卷积的成功是惊人的，但也有点令人惊讶的是，与整个图像相比，CNN 的卷积核的大小<strong>通常很小</strong>，也就是说，所谓的特征提取可以只从很小的聚类中获得。之前已有研究表明 [171-173] 全连接层中的权重矩阵可以施加很强的限制同时还不显著降低类型的预测精度。</p>
<p>我们认为，这可能是因为图片中蕴含的信息本身就是<strong>相当局域</strong>导致的结果 [176]。这种情况类似于物理系统中相互作用的局域性导致态的关联有很强的局域性一样。<strong>基于这个假设和物理学的启发</strong>，我们尝试用 MPO 来表示全连接层的权重矩阵以利用图片自身的局部性质。</p>
<p>我们考虑神经网络中的全连接层，如公式 <span class="math inline">\(y=W^{T} x+b\)</span> 所示，如果 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(y\)</span> 的元素个数分别是 <span class="math inline">\(N\)</span> 和 <span class="math inline">\(M,\)</span> 那么对于的权重矩阵 <span class="math inline">\(W\)</span> 的参数数目应该是 <span class="math inline">\(N \times M 。\)</span> 在实践中，一般 <span class="math inline">\(N\)</span> 和 <span class="math inline">\(M\)</span> 都会非常大，那么 <span class="math inline">\(W\)</span> 中大量的参数可以分解为 <span class="math display">\[
W_{i j}=W_{i_{1} i_{2} \ldots i_{n}, j_{1} j_{2} \ldots j_{n}}
\]</span> 满足 <span class="math display">\[
\prod_{i=1}^{n} I_{i}=N, \quad \prod_{i=1}^{n} J_{i}=M
\]</span> 这里 <span class="math inline">\(I_{m}\)</span> 和 <span class="math inline">\(J_{m}\)</span> 分别对应指标 <span class="math inline">\(i_{m}\)</span> 和 <span class="math inline">\(j_{m}\)</span> 的长度。</p>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210125222336729.png" srcset="/img/loading.gif" /></p>
<blockquote>
<p>图 4.2 全连接层中的权重矩阵 <span class="math inline">\(W\)</span> 用 <span class="math inline">\(M P O\)</span> 表示的图示</p>
<p>( a ) 全连接层的权重矩阵<span class="math inline">\(W\)</span>。这里用空心圆圈表示神经元, 这些神经元对应数据集的像素或模型的隐变量。连接着输入神经元 <span class="math inline">\(x_{i}\)</span> 和输出神经元 <span class="math inline">\(y_{j}\)</span> 的实线代表一个权值 <span class="math inline">\(W_{i j} .\)</span></p>
<p>( b ) W 的 <span class="math inline">\(MPO\)</span> 表示，这里的实心圆圈代表一个局域胀量 <span class="math inline">\(\omega\)</span>, 空心圆圈代表胀量的指标 <span class="math inline">\(i_{m}\)</span> and <span class="math inline">\(j_{m}\)</span> 。对任意给定的 <span class="math inline">\(i_{m}\)</span> 和 <span class="math inline">\(j_{m}, \omega^{(m)}\left[i_{m} j_{m}\right]\)</span> 是一个矩阵。</p>
<p>在本节中，如果 <span class="math inline">\(W\)</span> 的左边和右边分别对应输入和输出，那么（a）中的 <span class="math inline">\(W\)</span> 可以写作 <span class="math inline">\(W_{N}^{M},\)</span> 且它在（b）中的 <span class="math inline">\(MPO\)</span> 表示可以写作 <span class="math inline">\(M_{I_{1}, I_{2}, \ldots, I_{n}}^{J_{1}, J_{2}, \ldots J_{n}}\)</span> 此处 <span class="math inline">\(I_{m}\)</span> 和 <span class="math inline">\(J_{m}\)</span> 是 <span class="math inline">\(i_{m}\)</span> 和 <span class="math inline">\(j_{m}\)</span> 的维度，他们满巳公式 <span class="math inline">\(\prod_{i=1}^{n} I_{i}=N, \quad \prod_{i=1}^{n} J_{i}=M\)</span></p>
</blockquote>
<p>如图 4.2 所示，将全连接层的 <span class="math inline">\(W\)</span> 完全用 MPO 的表示来替代， <span class="math display">\[
W_{i_{1} i_{2} \ldots i_{n}, j_{1} j_{2} \ldots j_{n}}=\sum_{k_{1} k_{2} \ldots k_{(n-1)}} \omega_{k_{1}}^{(1)}\left[i_{1} j_{1}\right] \omega_{k_{1} k_{2}}^{(2)}\left[i_{2} j_{2}\right] \ldots \omega_{k_{(n-1)}}^{(n)}\left[i_{n} j_{n}\right]
\]</span> 这里 <span class="math inline">\(\omega^{(i)}\)</span> 是一个尺寸为 <span class="math inline">\(K_{(i-1)} \times K_{i} \times I_{i} \times J_{i}\)</span> 的张量, <span class="math inline">\(K_{i}\)</span> 是虚拟指标 <span class="math inline">\(k_{i}\)</span> 的长度。在传统的神经网络中，我们通过训练过程确定权重矩阵 <span class="math inline">\(W\)</span> ，在我们的例子中，我们通过训练过程确定每一个小张量 <span class="math inline">\(\omega 。\)</span></p>
<p>为了简洁起见，在我们的计算中，同一个 MPO 中的所有 <span class="math inline">\(K_{i}\)</span> 被设置为一样的值，记作 <span class="math inline">\(D_{\circ}\)</span> 另外，公式 <span class="math inline">\(W_{i j}=W_{i_{1} i_{2} \ldots i_{n}, j_{1} j_{2} \ldots j_{n}}\)</span> 中的指标分解并<strong>不唯一</strong>，我们至少根据便利选择了其中一种。</p>
<h3 id="相关工作简介">相关工作简介</h3>
<p>在我们的工作之前，已经有许多论文将张量网络态的表示应用于深度学习。</p>
<ol type="1">
<li>例如，Novikov 等人 [145] 使用了 MPO 表达了 CNN 中的一些全连接层。
<ul>
<li>但是, 他们为了输出结果仍然保持了一些全连接层</li>
<li>同时他们也没有在更深层和更复杂的神经网络，例如 VGG 等，中测试 MPO 的有效性。</li>
</ul></li>
<li>Kossaifi 等人 [177] 使用张量的另一种低秩近似。</li>
<li>Liu [3] 使用了幺正的树状张量网络结构来表达从输入数据到输出指标的整个映射。
<ul>
<li>此处 MERA [20] 和树张量网络 [19] 是两种不同于MPS 的张量网络结构。</li>
</ul></li>
</ol>
<p>本节讨论的工作与上述工作的不同主要要以下几点：</p>
<ol type="1">
<li>首先，这个工作中所有的全连接层，特别是全连接分类器，都被 MPO 表示所替代，即最终的神经网络完全没有全连接层。包括 VGG, FC2，LeNet5 和更深层的 ResNet 都证明了这种替代方案的有效性。同时，构造 MPO 的局部张量 <span class="math inline">\(\omega^{(i)}\)</span> 通过标准的反向传播方法与卷积核一起并行更新，这与 Novikov 所做的串行更新不同。</li>
<li>其次，我们使用 MPO 表示而不是 Tucker, MERA 和树这样的高维结构。在目前的工作中，我们的框架仍然是 CNN，我们不会尝试重建 Hallam 和 Liu 所做的神经网络的新框架，相反，我们只想在 CNN 中找到全连接层的替代表示。最后但并非最不重要的是，我们的出发点是信息的局部性和图像与量子态之间的相似性，Latorre [176] 早期在物理中也观察到了这一点。参数数目的压缩是结果而不是动机。</li>
</ol>
<p>值得注意的是，在应用数学和神经网络的背景下，MPS 和 MPO 也被称为张量列车（TT） [178] 和张量列车算符（TTO），在本文中，我们选择 MPO 的术语来强调神经网络中的线性变换与量子物理中的算符之间的潜在联系。</p>
<h2 id="mnist-数据集上的数值结果">3 MNIST 数据集上的数值结果</h2>
<p>为了简化描述，让我们介绍一些在我们的实验中使用的简写。</p>
<ol type="1">
<li><p>首先，在这项工作中, 我们把</p>
<ul>
<li>全连接层尺寸为 <span class="math inline">\(N \times M\)</span> 的权重矩阵表示为 <span class="math inline">\(W_{N}^{M}\)</span>,</li>
<li>并把图中指标维度为 <span class="math inline">\(D\)</span> 的一层 <span class="math inline">\(MPO\)</span> 表示为 <span class="math inline">\(M_{I_{1}, I_{2}, \ldots, I_{n}}^{J_{1}, J_{2} \ldots J_{n}}(D)\)</span> 。</li>
<li>也就是说，输出的维度被写为上标，而输入的维度被写为下标。虽然公式 E<span class="math inline">\(y =W^{T} x + b\)</span> 中的偏置矢量<span class="math inline">\(b\)</span> 有时可以提高类型的表现，但在这项工作中我们将它总是设置为零。</li>
</ul></li>
<li><p>第二，我们将 FC2 神经网络中的全连接层用 MPO 全部替换的新类型叫做MPO-FC2，MPO-LeNet5，MPO-VGG16等类似。</p></li>
<li><p>第三，我们定义 MPO 神经网络的压缩率 <span class="math inline">\(\rho\)</span> <span class="math display">\[
\rho=\frac{N_{ fc }}{N_{ mpo }}
\]</span> 此处</p>
<ul>
<li><span class="math inline">\(N_{ fc }\)</span> 是全连接层的参数数目</li>
<li><span class="math inline">\(N_{ mpo }\)</span> 是全连接层的 MPO 表示的参数数目。</li>
<li>如果有超过一个全连接层被替换为 MPO 表示，那么 <span class="math inline">\(N_{ fc }\)</span> 和 <span class="math inline">\(N_{ mpo }\)</span> 应理解为它们各自的总的参数个数之比。在这个定义中卷积层的参数并没有计入。</li>
</ul></li>
<li></li>
</ol>
<h2 id="a-simple-implementation-of-the-mps-born-machine">A simple implementation of the MPS Born Machine</h2>
<p>In this tutorial I would introduce: - Constructing matrix product states - Canonicalization of the MPS using QR - Single-site update of MPS - Gradient based single-site learning algorithm - How to accelarate tensor contractions using GPU.</p>
<pre><code class="hljs python"><span class="hljs-keyword">import</span> sys
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> torch 
<span class="hljs-keyword">import</span> math,time
%matplotlib inline
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

torch.manual_seed(<span class="hljs-number">1</span>) <span class="hljs-comment"># Fix seed of the random number generators</span>
np.random.seed(<span class="hljs-number">1</span>)</code></pre>
<h4 id="data-loading">Data loading</h4>
<p>1000 MNIST images have been stored as "mnist784_bin_1000.npy". Each image contains <span class="math inline">\(n=28 \times 28=784\)</span> pixels, each of which takes value 0 or 1 . Each image is viewed as a product state in the Hilbert space of dimension <span class="math inline">\(2^{n}\)</span>.</p>
<pre><code class="hljs python">
n=<span class="hljs-number">784</span> <span class="hljs-comment"># number of qubits</span>
<span class="hljs-comment">#m=1000 # m images</span>
m=<span class="hljs-number">20</span>
data=np.load(<span class="hljs-string">"mnist784_bin_1000.npy"</span>).astype(np.int32)
data=data[:m,:]
data=torch.LongTensor(data)</code></pre>
<pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">show_imgs</span><span class="hljs-params">(imgs,l1=<span class="hljs-number">4</span>,l2=<span class="hljs-number">5</span>,s1=<span class="hljs-number">6</span>,s2=<span class="hljs-number">6</span>)</span>:</span>
    <span class="hljs-string">"""    Plot images    """</span>
    plt.rcParams[<span class="hljs-string">'figure.figsize'</span>]=(s1,s2)
    imgs=imgs.cpu().reshape([<span class="hljs-number">-1</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>])
    g, ax = plt.subplots(l1,l2)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(l1): 
        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(l2):
            a=i*l2+j
            <span class="hljs-keyword">if</span>(a&gt;=imgs.shape[<span class="hljs-number">0</span>]):
                <span class="hljs-keyword">break</span>
            ax[i][j].imshow(imgs[a,:,:],cmap=<span class="hljs-string">'summer'</span>)
            ax[i][j].set_xticks([])
            ax[i][j].set_yticks([])
    plt.show()
    
show_imgs(data,<span class="hljs-number">2</span>,<span class="hljs-number">10</span>,<span class="hljs-number">10</span>,<span class="hljs-number">2</span>)</code></pre>
<figure>
<img src="http://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210125223027853.png" srcset="/img/loading.gif" alt="" /><figcaption>image-20210125223027853</figcaption>
</figure>
<pre><code class="hljs python">
print(<span class="hljs-string">"shape of data is "</span>,data.shape )
print(data)</code></pre>
<pre><code class="hljs python">
shape of data <span class="hljs-keyword">is</span>  torch.Size([<span class="hljs-number">20</span>, <span class="hljs-number">784</span>])
tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        ...,
        [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])</code></pre>
<h4 id="mps-initialization">MPS initialization</h4>
<p>Define the <span class="math inline">\(mps\)</span>, which is a list of 3 -way tensors containing random values</p>
<figure>
<img src="http://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210125223130988.png" srcset="/img/loading.gif" alt="" /><figcaption>image-20210125223130988</figcaption>
</figure>
<pre><code class="hljs python"><span class="hljs-comment">#mydevice=torch.device("cuda:0")</span>
mydevice=torch.device(<span class="hljs-string">"cpu"</span>)
data=data.to(mydevice)
bond_dims=[Dmax <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(n<span class="hljs-number">-1</span>)]+[<span class="hljs-number">1</span>]
tensors= [ torch.randn(bond_dims[i<span class="hljs-number">-1</span>],<span class="hljs-number">2</span>,bond_dims[i],device=mydevice) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(n)]</code></pre>
<pre><code class="hljs python">print(<span class="hljs-string">"shape of tensors is"</span>,len(tensors))
print([i.shape <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tensors])</code></pre>
<p>Now check the bond dimensions and tensors</p>
<pre><code class="hljs python">print(<span class="hljs-string">"There are"</span>,len(tensors), <span class="hljs-string">"tensors"</span>)
print(bond_dims)
print(<span class="hljs-string">"shape of a tensor is"</span>,tensors[<span class="hljs-number">5</span>].shape)</code></pre>
<p>Question: does the contration with one image give a probability of the image?</p>
<p>Answer: No</p>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Machine-learning/">Machine learning</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Tensor-Network/">Tensor Network</a>
                    
                      <a class="hover-with-bg" href="/tags/Machine-learning/">Machine learning</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2021/01/27/topology08_Exp01_Meeting_Entanglement/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">冷原子实验测量纠缠熵与拓扑数</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2021/01/25/ML_04_TensorNetwork2c/">
                        <span class="hidden-mobile">基于张量网络的机器学习模型</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>







  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "矩阵乘积态与机器学习&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js" ></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  
















</body>
</html>
