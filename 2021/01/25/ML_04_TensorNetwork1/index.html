<!DOCTYPE html>
<html lang="en">





<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="JPZhuang">
  <meta name="keywords" content="">
  <title>Python张量网络 - JPZ</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
<meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Physics</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner intro-2" id="background" parallax=true
         style="background: url('/img/tag-bg.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fade-in-up">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2021-01-25 13:23">
      January 25, 2021 pm
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      2.3k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      44
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <p>[TOC]</p>
<h2 id="参考文献">参考文献</h2>
<ol type="1">
<li>《<a href="https://arxiv.org/pdf/1905.01330.pdf" target="_blank" rel="noopener">TensorNetwork</a>: A Library for Physics and Machine Learning》
<ul>
<li>王老师建议</li>
<li><a href="https://github.com/google/TensorNetwork" target="_blank" rel="noopener">github</a></li>
</ul></li>
<li>导览 <a href="https://pypi.org/project/tensornetwork/" target="_blank" rel="noopener">tensor network wrapper</a> 是一种python库用来处理张量网络的
<ul>
<li>有讲义论文导览</li>
<li>有代码导览</li>
</ul></li>
<li><a href="https://arxiv.org/pdf/1906.06329v1.pdf" target="_blank" rel="noopener">TensorNetwork for Machine Learning</a></li>
<li>TensorNetwork on TensorFlow: <a href="https://arxiv.org/pdf/1905.01331.pdf" target="_blank" rel="noopener">A Spin Chain Application Using Tree Tensor Networks</a>
<ul>
<li>TensorNetwork库在物理上的一个应用</li>
<li><a href="https://github.com/amilsted/unitree" target="_blank" rel="noopener">https://github.com/amilsted/unitree</a></li>
</ul></li>
<li><a href="https://paperswithcode.com/" target="_blank" rel="noopener">papers with code</a> 是一个论文和代码结合的好网站
<ul>
<li>主题1：<a href="https://paperswithcode.com/task/tensor-networks" target="_blank" rel="noopener">Tensor Networks</a></li>
<li>主题2：<a href="https://paperswithcode.com/task/quantum-machine-learning" target="_blank" rel="noopener">Quantum Machine Learning</a></li>
</ul></li>
</ol>
<h2 id="概览">1 概览</h2>
<p>TensorNetwork is an open source library for implementing tensor network algorithms [1]. Tensor networks are sparse data structures originally designed for simulating quantum many-body physics, but are currently also applied in a number of other research areas, including machine learning. We demonstrate the use of the API with applications both physics and machine learning, with details appearing in companion papers.</p>
<h3 id="介绍">1.1 介绍</h3>
<p>Tensor networks are sparse data structures engineered for the efficient representation and manipulation of very <strong>high-dimensional data</strong>. They have largely been developed and used in condensed matter physics [2-19], quantum chemistry, statistical mechanics, quantum field theory, and even quantum gravity and cosmology</p>
<p>应用张量网络到机器学习 Substantial progress has been made recently in applying tensor networks to machine learning.</p>
<ol type="1">
<li>Stoudenmire and Schwab used a matrix product state (MPS) for classification of the MNIST dataset [35]. 乘积态用来分类</li>
<li>Levine et al. showed that a deep convolutional arithmetic circuit (ConvAC) is equivalent to a tree tensor network, and gave empirical support for a more general relationship between tensor networks and convolutional network architectures [36]. 卷积(ConvAC) 和一种树张量的关系</li>
<li>Liu et al. employed a two-dimensional hierarchical tree tensor network for image recognition on both MNIST and CIFAR-10 [37]. 二维层次树用来图像识别</li>
</ol>
<p>These are only a few examples from the growing body of literature on tensor networks and machine learning (for more, see e.g. <span class="math inline">\([38-45])\)</span>. One of our primary goals in creating the TensorNetwork library is to accelerate this research.</p>
<p>To keep this document self-contained, we will review the basics of tensor networks in Section II. Then in Section III we will describe how to perform the most common tensor network computations using the TensorNetwork API, which is built on top of Tensorflow [46]. A pair of companion papers will describe sample applications in physics and machine learning, respectively, in greater detail.</p>
<h2 id="张量网络">2 张量网络</h2>
<p>In this section we will introduce the graphical notation for tensor networks.</p>
<h4 id="张量的表示">张量的表示</h4>
<p>For our purposes a "tensor" is synonymous with a multidimensional matrix or multidimensional array. That is, a tensor <span class="math inline">\(A\)</span> with rank <span class="math inline">\(r\)</span> consists of a set of numbers <span class="math inline">\(A_{i_{1}, \ldots i_{r}},\)</span> where the index <span class="math inline">\(i_{k}\)</span> has <span class="math inline">\(\operatorname{dim}_{k}\)</span> possible values.</p>
<ol type="1">
<li>The special case <span class="math inline">\(r=0\)</span> is just a number, also called a scalar.</li>
<li>The case <span class="math inline">\(r=1\)</span> is a vector.</li>
<li>The case <span class="math inline">\(r=2\)</span> is an ordinary matrix.</li>
</ol>
<p>Graphically, a rank <span class="math inline">\(r\)</span> tensor is represented by a node (for us a colored circle) with <span class="math inline">\(r\)</span> lines, or "legs," coming out of it. Each of those lines represents an index. See Figure 1 for examples.</p>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210125152532528.png" srcset="/img/loading.gif" style="zoom:50%;" /></p>
<h4 id="张量的收缩">张量的收缩</h4>
<p>Now we will discuss the contraction of tensors, which is best illustrated by example. See Figure 2 for a summary.</p>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210125153724271.png" srcset="/img/loading.gif" style="zoom:50%;" /></p>
<p>If we are given two vectors <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> with the same dimension, then we can form the inner product <span class="math inline">\(A \cdot B=\sum_{i} A_{i} B_{i}\)</span>. Notationally, we represent this by connecting the leg of <span class="math inline">\(A\)</span> with the leg of <span class="math inline">\(B\)</span>. When we do this, the resulting object has no free legs. This reflects the fact that there are no remaining indices in the product <span class="math inline">\(A \cdot B\)</span>. In other words, <span class="math inline">\(A \cdot B\)</span> is a scalar.</p>
<p>If <span class="math inline">\(A\)</span> is a matrix and <span class="math inline">\(B\)</span> is a vector where the second dimension of <span class="math inline">\(A\)</span> is equal to the dimension of <span class="math inline">\(B,\)</span> then we can do the matrix-vector multiplication <span class="math inline">\(\sum_{j} A_{i j} B_{j} .\)</span> Graphically, <span class="math inline">\(A\)</span> has two legs while <span class="math inline">\(B\)</span> has one, and <span class="math inline">\(\sum_{j} A_{i j} B_{j}\)</span> is represented by connecting the second leg of <span class="math inline">\(\bar{A}\)</span> to the leg of <span class="math inline">\(B\)</span>. The resulting object has a single leg remaining, corresponding to the one free index of <span class="math inline">\(\sum_{j} A_{i j} B_{j} .\)</span> In other words, this object is a vector.</p>
<p>Similarly, if <span class="math inline">\(A\)</span> is a matrix and <span class="math inline">\(B\)</span> is a matrix where the second dimension of <span class="math inline">\(A\)</span> is equal to the first dimension of <span class="math inline">\(B\)</span>, then we can do the matrix-matrix multiplication <span class="math inline">\(\sum_{j} A_{i j} B_{j k} .\)</span> The resulting object is a matrix with two free legs - a matrix.</p>
<p>If a single tensor has multiple legs, we can also connect them to each other if the dimensions match. For example, we can connect the two legs of a square matrix <span class="math inline">\(A\)</span> to each other. This corresponds to the scalar <span class="math inline">\(\sum_{i} A_{i i},\)</span> which is just the trace <span class="math inline">\(\operatorname{Tr} A\)</span></p>
<h4 id="张量网络-1">张量网络</h4>
<p>Moving beyond these elementary examples, a useful tensor network is made by connecting several nodes together as part of a larger architecture. The result is a total network with many open legs that is made from the contraction of several smaller tensors. The network itself can be viewed as a very high-dimensional array, but a very sparse one. The tensor network is an efficient representation of that sparse data, and by intelligently manipulating the tensor network we can effectively deal with the high-dimensional data without a large cost. We display three examples of more advanced tensor networks in Figure 3 .</p>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210125154856207.png" srcset="/img/loading.gif" style="zoom:50%;" /></p>
<h3 id="基础对象">基础对象</h3>
<h4 id="tensor-networks">1. Tensor Networks</h4>
<p>A TensorNetwork is the main object of the library. It keeps track of its own set of Node objects, and contains methods to add additional nodes, connect them with edges, contract them, and manipulate them in other ways. #### 2. Nodes</p>
<p>Nodes are one of the <strong>basic building blocks</strong> of a tensor network. They represent a tensor in the computation. Each axis will have a corresponding edge that can possibly connect different nodes (or even the same node) together. <strong>The number of edges represents the rank of the underlying tensor</strong>. For example, a node without any edges is a scalar, a node with one edge is a vector, etc. Nodes are created within a <em>TensorNetwork</em> by passing a tensor to the <em>add_node</em> method.</p>
<pre><code class="hljs python"><span class="hljs-keyword">import</span> tensornetwork
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
net = tensornetwork.TensorNetwork()
a = net.add_node(tf.eye(<span class="hljs-number">2</span>)) <span class="hljs-comment"># Numpy arrays can also be passed.</span>
print(a.get_tensor()) <span class="hljs-comment"># This is how you access the underlying tensor.</span></code></pre>
<h4 id="edges">3 Edges</h4>
<p>Edges describe different contractions of the underlying tensors in the tensor network. Each edge points to the axes of the tensors to be contracted. There are three basic kinds of edges in a tensor network (see Figure 4 ):</p>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210125231406463.png" srcset="/img/loading.gif" style="zoom:50%;" /></p>
<blockquote>
<p>FIG.4 The three types of edges that appear in a tensor network.</p>
</blockquote>
<p><strong>1. Standard Edges</strong></p>
<p>Standard edges are like any other edge you would find in an undirected graph. They connect two different nodes and define a dot product among the given vector spaces. In numpy terms, this edge defines a tensordot operation over the given axes.</p>
<p><strong>2. Trace Edges</strong></p>
<p>Trace edges connect a node to itself. To contract this type of edge, you take a trace of the matrix created by the two given axes.</p>
<p><strong>3. Dangling Edges</strong></p>
<p>Dangling edges are edges that only have one side point to a node, whereas the other side is left "dangling." These edges represent output axes or intermediate axes that have yet to be connected to other dangling edges. These edges are automatically created when adding a node to the network.</p>
<h3 id="基本操作">基本操作</h3>
<h4 id="connecting-dangling-edges">1 Connecting Dangling Edges</h4>
<p>Prior to contracting an edge, it must be connected in the network (see Figure 5 ).</p>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210125231655517.png" srcset="/img/loading.gif" style="zoom:50%;" /></p>
<blockquote>
<p>FIG.5 Connecting two dangling edges creates a new Edge object between the two Nodes.</p>
</blockquote>
<p>When constructing a tensor network, edges which begin as dangling can be connected using the <strong>connect method</strong>. This method will create a <strong>new Edge object</strong> which points to the two nodes that are being connected, replacing the two previous dangling edges.</p>
<p>We can illustrate with an example from quantum computing. Here, we have a single qubit quantum circuit where we apply a Hadamard operation several times. We connect the dangling edge of the qubit to the Hadamard operation and return the resulting "output" edge:</p>
<pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">apply_hadamard</span><span class="hljs-params">(net, edge)</span>:</span>
    hadamard_op = np.array([[<span class="hljs-number">1.0</span>, <span class="hljs-number">1.0</span>], [<span class="hljs-number">1.0</span>, <span class="hljs-number">-1.0</span>]]) / np.sqrt(<span class="hljs-number">2.0</span>)
    hadamard_node = net.add_node(hadamard_op)
    <span class="hljs-comment"># Connect the "qubit edge" to the operator "input edge"</span>
    net.connect(edge, hadamard_node[<span class="hljs-number">1</span>])
    <span class="hljs-keyword">return</span> hadamard_node[<span class="hljs-number">0</span>] <span class="hljs-comment"># This is the "output edge".</span>

<span class="hljs-comment"># Build the quantum circuit.</span>
net = tensornetwork.TensorNetwork()
qubit = net.add_node(np.array([<span class="hljs-number">1.0</span>, <span class="hljs-number">0.0</span>])) <span class="hljs-comment"># A "zero state" qubit.</span>
qubit_edge = qubit.get_edge(<span class="hljs-number">0</span>) <span class="hljs-comment"># qubit[0] is equivalent.</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">5</span>):
    qubit_edge = apply_hadamard(net, qubit_edge)</code></pre>
<h2 id="应用">3 应用</h2>
<ol type="1">
<li>Machine Learning</li>
<li>Physics</li>
</ol>
<h3 id="a机器学习">A机器学习</h3>
<p>Our applications of tensor networks to machine learning follows the example of Stoudenmire and Schwab [35], which uses a matrix product state (Figure 3a) to classify digits from the MNIST dataset. The method is conceptually very similar to finding the ground state energy of a physical system, except the role of "energy" is played by a loss function. Here we will review how the image data is encoded in the tensor network, and for the details of the implementation we refer readers to [47] .</p>
<p>Tensor networks naturally operate in linear spaces of states (like the space of quantum states in physics). For image classification, this would be the space of images. One must be careful to define what the space of images is. An image consists of an ordered list of <span class="math inline">\(N\)</span> pixels, and each of those pixels has a value. For simplicity, let us assume the images are greyscale, so that a pixel's value is a single number. A naive definition of the "space of images" would be an <span class="math inline">\(N\)</span> -dimensional space, where the list of pixel values corresponding to a given image could be directly interpreted as a column vector in that space. That is not the space of images we will use. This naive image space has the property that changing the value of a single pixel gives an almost-identical image vector. We will instead use a more powerful notion of image space that is <span class="math inline">\(2^{N}\)</span> -dimensional, where changing the value of a single pixel can lead to a totally independent vector.</p>
<p>To construct the image space, we first encode each pixel of our image into a two-dimensional pixel-space. If the image is black-and-white, so that each pixel has only two possible values, then we can use the familiar one-hot encoding into the pixel-space. That means one value of the pixel is mapped to <span class="math inline">\((1,0)^{T}\)</span> and the other to <span class="math inline">\((0,1)^{T}\)</span>. In the more general greyscale case, we can define a local feature map which takes each pixel value <span class="math inline">\(p\)</span> and maps it to a vector: <span class="math display">\[
\Phi(p)=\left(\begin{array}{l}
\cos \pi p / 2 \\
\sin \pi p / 2
\end{array}\right)
\]</span> This is not the only possible feature map, but it is a particularly simple choice. Restricting to <span class="math inline">\(p \in\{0,1\}\)</span> recovers one-hot encoding for a black-and-white image. If we perform this local feature map on every pixel, then the entire image has been encoded into the tensor product of all of the two-dimensional pixel-spaces. This is our <span class="math inline">\(2^{N}\)</span> -dimensional pixel space.</p>
<h3 id="b-物理">B 物理</h3>
<p>For our physics-based application we use a tree tensor network (Figure <span class="math inline">\(3 b\)</span> ) to find the ground state of a quantum spin chain. The details of the implementation can be found in <span class="math inline">\([48],\)</span> but here we will review the basic setup and summarize the results.</p>
<p>The basic idea (familiar to physicists) is very similar to the image classification setup above. The tensor network represents a vector in a state space, but in this case it is the space of possible spin configurations in a physical system, rather than the space of images. This is the setting of quantum mechanics, where the state space of any physical system is a vector space. In this context, "training" the tensor network amounts to turning the tensors so that the resulting state has some desired property. In our case, we wish the vector to be the ground state of a particular spin system. That is, we want the vector to represent the state of lowest energy with respect to some chosen energy function.</p>
<p>The algorithm for training a TTN to find the ground state energy is a well-known one, and can be found in <span class="math inline">\([10] .\)</span> A benefit of using the TensorNetwork library is that the same code can be run on multicore CPUs as well as GPUs with minimal effort. The results, reported in [48], show that a 100x improvement in speed can be found by running on GPUs. This particular algorithm is well-suited for implementation in a TensorFlow-based library like TensorNetwork, since the primary computational bottleneck is tensor contractions. These are the operations for which one expects a GPU to have the greatest advantage over a CPU, and indeed we find that to be the case.</p>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Machine-learning/">Machine learning</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Tensor-Network/">Tensor Network</a>
                    
                      <a class="hover-with-bg" href="/tags/Machine-learning/">Machine learning</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2021/01/25/ML_04_TensorNetwork2c/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">基于张量网络的机器学习模型</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2021/01/24/ML_02_review02_CMP/">
                        <span class="hidden-mobile">凝聚态物理中的机器学习</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div id="tocbot"></div>
</div>


      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>







  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 1,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "Python张量网络&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js" ></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  
















</body>
</html>
