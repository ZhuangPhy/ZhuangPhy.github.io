<!DOCTYPE html>
<html lang="en">





<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="JPZhuang">
  <meta name="keywords" content="">
  <title>变分推理 - JPZ</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
<meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Physics</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner intro-2" id="background" parallax=true
         style="background: url('/img/tag-bg.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fade-in-up">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2021-04-02 14:23">
      April 2, 2021 pm
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      6.1k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      121
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <h2 id="section"></h2>
<p>[TOC]</p>
<h2 id="参考文献">参考文献</h2>
<ol type="1">
<li><a href="https://arxiv.org/pdf/2103.06720.pdf" target="_blank" rel="noopener">Variational inference with a quantum computer</a></li>
<li><a href="https://medium.com/cambridge-quantum-computing/reasoning-under-uncertainty-with-a-near-term-quantum-computer-99882dc04bb" target="_blank" rel="noopener">Reasoning under uncertainty with a near-term quantum computer</a></li>
</ol>
<h2 id="摘要">摘要</h2>
<p>Inference is the task of drawing conclusions about unobserved variables given observations of related variables. Applications range from identifying diseases from symptoms to classifying economic regimes from price movements. Unfortunately, performing exact inference is intractable in general. One alternative is variational inference, where a candidate probability distribution is optimized to approximate the posterior distribution over unobserved variables. For good approximations a flexible and highly expressive candidate distribution is desirable. In this work, we propose quantum Born machines as variational distributions over discrete variables. We apply the framework of operator variational inference to achieve this goal. In particular, we adopt two specific realizations: one with an adversarial objective and one based on the kernelized Stein discrepancy. We demonstrate the approach numerically using examples of Bayesian networks, and implement an experiment on an IBM quantum computer. Our techniques enable efficient variational inference with distributions beyond those that are efficiently representable on a classical computer.</p>
<h2 id="简介">简介</h2>
<p>Probabilistic graphical models describe the dependencies of random variables in complex systems [1]. This framework enables two important tasks: learning and inference. Learning yields a model that approximates the observed data distribution. Inference uses the model to answer queries about unobserved variables given observations of other variables. In general, exact inference is intractable and so producing good approximate solutions becomes a desirable goal. This article introduces approximate inference solutions using a hybrid quantum-classical framework.</p>
<p>Prominent examples of probabilistic graphical models are Bayesian and Markov networks. <span class="math inline">\(\quad\)</span> Applications across many domains employ inference on those models, including in health care and medicine [2,3] , biology, genetics and forensics <span class="math inline">\([4,5],\)</span> finance [6] and fault diagnosis [7]. These are applications where qualifying and quantifying the uncertainty of conclusions is crucial. The posterior distribution can be used to quantify this uncertainty. It can also be used in other downstream tasks such as determining the likeliest configuration of unobserved variables which best explains observed data.</p>
<p>Approximate inference methods broadly fall into two categories: Markov chain Monte Carlo (MCMC) and variational inference (VI). MCMC methods produce samples from the true posterior distribution in an asymptotic limit [8,9] . VI is a machine learning technique which casts inference as an optimization problem over a parameterized family of probability distributions [10]. If quantum computers can deliver even a small improvement to these methods, the impact across science and engineering could be large.</p>
<p>Initial progress in combining MCMC with quantum computing was achieved by replacing standard MCMC with quantum annealing hardware in the training of some types of Bayesian and Markov networks [11-16]. Despite promising empirical results, it proved difficult to show if and when a quantum advantage can be delivered. An arguably more promising path towards quantum advantage is through gate-based quantum computers. In this context, there exist algorithms for MCMC with proven asymptotic advantage <span class="math inline">\([17-20]\)</span>, but they require error correction and other features that go beyond the capability of existing machines. Researchers have recently shifted their focus to near-term machines, proposing new quantum algorithms for sampling thermal distributions <span class="math inline">\([21-25]\)</span></p>
<p>The advantage of combining quantum computing with VI has not been explored to the same extent of MCMC. Previous work includes classical VI algorithms using ideas from quantum annealing <span class="math inline">\([26-28] .\)</span> In broad terms, VI exhibits faster convergence than MCMC in high dimensions but it does not provide the same theoretical guarantees.</p>
<p>In this work, we turn our attention to performing VI on a quantum computer. We adopt a distinct approach that focuses on improving inference in classical probabilistic models using quantum resources. We use Born machines, which are quantum machine learning models that exhibit high expressivity. We show how to employ gradient-based methods and amortization in the training phase. These choices are inspired by recent advances in classical VI [29]</p>
<p>Finding a quantum advantage in machine learning in any capacity is an exciting research goal, and promising theoretical works in this direction have been recently developed across several prominent subfields. Advantages in supervised learning have been proposed considering: information theoretic arguments [30,31] , probably approximately correct (PAC) learning <span class="math inline">\([32-35]\)</span> and the representation of data in such models <span class="math inline">\([36-38]\)</span>. More relevant for our purposes are results in unsupervised learning, which have considered complexity and learning theory arguments for distributions <span class="math inline">\([39-41]\)</span> and quantum non-locality and contextuality [42]. Furthermore, some advantages have been observed experimentally [43-46]. In Section II we describe VI and its applications. In Section III we describe using Born machines to approximate posterior distributions. In Section IV we use the framework of operator VI to derive two suitable objective functions, and we employ classical techniques to deal with the problematic terms in the objectives. In Section <span class="math inline">\(V\)</span> we demonstrate the methods on Bayesian networks. We conclude in Section VI with a discussion of possible generalizations and future work.</p>
<h2 id="变分推理的应用">变分推理的应用</h2>
<p>It is important to clarify what type of inference we are referring to. Consider a probabilistic model <span class="math inline">\(p\)</span> over some set of random variables, <span class="math inline">\(Y .\)</span> The variables in <span class="math inline">\(Y\)</span> can be continuous or discrete. Furthermore, assume we are given evidence for some variables in the model. This set of variables, denoted <span class="math inline">\(X \subseteq Y\)</span>, is then observed (fixed to the values of the evidence) and we use the vector notation <span class="math inline">\(x\)</span> to denote a realization of these observed variables. It is assumed that <span class="math inline">\(x \sim p( x )\)</span>, meaning that the probabilistic model is able to capture the distribution of observed data <span class="math inline">\(^{1}\)</span>. We now want to infer the posterior distribution of the unobserved variables, those in the set <span class="math inline">\(Z := Y \backslash X\)</span>. Denoting these by a vector <span class="math inline">\(z ,\)</span> our target is that of computing the posterior distribution <span class="math inline">\(p( z \mid x ),\)</span> the conditional probability of <span class="math inline">\(z\)</span> given <span class="math inline">\(x\)</span>. By definition, the conditional can be expressed in terms of the joint divided by the marginal: <span class="math inline">\(p( z \mid x )=p( x , z ) / p( x )\)</span>. Also recall that the joint can be written as <span class="math inline">\(p( x , z )=p( x \mid z ) p( z )\)</span>. Bayes' theorem combines the two identities and yields <span class="math inline">\(p( z \mid x )=p( x \mid z ) p( z ) / p( x )\)</span></p>
<p>As described above, the inference problem is rather general. To set the scene, let us discuss some concrete examples in the context of Bayesian networks. These are commonly targeted in inference problems, and as such shall be our test bed for this work. A Bayesian network describes a set of random variables with a clear conditional probability structure. This structure is a directed acyclic graph where the conditional probabilities are modeled by tables, by explicit distributions, or even by neural networks. Figure <span class="math inline">\(1 a\)</span> is the textbook example of a Bayesian network for the distribution of binary variables: cloudy <span class="math inline">\((C),\)</span> sprinkler <span class="math inline">\((S),\)</span> rain <span class="math inline">\((R)\)</span> and the grass being wet <span class="math inline">\((W)\)</span>. According to the graph this distribution factorizes as <span class="math inline">\(P(C, S, R, W)=\)</span> <span class="math inline">\(P(C) P(S \mid C) P(R \mid C) P(W \mid S, R) . \quad\)</span> A possible inferential question is: what is the probability distribution of <span class="math inline">\(C\)</span>, <span class="math inline">\(S\)</span> and <span class="math inline">\(R\)</span> given that <span class="math inline">\(W=\)</span> tr? This can be estimated by 'inverting' the probabilities using Bayes' theorem: <span class="math inline">\(p(C, S, R \mid W=\operatorname{tr})=p(W=\operatorname{tr} \mid C, S, R) p(C, S, R) / p(W)\)</span> Figures <span class="math inline">\(1 b -1 d\)</span> show a few additional applications of inference on Bayesian networks.</p>
<p>The hidden Markov model in Fig. <span class="math inline">\(1 b\)</span> describes the joint probability distribution of a time series of asset returns and an unobserved 'market regime' (e.g. a booming vs a recessive economic regime). A typical inference task is to detect regime switches by observing asset returns [48]. Figure <span class="math inline">\(1 c\)</span> illustrates a modified version of the 'lung cancer' Bayesian network which is an example from medical diagnosis (see e.g. [3] and references therein). This network encodes expert knowledge about the relationship between risk factors, diseases and symptoms, and we revisit it in more specificity in Section VC. In health care, careful design of the network and algorithm are critical in order to reduce biases, e.g. in relation to health care access [49]. Note that inference in medical diagnosis is often causal instead of associative [3]. Bayesian networks can be interpreted causally and help answer causal queries using Pearl's do-calculus [50]. Finally, Figure id shows a Bayesian network representation of turbo codes, which are error correction scheme used in <span class="math inline">\(3 G\)</span> and <span class="math inline">\(4 G\)</span> mobile communications. The inference task for the receiver is to recover the original information bits from the information bits and codewords received over a noisy channel [51].</p>
<p>Inference is a computationally hard task in all but the simplest probabilistic models. Roth [52) extended results of Cooper [53] and showed that exact inference in Bayesian networks with discrete variables is #P-complete. Dagum and Luby [54] showed that even approximate inference is NP-hard. Therefore, unless some particular constraints are in place, these calculations are intractable. In many cases one is able to perform a 'forward pass' and obtain unbiased samples from the joint <span class="math inline">\(( x , z ) \sim p( x , z )=p( x \mid z ) p( z )\)</span> However, obtaining unbiased samples from the posterior <span class="math inline">\(z \sim p( z \mid x )=p( x , z ) / p( x )\)</span> is intractable due to the unknown normalization constant. One can perform MCMC sampling by constructing an ergodic Markov chain whose stationary distribution is the desired posterior. MCMC methods have nice theoretical guarantees, but they may converge slowly in practice [9]. In contrast, VI is often faster in high dimensions but does not come with guarantees. The idea in VI is to optimize a variational distribution <span class="math inline">\(q\)</span> by minimizing its 'distance' from the true posterior <span class="math inline">\(p\)</span> (see [10] for an introduction to the topic and [29] for a review of the recent advances).</p>
<p>VI has experienced a resurgence in recent years due to substantial developments. First, generic methods have reduced the amount of analytical calculations required and have made VI much more user friendly (e.g. black-box VI [55]). Second, machine learning has enabled the use of highly expressive variational distributions implemented via neural networks [56], probabilistic programs [57], non-parametric models [58], normalizing flows [59] and others. In contrast, the original VI methods were mostly limited to analytically tractable distributions such as those which could be factorized. Third, amortization methods have reduced the costs by optimizing <span class="math inline">\(q_{ \theta }( z \mid x )\)</span> where the vector of parameters <span class="math inline">\(\theta\)</span> is 'shared' among all possible observations <span class="math inline">\(x\)</span> instead of optimizing individual parameters for each observation. This approach can also generalize across inferences.</p>
<p>Casting the inference problem as an optimization problem comes itself with challenges, in particular when the unobserved variables are discrete. REINFORCE [60] is a generic method which requires calculation of the score <span class="math inline">\(\partial_{\theta} \log q_{\theta}( z \mid x )\)</span> and may suffer from high variance. Gumbel-softmax reparametrizations [61, 62] use a continuous relaxation which does not follow the exact distribution. We now show that near-term quantum computers provide an alternative tool for VI. We use the quantum Born machine as a candidate generator for variational distributions. These models are highly expressive and they naturally represent discrete distributions as a result of quantum measurement. Furthermore, the Born machine can be trained by gradient-based optimization.</p>
<h2 id="波恩机器作为隐性的变分分布">波恩机器作为隐性的变分分布</h2>
<p>By exploiting the inherent probabilistic nature of quantum mechanics, one can model the probability distribution of classical data using a pure quantum state. This model based on the Born rule in quantum mechanics is referred to as Born machine [63]. Let us consider binary data <span class="math inline">\(z \in\{0,1\}^{n}\)</span> where <span class="math inline">\(n\)</span> is the number of variables. The Born machine is a normalized quantum state <span class="math inline">\(|\psi( \theta )\rangle\)</span> parameterized by <span class="math inline">\(\theta\)</span> which outputs <span class="math inline">\(n\)</span> -bit strings with probabilities <span class="math inline">\(q_{ \theta }( z )=|\langle z \mid \psi( \theta )\rangle|^{2}\)</span>. Here <span class="math inline">\(| z \rangle\)</span> are computational basis states, thus sampling the above probability boils down to a simple measurement. Other forms of discrete data can be dealt with by a suitable encoding. When using amortization, the variational distribution requires conditioning on observed variables. To extend the Born machine and include this feature, we let <span class="math inline">\(x\)</span> play the role of additional parameters. This yields a pure state where the output probabilities are <span class="math inline">\(q_{ \theta }( z \mid x )=|\langle z \mid \psi( \theta , x )\rangle|^{2}\)</span>. Figure 2 shows the relation between the classical model for the observed data and the quantum model for approximate inference.</p>
<p>Born machines have been applied for benchmarking hybrid quantum-classical systems <span class="math inline">\([64-67],\)</span> generative modeling <span class="math inline">\([68,69],\)</span> finance <span class="math inline">\([44,45,70], \quad\)</span> anomaly detection [71], and have been proposed for demonstrating quantum advantage <span class="math inline">\([40]^{2}\)</span>. These models can be realized in a variety of ways, in both classical and quantum computers. When realized via certain classes of quantum circuits they are classically intractable to simulate. For example, instantaneous quantum poly-time (IQP) circuits are Born machines with <span class="math inline">\(O(\)</span> poly <span class="math inline">\((n))\)</span> parameters that yield classically intractable distributions in the average case under widely accepted complexity theoretic assumptions [75] .</p>
<p>Additional examples for such classically hard circuits are boson sampling [76] and random circuits [77,78] . Thus quantum Born machines have expressive power larger than that of classical models, including neural networks [79] and partially matrix product states [80]. It can be shown that the model remains classically intractable throughout training, which is itself a form of quantum advantage [40].</p>
<p>A useful way to classify probabilistic models is the following: prescribed models provide an explicit parametric specification of the distribution, implicit models define only the data generation process [81]. Born machines can be effectively regarded as implicit models. It is easy to obtain an unbiased sample as long as we can execute the corresponding circuit on a quantum computer and measure the computational basis. However, it requires exponential resources to estimate the distribution even on a quantum computer. To see this consider the conditional distribution <span class="math inline">\(q_{ \theta }\left( z _{a} \mid z _{b}, x \right)\)</span> where <span class="math inline">\(\left( z _{a}, z _{b}\right)\)</span> is a partition of <span class="math inline">\(z\)</span>. To estimate this from samples of the joint distribution we would need the unnatural resource of post-selection [82]. Simulating post-selection has exponential cost in the worst case. There exist classes of Born machines for which the dependency between parameters and distribution is clear. For example, when restricting the Born machine to an IQP circuit [40], amplitudes <span class="math inline">\(\langle z \mid \psi( \theta )\rangle\)</span> are proportional to partition functions of complex Ising models [83,84] . Yet estimation of arbitrary amplitudes remains intractable.</p>
<p>Implicit models are challenging to train with standard methods. The major challenge is the design of the objective function precisely because likelihoods are 'prohibited'. Valid objectives involve only statistical quantities (such as expectation values) that can be efficiently estimated from samples. <span class="math inline">\({ }^{4}\)</span> For generative modeling with Born machines progress has been made towards practical objectives such as moment-matching [64], maximum mean discrepancy [85], Stein and Sinkhorn divergences [40], adversarial objectives [86,87] as well as incorporating Bayesian priors on the parameters [79]. In the next section we make some progress towards practical objectives for VI with Born machines. First, we mention some related work. The approaches in [22] use VI ideas to deal with thermal states and quantum data. In contrast, our inference methods apply to classical graphical models and classical data. The approaches of [17,88,89] aim to encode Bayesian networks directly in a quantum state, and subsequently perform exact inference. In contrast, our inference methods are approximate, efficient, and apply to graphical models that are not Bayesian networks.</p>
<h2 id="变分推理算子">变分推理算子</h2>
<p>Operator Variational Inference (OPVI) [90] is a rather general method that uses mathematical operators to design objectives for the approximate posterior. Suitable operators are those for which (i) the minima of the variational objective is attained at the true posterior, and (ii) it is possible to estimate the objective without computing the true posterior. In general, the amortized OPVI objective is: <span class="math display">\[
E _{ x \sim p( x )} \sup _{f \in F } h\left( E _{ z \sim q( z \mid x )}\left[\left(O^{p, q} f\right)( z )\right]\right)
\]</span> where <span class="math inline">\(f(\cdot) \in R ^{d}\)</span> is a test function within the chosen family <span class="math inline">\(F , O^{p, q}\)</span> is an operator that depends on <span class="math inline">\(p( z \mid x )\)</span> and <span class="math inline">\(q( z \mid x )\)</span>, and <span class="math inline">\(h(\cdot) \in[0, \infty]\)</span> yields a non-negative objective. We present two methods that follow directly from two operator choices. These operator choices result in objectives based on the Kullback-Leibler (KL) divergence and the Stein discrepancy. The former utilizes an adversary to make the computation tractable, whereas in the latter, the tractability arises from the computation of a kernel function.</p>
<p>The KL divergence is an example of an <span class="math inline">\(f\)</span> -divergence, while the Stein discrepancy is in the class of integral probability metrics, two fundamentally different families of probability distance measures. OPVI can therefore yield methods from these two different classes under a suitable choice of operator. However, it is shown in [91] that these two families intersect non-trivially only at the total variation distance (TVD). It is for this reason that we choose the TVD as our benchmark in later numerical results.</p>
<h3 id="a.-the-adversarial-method">A. The adversarial method</h3>
<p>One possible objective function for VI is the Kullback-Leibler divergence (KL) of the true posterior relative to the approximate one. This is obtained from Eq. (1) by choosing <span class="math inline">\(f\)</span> and <span class="math inline">\(h\)</span> to be identity functions, and by choosing the operator <span class="math inline">\(\left(O^{p, q} f\right)( z )=\log \frac{q( z \mid x )}{p( z \mid x )}:\)</span> <span class="math display">\[
E _{ x \sim p( x )} \underbrace{ E _{ z \sim q_{ \theta }( z \mid x )}\left[\log \frac{q_{ \theta }( z \mid x )}{p( z \mid x )}\right]}_{\operatorname{KL}\left[q_{ \theta }( z \mid x ) \| p( z \mid x )\right]} .
\]</span> Here <span class="math inline">\(q_{\theta}\)</span> is the variational distribution parameterized by <span class="math inline">\(\theta\)</span>. The objective's minimum is zero and is attained by the true posterior <span class="math inline">\(q_{ \theta }( z \mid x )=p( z \mid x ), \forall x .\)</span> There are many ways to rewrite this objective. Here we focus on the form known as prior-contrastive. Substituting Bayes formula <span class="math inline">\(p( z \mid x )=p( x \mid z ) p( z ) / p( x )\)</span> in the equation above we obtain: <span class="math display">\[
E _{ x \sim p( x )} E _{ z \sim q_{ \theta }( z \mid x )}\left[\log \frac{q_{ \theta }( z \mid x )}{p( z )}-\log p( x \mid z )\right]- H [p( x )]
\]</span> where the entropy <span class="math inline">\(H [p( x )]:=- E _{ x \sim p( x )}[\log p( x )]\)</span> is constant with respect to <span class="math inline">\(q\)</span> and can be ignored. We are assuming an explicit conditional <span class="math inline">\(p( x \mid z )\)</span> that can be efficiently computed (e.g. when the observed <span class="math inline">\(x\)</span> are 'leaves' in a Bayesian networks). This objective has been used in many generative models, including the celebrated variational autoencoder (VAE) [56, 92]. While the original VAE relies on tractable variational posteriors, one could also use more powerful, implicit distributions as demonstrated in adversarial variational Bayes [93] and in prior-contrastive adversarial VI [94].</p>
<p>Let us use a Born machine to model the variational distribution <span class="math inline">\(q_{ \theta }( z \mid x )=|\langle z \mid \psi( \theta , x )\rangle|^{2}\)</span>. As the model is implicit, the ratio <span class="math inline">\(q_{ \theta }( z \mid x ) / p( z )\)</span> in Eq. (3) cannot be computed efficiently. Therefore, we introduce an adversarial method for estimating the ratio above approximately. The key insight here is that this ratio can be estimated from the output of a binary classifier [81] . Let us ascribe samples <span class="math inline">\(( z , x ) \sim q_{ \theta }( z \mid x ) p( x )\)</span> to the first class, and samples <span class="math inline">\(( z , x ) \sim p( z ) p( x )\)</span> to the second class. A binary classifier <span class="math inline">\(d_{\phi}\)</span> parameterized by <span class="math inline">\(\phi\)</span> outputs the probability <span class="math inline">\(d_{\phi}(z, x)\)</span> that the pair <span class="math inline">\((z, x)\)</span> belongs to one of two classes. Hence, <span class="math inline">\(1-d_{\phi}( z , x )\)</span> indicates the probability that <span class="math inline">\(( z , x )\)</span> belongs to the other class. There exist many possible choices of objective function for the classifier [81]. In this work we consider the cross-entropy: <span class="math display">\[
\begin{aligned}
G _{ KL }( \phi ; \theta ) &amp;= E _{ x \sim p( x )} E _{ z \sim q_{ \theta }( z \mid x )}\left[\log d_{ \phi }( z , x )\right] \\
&amp;+ E _{ x \sim p( x )} E _{ z \sim p( z )}\left[\log \left(1-d_{ \phi }( z , x )\right)\right]
\end{aligned}
\]</span> The optimal classifier which maximizes this equation is [93]: <span class="math display">\[
d^{*}( z , x )=\frac{q_{ \theta }( z \mid x )}{q_{ \theta }( z \mid x )+p( z )}
\]</span> Since the probabilities in Eq. 5 are unknown, the classifier must be trained on a dataset of samples. This does not pose a problem because samples from the Born machine <span class="math inline">\(q_{ \theta }( z \mid x )\)</span> and prior <span class="math inline">\(p( z )\)</span> are easy to obtain by assumption. Once the classifier is trained, the logit transformation provides the log-odds of a data point coming from the Born machine joint <span class="math inline">\(q_{ \theta }( z \mid x ) p( x )\)</span> vs the prior joint <span class="math inline">\(p( z ) p( x )\)</span>. The log-odds are an approximation to the log-ratio of the two distributions: <span class="math display">\[
\operatorname{logit}\left(d_{\phi}(z, x)\right) \equiv \log \frac{d_{\phi}(z, x)}{1-d_{\phi}(z, x)} \approx \log \frac{q_{\theta}(z \mid x)}{p(z)}
\]</span> which is exact if <span class="math inline">\(d_{\phi}\)</span> is the optimal classifier in Eq. (5). Now, we can avoid the computation of the problematic term in the KL divergence. Plugging this result in Eq. (3) and ignoring the constant terms, the final objective for the Born machine is: <span class="math display">\[
\begin{array}{r} 
L _{ KL }( \theta ; \phi )= E _{ x \sim p( x )} E _{ z \sim q_{ \theta }( z \mid x )}\left[\operatorname{logit}\left(d_{ \phi }( z , x )\right)\right. \\
-\log p( x \mid z )]
\end{array}
\]</span> The optimization can be performed in tandem as: <span class="math display">\[
\begin{array}{c}
\max _{\phi} G _{ KL }( \phi ; \theta ) \\
\min _{ \theta } L _{ KL }( \theta ; \phi ),
\end{array}
\]</span> using gradient ascent and descent, respectively. It can be shown [93,94] that the gradient of <span class="math inline">\(\log \frac{q_{\theta}(z \mid x)}{p(z)}\)</span> with respect to <span class="math inline">\(\theta\)</span> vanishes. Thus, under the assumption of an optimal classifier <span class="math inline">\(d_{\phi}\)</span>, the gradient of Eq. (7) is significantly simplified. The gradient of Eq. (8) is derived in Appendix A.</p>
<p>A more intuitive interpretation of the procedure just described is as follows. The log-likelihood in Eq. (3) can be expanded as <span class="math inline">\(\log p( x \mid z )=\log \frac{p( z \mid x )}{p( z )}+\)</span> <span class="math inline">\(\log p( x )\)</span>. Then Eq. (3) can be rewritten as <span class="math inline">\(E _{ x \sim p( x )} E _{ z \sim q_{ \theta }( z \mid x )}\left[\log \frac{q_{\theta}( z \mid x )}{p( z )}-\log \frac{p( z \mid x )}{p( z )}\right] . \quad\)</span> Comparing the expression in the square brackets with Eq. (6) reveals that the difference is between two log-odds: the first is given by an optimal classifier for the approximate posterior and prior, and the second is given by a hypothetical classifier for the true posterior and prior. The adversarial method is illustrated in Fig. <span class="math inline">\(3 .\)</span></p>
<h3 id="b.-quad-the-kernelized-method">B. <span class="math inline">\(\quad\)</span> The kernelized method</h3>
<p>Another possible objective function for VI is the Stein discrepancy (SD) of the true posterior from the approximate one. This is obtained from Eq. (1) assuming that the image of <span class="math inline">\(f\)</span> has the same dimension as <span class="math inline">\(z\)</span>, choosing <span class="math inline">\(h\)</span> to be the absolute value, and choosing <span class="math inline">\(O^{p, q}\)</span> to be a Stein operator. A Stein operator is independent from <span class="math inline">\(q\)</span> and is characterized by having zero expectation under the true posterior for all functions <span class="math inline">\(f\)</span> in the chosen family <span class="math inline">\(F\)</span>. Then, for binary variables a possible Stein operator is <span class="math inline">\(\left(O^{p} f\right)( z )=s_{p}( x , z )^{T} f( z )-\operatorname{tr}(\Delta f( z ))\)</span> where: <span class="math display">\[
\left(s_{p}( x , z )\right)_{i}=1-\frac{p\left( x , \neg_{i} z \right)}{p( x , z )},
\]</span> is the difference score function, <span class="math display">\[
(\Delta f( z ))_{i j}=(f( z ))_{j}-\left(f\left(\neg_{i} z \right)\right)_{j},
\]</span> is the partial difference operator, and <span class="math inline">\(\neg_{i} z\)</span> flips the <span class="math inline">\(i\)</span> -th bit in binary vector <span class="math inline">\(z\)</span>. We show in Appendix B that this is a valid Stein operator for binary variables. For more general discrete variables we refer the interested reader to <span class="math inline">\([95] .\)</span> Plugging these definitions in Eq. (1) we obtain: <span class="math display">\[
E _{ x \sim p( x )} \underbrace{\sup _{f \in F }\left| E _{ z \sim q_{ \theta }( z \mid x )}\left[s_{p}( x , z )^{T} f( z )-\operatorname{tr}(\Delta f( z ))\right]\right|}_{\operatorname{SD}\left[q_{ \theta }( z \mid x ) \| p( z \mid x )\right]}
\]</span> At this point one can parameterize the test function <span class="math inline">\(f\)</span> and obtain an adversarial objective similar in spirit to the one presented in Section IVA. Here, however, we take a different route. By restricting the Hilbert space norm of <span class="math inline">\(f\)</span> to be at most <span class="math inline">\(1,\)</span> the supremum in Eq. (11) can be calculated in closed form using a kernel [95]. The result is the kernelized Stein discrepancy (KSD): <span class="math display">\[
\operatorname{KSD}\left[q_{ \theta }( z \mid x ) \| p( z \mid x )\right]=\sqrt{ E _{ z , z ^{\prime} \sim q_{ \theta }( z \mid x )}\left[\kappa_{p}\left( z , z ^{\prime}\right)\right]}
\]</span> where <span class="math inline">\(\kappa_{p}\)</span> is a Stein kernel. For binary variables this can be written as: <span class="math display">\[
\begin{array}{r}
\kappa_{p}\left( z , z ^{\prime}\right)=s_{p}( x , z )^{T} k\left( z , z ^{\prime}\right) s_{p}\left( x ^{\prime}, z ^{\prime}\right) \\
\qquad \begin{aligned}
-s_{p}( x , z )^{T} \Delta_{ z ^{\prime}} k\left( z , z ^{\prime}\right)-&amp; \Delta_{ z } k\left( z , z ^{\prime}\right)^{T} s_{p}\left( x ^{\prime}, z ^{\prime}\right) \\
+&amp; \operatorname{tr}\left(\Delta_{ z , z ^{\prime}} k\left( z , z ^{\prime}\right)\right)
\end{aligned}
\end{array}
\]</span> The Stein kernel, <span class="math inline">\(\kappa,\)</span> depends on another kernel <span class="math inline">\(k\)</span>. For <span class="math inline">\(n\)</span> unobserved Bernoulli variables one possibility is the generic Hamming kernel <span class="math inline">\(k\left( z , z ^{\prime}\right)=\exp \left(-\frac{1}{n}\left\| z - z ^{\prime}\right\|_{1}\right)\)</span>. The KSD is a valid discrepancy measure if this 'internal' kernel, <span class="math inline">\(k\)</span>, is positive definite which is the case for the Hamming kernel [95]. In summary, constraining <span class="math inline">\(\|f\|_{ H } \leq 1\)</span> in Eq. (11) and substituting the KSD we obtain: <span class="math display">\[
L _{ KSD }( \theta )= E _{ x \sim p( x )} \sqrt{ E _{ z , z ^{\prime} \sim q_{\theta}( z \mid x )}\left[\kappa_{p}\left( z , z ^{\prime}\right)\right]}
\]</span> and the problem consists of finding <span class="math inline">\(\min _{ \theta } L _{ KSD }( \theta ) .\)</span> The gradient of Eq. (14) is derived in Appendix C. The kernelized method is illustrated in Fig. <span class="math inline">\(4 .\)</span></p>
<figure>
<img src="http://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210403141949928.png" srcset="/img/loading.gif" alt="" /><figcaption>image-20210403141949928</figcaption>
</figure>
<blockquote>
<p>Figure 4: Kernelized Stein variational inference with a Born machine. This method minimizes the kernelized Stein discrepancy between the approximate and true posterior distributions. First, both distributions are embedded in a functional Hilbert space <span class="math inline">\(H\)</span>. The map <span class="math inline">\(\kappa_{p}\)</span> is designed so that elements mapped from the unknown true posterior evaluate to zero. Second, the discrepancy is evaluated using samples from the Born machine <span class="math inline">\(q_{ \theta }( z \mid x )\)</span> and optimized by gradient descent.</p>
</blockquote>
<p>The KSD was used in [40] for generative modeling with Born machines. In that context the distribution <span class="math inline">\(p\)</span> is unknown, thus the authors derived methods to approximate the score <span class="math inline">\(s_{p}\)</span> from available data. VI is a suitable application of the KSD because in this context we do know the joint <span class="math inline">\(p( z , x )\)</span>. The Stein kernel can be efficiently computed even if the joint is unnormalized, as the normalization cancels in Eq. (9). Thus the evaluation of the true posterior is avoided.</p>
<h2 id="实验">实验</h2>
<p>To demonstrate our approach, we employ three experiments as the subject of the following sections. First, we validate both methods using the canonical 'sprinkler' Bayesian network. Second, we extend the regime of applications by considering continuous observed variables in a hidden Markov model (HMM), and illustrate how multiple observations can be incorporated effectively via amortization. Finally, we consider a larger discrete model in the 'lung cancer' Bayesian network and demonstrate the methods using an IBM quantum computer.</p>
<h3 id="a.-proof-of-principle-with-the-sprinkler-bayesian-network">A. Proof of principle with the 'sprinkler' Bayesian network</h3>
<p>As a first experiment we classically simulate the methods on the 'sprinkler' network in Fig. 1a, one of the simplest possible examples. The purpose is to show that both methods are expected to work well even when using limited resources and without much fine-tuning. First, we randomly generate the entries of each probability table from the uniform distribution <span class="math inline">\(U ([0.01,0.99])\)</span> and produced a total of 30 instances of this network. For each instance we condition on 'Grass wet' being true which means our empirical data distribution becomes <span class="math inline">\(p(W)=\delta(W=\operatorname{tr}) .\)</span> We infer the posterior of the remaining variables using a Born machine on 3 qubits and with the hardware-efficient ansatz shown in Fig. 5 . We use a layer of Hadamard gates as a state preparation, <span class="math inline">\(S( x )=H \otimes H \otimes H,\)</span> and initialized all parameters to <span class="math inline">\(\approx 0\)</span> Thus the initial distribution is approximately uniform over all bit-strings and does not make any assumption. Such hardware-efficient ansätze, while simple, have been shown to be vulnerable to barren plateaus <span class="math inline">\([97-100]\)</span>, or regions of exponentially vanishing gradient magnitudes which makes training untenable. Alternatively, one could consider ansätze which have been shown to be somewhat 'immune' to this phenomenon [101, 102], but we leave such investigation to future work.</p>
<p>For the KL objective, we utilize a multi-layer perceptron (MLP) classifier made of 3 input units, 6 ReLU hidden units and 1 sigmoid output unit. The classifier is trained with a dataset of 100 samples from the prior <span class="math inline">\(p(C, R, S)\)</span> and 100 samples from the Born machine <span class="math inline">\(q(C, R, S \mid W=\)</span> tr <span class="math inline">\()\)</span>. Here we use stochastic gradient descent with batches of size 10 and a learning rate of 0.03. For the KSD objective, we use the Hamming kernel as above. For both cases, we compute the total variation distance (TVD) of true and approximate posterior at each epoch. We emphasize that TVD cannot be efficiently computed in general and can be shown only for small examples. Figure 6a (Figure 6b) shows the median TVD out of the 30 instances for 1000 epochs of training for the KL (KSD) objective respectively. For both methods, the Born machine is trained using 100 samples to estimate each expectation value for the gradients, and using vanilla gradient descent with a small learning rate of 0.003 . The 0-layers Born machine generates unentangled states and it can be thought of as a classical mean-field approximation. Increasing number of layers lead to better approximations to the posterior in all cases.</p>
<p>Qualitatively, KL tends to converge to slightly better approximate posteriors, but requires more memory and computation than KSD. This is because of the MLP classifier which is trained along the Born machine. In Appendix <span class="math inline">\(E\)</span> we provide additional results to supplement those in Fig. 6 . We do not attempt to compare the two methods quantitatively as it would require the fine-tuning of a large number of hyper-parameters and we leave this study for future work.</p>
<h3 id="b.-continuous-observed-variables-and-amortization-with-a-hidden-markov-model">B. Continuous observed variables and amortization with a hidden Markov model</h3>
<p>In our second experiment, we simulate the adversarial VI method on the hidden Markov model (HMM) in Fig. 1b. The purpose was to demonstrate two interesting features: continuous observed variables and amortization. We set up the HMM for <span class="math inline">\(T=8\)</span> time steps (white circles in Fig. <span class="math inline">\(1 b\)</span> ), each represented by a Bernoulli latent variable with conditional dependency: <span class="math display">\[
\begin{array}{l}
z_{1} \sim B \left(\frac{1}{2}\right), \\
z_{t} \sim\left\{\begin{array}{ll} 
B \left(\frac{1}{3}\right) &amp; \text { if } z_{t-1}=0 \\
B \left(\frac{2}{3}\right) &amp; \text { if } z_{t-1}=1
\end{array}\right.
\end{array}
\]</span> These represent the unknown 'regime' at time <span class="math inline">\(t\)</span>. The regime affects how the observable data is generated. We use Gaussian observed variables (green circles in Fig. <span class="math inline">\(1 b\)</span> ) whose mean and standard deviation depends on the latent variables as: <span class="math display">\[
x_{t} \sim\left\{\begin{array}{ll} 
N (0,1) &amp; \text { if } z_{t}=0 \\
N \left(1, \frac{1}{2}\right) &amp; \text { if } z_{t}=1
\end{array}\right.
\]</span> We sample two time series observations <span class="math inline">\(x ^{(1)}, x ^{(2)}\)</span> from the HMM and take this to be our empirical data distribution. These time series are shown by green dots in Fig. 7. Instead of fitting two approximate posteriors separately, we use a single Born machine with amortization <span class="math inline">\(|\psi( \theta , x )\rangle\)</span>. We used the ansatz in Fig. 5 for 8 qubits with a state preparation layer <span class="math inline">\(S( x )=\otimes_{t=1}^{8} R_{x}\left(x_{t}\right)\)</span> In practice, this encoding step can be done under more careful considerations <span class="math inline">\([38,103] .\)</span> Parameters <span class="math inline">\(\theta\)</span> are initialized to small values at random. We optimize the KL objective and a MLP classifier with 16 inputs, 24 ReLU, and 1 sigmoid units and the system is trained for 3000 epochs. The learning rates are set to be 0.006 for the Born machine and 0.03 for the MLP respectively. The Born machine used 100 samples to estimate each expectation value, the MLP used 100 samples from each distribution and mini-batches of size 10 .</p>
<p>The histograms in Fig. 7 show the 10 most probable configurations of latent variables for the true posterior along with the probabilities assigned by the Born machine. Conditioning on datapoint <span class="math inline">\(x ^{(1)},\)</span> the inferred most likely explanation is |01100011(i.e. the mode of the Born machine). This coincides with the true posterior mode. For <span class="math inline">\(x ^{(2)},\)</span> the inferred mode was <span class="math inline">\(|10001000\rangle,\)</span> which differs from the true posterior mode |10000000by a single bit. The regime switching has therefore been modeled with high accuracy.</p>
<p>Rather than focusing on the mode, one can make use of the whole distribution to estimate some quantity of interest. This is done by taking samples from the Born machine and using them in a Monte Carlo estimate for such a quantity. For example, we could predict the expected value of the next latent variable <span class="math inline">\(z_{T+1}\)</span> given the available observations. For datapoint <span class="math inline">\(x ^{(1)}\)</span> this entails the estimation of: <span class="math display">\[
E _{z_{T} \sim q_{\theta}\left(z_{T} \mid x ^{(1)}\right)} E _{z_{T+1} \sim p\left(z_{T+1} \mid z_{T}\right)}\left[z_{T+1}\right]
\]</span> To conclude, we mention that inference in this HMM can be performed exactly and efficiently with classical algorithms. The most likely explanation for the observed data can be found with the Viterbi algorithm in time <span class="math inline">\(O\left(T|S|^{2}\right)\)</span> where <span class="math inline">\(T\)</span> is the length of the sequence and <span class="math inline">\(|S|\)</span> is the size of the unobserved variables (e.g. <span class="math inline">\(|S|=2\)</span> for Bernoulli variables). This is not the case for more general models. For example, a factorial HMM has multiple independent chains of unobserved variables [104]. Exact inference costs <span class="math inline">\(O\left(T M|S|^{M+1}\right)\)</span> where <span class="math inline">\(M\)</span> is number of chains, and is typically replaced by approximate inference <span class="math inline">\([104,105] .\)</span> Our VI methods are generic and apply to factorial and other HMMs without changes.</p>
<h3 id="c.-demonstration-on-ibmq-with-the-lung-cancer-bayesian-network">C. Demonstration on IBMQ with the 'lung cancer' Bayesian network</h3>
<p>As a final experiment, we test the performance on real quantum hardware using the slightly more complex 'lung cancer' network [47], specifically using the 5 qubit ibmq_rome quantum processor. We access this device using PyQuil [106] and tket [107]</p>
<p>The lung cancer network (also called the 'Asia' network) is an example of a medical diagnosis Bayesian network, and is illustrated by Fig. <span class="math inline">\(1 c .\)</span> This network was chosen since it is small enough to fit without adaptation onto the quantum device, but also comes closer to a 'real-world' example. A more complicated version with extra variables can be found in <span class="math inline">\([108] .\)</span> We note that the version we use is slightly modified from that of [47] for numerical reasons.</p>
<p>The network has 8 possible nodes: two 'symptoms', whether the patient presented with dyspnoea (D) (shortness of breath), or had a positive X-ray <span class="math inline">\(( X ) ; 4\)</span> possible 'diseases' causing the symptoms: bronchitis (B), tuberculosis (T), lung cancer (L), or an 'illness" (I) (which</p>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Quantum-Computation/">Quantum Computation</a>
                    
                      <a class="hover-with-bg" href="/categories/Quantum-Computation/%E9%87%8F%E5%AD%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">量子机器学习</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/AI/">AI</a>
                    
                      <a class="hover-with-bg" href="/tags/Quantum-Computation/">Quantum Computation</a>
                    
                      <a class="hover-with-bg" href="/tags/%E9%87%8F%E5%AD%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">量子机器学习</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2021/04/06/Cold_03_Basic03_SOC/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">自旋轨道耦合</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2021/03/30/Entenglement_04_Percolation7_CFT/">
                        <span class="hidden-mobile">纠缠相变 在张量网络，渗流等统计系统中</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div id="tocbot"></div>
</div>


      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>







  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 1,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "变分推理&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js" ></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  
















</body>
</html>
