<!DOCTYPE html>
<html lang="en">





<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="JPZhuang">
  <meta name="keywords" content="">
  <title>放码过来之 Michael Nielsen《神经网络与深度学习》 - JPZ</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
<meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Physics</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner intro-2" id="background" parallax=true
         style="background: url('/img/tag-bg.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fade-in-up">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2021-04-10 13:28">
      April 10, 2021 pm
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      4.4k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      56
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <p>[TOC]</p>
<h2 id="参考文献">参考文献</h2>
<ol type="1">
<li><p>Michael Nielsen《神经网络与深度学习》</p></li>
<li><p><a href="https://github.com/mnielsen/neural-networks-and-deep-learning" target="_blank" rel="noopener">代码</a></p></li>
</ol>
<h2 id="概览">概览</h2>
<ol type="1">
<li>使用神经网络识别手写数字</li>
<li>反向传播算法如何⼯作</li>
<li>改进神经网络的学习方法</li>
<li>卷积神经网络</li>
</ol>
<h2 id="代码1-使用神经网络识别手写数字">代码1 使用神经网络识别手写数字</h2>
<h3 id="数据集">数据集</h3>
<p>MNIST 数据分为两个部分。</p>
<ol type="1">
<li>第一部分包含 60,000 幅用于训练数据的图像。这些图像扫描自 250 人的手写样本，他们中一半人是美国人口普查局的员工，一半人是高校学生。这些图像是 <span class="math inline">\(28 \times 28\)</span> 大小的灰度图像。</li>
<li>第二部分是 10,000 幅用于测试数据的图像，同样是 <span class="math inline">\(28 \times 28\)</span> 的灰度图</li>
</ol>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210421123910931.png" srcset="/img/loading.gif" /></p>
<ul>
<li><span class="math inline">\(28 \times 28\)</span> 的手写数字的图像组成
<ul>
<li>所有输入层包含有 <span class="math inline">\(784=28 \times 28\)</span></li>
<li>值为 <span class="math inline">\(0.0\)</span> 表示白色，值为 <span class="math inline">\(1.0\)</span> 表示黑色，中间数值表示逐渐暗淡的灰色</li>
</ul></li>
<li>10 个输出神经元表示数字 <span class="math inline">\((0,1,2, \ldots, 9)\)</span>
<ul>
<li>如果第一个神经元激活，即输出 <span class="math inline">\(\approx 1\)</span>, 那么表明网络认为</li>
</ul></li>
</ul>
<blockquote>
<p>四个神经元足够编码，为什么我们反而要用 10 个神经元呢?</p>
</blockquote>
<h3 id="使用梯度下降算法进行学习">使用梯度下降算法进行学习</h3>
<p>我们希望有一个算法，能让我们找到权重和偏置，以至于网络的输出 <span class="math inline">\(y(x)\)</span> 能够拟合所有的训练输入 $x $</p>
<p>例如：如果有一个特定的画成 6 的训练图像，x，那么<span class="math inline">\(y(x)=(0,0,0,0,0,0,1,0,0,0)^{T}\)</span> 则是网络的期望输出。</p>
<p>代价函数 <span class="math display">\[
C(w, b) \equiv \frac{1}{2 n} \sum_{x}\|y(x)-a\|^{2}
\]</span></p>
<ol type="1">
<li>一种解决这个问题的方式是用微积分来解析最小值。我们可以计算导数去寻找 <span class="math inline">\(C\)</span> 的极值点。但是变量过多的话那就是哥梦。</li>
<li>首先把我们的函数想象成一个山谷。那么我们将会采取什么样的运动学定律来让球体能句总是滚落到谷底呢?</li>
</ol>
<p>当我们在 <span class="math inline">\(v 1\)</span> 和 <span class="math inline">\(v 2\)</span> 方向分别将球体移动一个很小的量，即 <span class="math inline">\(\Delta v 1\)</span> 和 <span class="math inline">\(\Delta v 2\)</span> 时，微积分告诉我们 <span class="math inline">\(C\)</span> 将会有如下变化： <span class="math display">\[
\Delta C \approx \frac{\partial C}{\partial v_{1}} \Delta v_{1}+\frac{\partial C}{\partial v_{2}} \Delta v_{2}
\]</span> 向量化</p>
<ul>
<li><span class="math inline">\(v\)</span> 变化的向量 <span class="math inline">\(\Delta v \equiv\left(\Delta v_{1}, \Delta v_{2}\right)^{T}\)</span></li>
<li><span class="math inline">\(C\)</span> 的梯度为偏导数的向量，<span class="math inline">\(\nabla C \equiv\left(\frac{\partial C}{\partial v_{1}}, \frac{\partial C}{\partial v_{2}}\right)^{T}\)</span></li>
</ul>
<p>那么 <span class="math display">\[
\begin{aligned}
\Delta C &amp;\approx \frac{\partial C}{\partial v_{1}} \Delta v_{1}+\frac{\partial C}{\partial v_{2}} \Delta v_{2}
\\&amp;\approx \nabla C \cdot \Delta v
\end{aligned}
\]</span> 如何选取 <span class="math inline">\(\Delta v\)</span> 才能让<span class="math inline">\(\Delta C\)</span> 为负数。假设我们选取： <span class="math display">\[
\Delta v=-\eta \nabla C
\]</span> 其中</p>
<ul>
<li><span class="math inline">\(\eta\)</span> 是个很小的正数 (称为学习速率)</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
\Delta C &amp;\approx \frac{\partial C}{\partial v_{1}} \Delta v_{1}+\frac{\partial C}{\partial v_{2}} \Delta v_{2}
\\&amp; = \nabla C \cdot \Delta v
\\&amp; =-\eta \nabla C \cdot \nabla C
\\&amp; =-\eta\|\nabla C\|^{2}
\end{aligned}
\]</span></p>
<p>由于 <span class="math inline">\(\|\nabla C\|^{2} \geq 0\)</span>, 这保证了 <span class="math inline">\(\Delta C \leq 0\)</span></p>
<p>定义球体在梯度下降算法下的 “运动定律”。 <span class="math display">\[
v \rightarrow v^{\prime}=v-\eta \nabla C
\]</span> 用它再次更新规则来计算下一次移动</p>
<h4 id="多变量">多变量</h4>
<p>如果 <span class="math inline">\(C\)</span> 是一个有 <span class="math inline">\(m\)</span> 个变量 <span class="math inline">\(v_{1}, \ldots, v_{m}\)</span> 的多元函数。</p>
<ul>
<li>对 <span class="math inline">\(C\)</span> 中自变量的变化 <span class="math inline">\(\Delta v=\left(\Delta v_{1}, \ldots, \Delta v_{m}\right)^{T}\)</span></li>
<li>梯度 <span class="math inline">\(\nabla C \equiv\left(\frac{\partial C}{\partial v_{1}}, \ldots, \frac{\partial C}{\partial v_{m}}\right)^{T}\)</span></li>
</ul>
<p><span class="math inline">\(C\)</span> 的变化 <span class="math display">\[
\Delta C \approx \nabla C \cdot \Delta v
\]</span></p>
<blockquote>
<p>怎么在神经网络中用梯度下降算法去学习呢?</p>
</blockquote>
<p>权重 <span class="math inline">\(w_{k}\)</span> 和偏置 <span class="math inline">\(b_{l}\)</span> <span class="math display">\[
\begin{aligned}
w_{k} \rightarrow w_{k}^{\prime} &amp;=w_{k}-\eta \frac{\partial C}{\partial w_{k}} \\
b_{l} \rightarrow b_{l}^{\prime} &amp;=b_{l}-\eta \frac{\partial C}{\partial b_{l}}
\end{aligned}
\]</span></p>
<h4 id="随机梯度下降">随机梯度下降</h4>
<p>当训练输入的数量过大时会花费很长时间，通过随机选取小量训练输入样本来计算 <span class="math inline">\(\nabla C_{x}\)</span>, 进而估算梯度 <span class="math inline">\(\nabla C\)</span>。</p>
<ol type="1">
<li>随机梯度下降通过随机选取小量的 <span class="math inline">\(m\)</span> 个训练输入来工作。我们将这些随机的训练输入标记为 <span class="math inline">\(X_{1}, X_{2}, \ldots, X_{m}\)</span>, 并把它们称为一个小批量数据（mini-batch）。</li>
<li>假设样本数量 <span class="math inline">\(m\)</span> 足够大，我们期望 <span class="math inline">\(\nabla C_{X_{j}}\)</span> 的平均值大致相等于整个 <span class="math inline">\(\nabla C_{x}\)</span> 的平均值，即,</li>
</ol>
<p><span class="math display">\[
\frac{\sum_{j=1}^{m} \nabla C_{X_{j}}}{m} \approx \frac{\sum_{x} \nabla C_{x}}{n}=\nabla C
\]</span></p>
<ol start="3" type="1">
<li>在神经网络中的权重更迭</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
w_{k} \rightarrow w_{k}^{\prime} &amp;=w_{k}-\frac{\eta}{m} \sum_{j} \frac{\partial C_{X_{j}}}{\partial w_{k}} \\
b_{l} \rightarrow b_{l}^{\prime} &amp;=b_{l}-\frac{\eta}{m} \sum_{j} \frac{\partial C_{X_{j}}}{\partial b_{l}}
\end{aligned}
\]</span></p>
<ol start="4" type="1">
<li>然后我们再挑选另一随 机选定的小批量数据去训练。直到我们用完了所有的训练输入，这被称为完成了一个训练迭代期 (epoch) 。</li>
<li>然后我们就会开始一个新的训练迭代期。</li>
</ol>
<p>注意点</p>
<ul>
<li>有时候会忽略 <span class="math inline">\(C(w, b) \equiv \frac{1}{2 n} \sum_{x}\|y(x)-a\|^{2}\)</span> 的 <span class="math inline">\(\frac{1}{n}\)</span>
<ul>
<li>这对我们不能提前知道训练数据数量的情况下特别有效。</li>
</ul></li>
<li>忽略 <span class="math inline">\(\frac{1}{n}\)</span> 等价于改变了学习速率 <span class="math inline">\(\eta\)</span> 的大小。但在对不同工作进行详细对比时，需要对它警楊。</li>
</ul>
<h4 id="梯度下降的缺点">梯度下降的缺点</h4>
<p>主要的缺点：它最终必需去计算 <span class="math inline">\(C\)</span> 的二阶偏导，这代价可是非常大的。</p>
<ul>
<li>假设我们想求所有的二阶偏导 $^{2} C / v_{j} v_{k} $</li>
<li>如果我们有上百万的变量 <span class="math inline">\(v_{j}\)</span>, 那我们必须要计算数万亿级别的二阶偏导</li>
</ul>
<h3 id="先跑起来">先跑起来</h3>
<pre><code class="hljs python"><span class="hljs-string">""" 加载MNIST 数据 """</span> 
<span class="hljs-keyword">import</span> mnist_loader
<span class="hljs-string">""" 设置⼀个有30 个隐藏层神经元的Network """</span> 
<span class="hljs-keyword">import</span> network
net = network.Network([<span class="hljs-number">784</span>, <span class="hljs-number">30</span>, <span class="hljs-number">10</span>])
<span class="hljs-string">""" 使⽤随机梯度下降来从MNIST training_data 学习</span>
<span class="hljs-string">30 次迭代期，</span>
<span class="hljs-string">⼩批量数据⼤⼩为10，</span>
<span class="hljs-string">学习速率 0.3 """</span> 

net.SGD(training_data, <span class="hljs-number">30</span>, <span class="hljs-number">10</span>, <span class="hljs-number">3.0</span>, test_data=test_data)</code></pre>
<p>结构</p>
<ol type="1">
<li><code>net = network.Network([784, 30, 10])</code></li>
<li><code>net.SGD(training_data, 30, 10, 3.0, test_data=test_data)</code>
<ul>
<li><code>update_mini_batch(mini_batch, eta)</code>
<ul>
<li><code>delta_nabla_b, delta_nabla_w = self.backprop(x, y)</code></li>
</ul></li>
</ul></li>
</ol>
<p>解读</p>
<ol type="1">
<li><code>net = network.Network([784, 30, 10])</code> 设置网络结构</li>
<li><code>net.SGD(training_data, 30, 10, 3.0, test_data=test_data)</code> 用SDG法学习
<ul>
<li><code>update_mini_batch(mini_batch, eta)</code> 单次梯度下降的迭代更新网络的权重和偏置
<ul>
<li><code>delta_nabla_b, delta_nabla_w = self.backprop(x, y)</code> 为反向传播的算法，快速计算代价函数的梯度</li>
</ul></li>
</ul></li>
</ol>
<p>输出</p>
<pre><code class="hljs python">Epoch <span class="hljs-number">0</span>: <span class="hljs-number">9129</span> / <span class="hljs-number">10000</span>
Epoch <span class="hljs-number">1</span>: <span class="hljs-number">9295</span> / <span class="hljs-number">10000</span>
Epoch <span class="hljs-number">2</span>: <span class="hljs-number">9348</span> / <span class="hljs-number">10000</span>
...
Epoch <span class="hljs-number">27</span>: <span class="hljs-number">9528</span> / <span class="hljs-number">10000</span>
Epoch <span class="hljs-number">28</span>: <span class="hljs-number">9542</span> / <span class="hljs-number">10000</span>
Epoch <span class="hljs-number">29</span>: <span class="hljs-number">9534</span> / <span class="hljs-number">10000</span></code></pre>
<ol type="1">
<li>在仅仅一次迭代期后，达到了10,000 中选中的9,129 个。</li>
<li>而且数目还在持续增长</li>
<li>识别率在峰值时为 95.42% (“Epoch 28")</li>
</ol>
<h3 id="神经网络代码的核心">神经网络代码的核心</h3>
<p>Network 类，我们用来表示一个神经网络</p>
<pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Network</span><span class="hljs-params">(object)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, sizes)</span>:</span>
    self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> sizes[<span class="hljs-number">1</span>:]]
        self.weights = [np.random.randn(y, x)
                        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(sizes[:<span class="hljs-number">-1</span>], sizes[<span class="hljs-number">1</span>:])]</code></pre>
<h4 id="代码解读怎么使用">代码解读：怎么使用</h4>
<p>在这段代码中，列表 sizes 包含各层神经元的数量。例如，如果我们想创建一个Network 对象</p>
<ul>
<li>在第一层有 3 个神经元，</li>
<li>第二层有 4 个神经元，</li>
<li>最后层有 1 个神经元</li>
</ul>
<p>我们应这样写代码：</p>
<pre><code class="hljs python">net = Network([<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">1</span>])
    test_data=(<span class="hljs-literal">None</span>):</code></pre>
<h4 id="代码解读初始化">代码解读：初始化</h4>
<p>Network 对象中的偏置和权重都是被随机初始化的，使用 Numpy 的 <code>np .random. randn</code> 函数来生成</p>
<ul>
<li>均值为 0，</li>
<li>标准差为 1</li>
<li>的高斯分布。</li>
</ul>
<h4 id="代码解读神经元权重的矩阵">代码解读：神经元权重的矩阵</h4>
<p>另外注意，偏置和权重以 Numpy 矩阵列表的形式存储。</p>
<ul>
<li>例如 net.weights[1] 是一个存储着连 接第二层和第三层神经元权重的 Numpy 矩阵。 <span class="math inline">\((\)</span> 不是第一层和第二层，因为 <span class="math inline">\(Python\)</span> 列表的索引 从 0 开始。 )</li>
<li>既然 net.weights [1] 相当冗长，让我们用 <span class="math inline">\(w\)</span> 表示矩阵。</li>
<li>矩阵的 <span class="math inline">\(w_{j k}\)</span> 是连接第二层的 <span class="math inline">\(k^{\text {th }}\)</span> 神经元和第三层的 <span class="math inline">\(j^{ th }\)</span> 神经元的权重。</li>
</ul>
<h3 id="定义函数">定义函数</h3>
<h4 id="定义s-型函数">定义S 型函数</h4>
<pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span><span class="hljs-params">(z)</span></span>
<span class="hljs-function">	<span class="hljs-title">return</span> 1.0/<span class="hljs-params">(<span class="hljs-number">1.0</span>+np.exp<span class="hljs-params">(-z)</span>)</span></span></code></pre>
<h4 id="feedforward">feedforward</h4>
<p>我们然后对 Network 类添加一个 feedforward 方法</p>
<pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">feedforward</span><span class="hljs-params">(self,a)</span></span>
        for b, w in zip(self.biases, self.weights):
            a = sigmoid(np.dot(w, a)+b)
        <span class="hljs-keyword">return</span> a</code></pre>
<p>对于网络给定一个输入 <span class="math inline">\(a\)</span>, 返回对应的输出 。这个方法所做的是对每一层应用方程： <span class="math display">\[
a^{\prime}=\sigma(w a+b)
\]</span></p>
<h4 id="代码解读zip函数">代码解读：zip函数</h4>
<p><code>zip</code></p>
<p><strong>zip()</strong> 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。</p>
<p>如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表。</p>
<pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>b = [<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>c = [<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>zipped = zip(a,b)     <span class="hljs-comment"># 打包为元组的列表</span>
[(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">5</span>), (<span class="hljs-number">3</span>, <span class="hljs-number">6</span>)]
<span class="hljs-meta">&gt;&gt;&gt; </span>zip(a,c)              <span class="hljs-comment"># 元素个数与最短的列表一致</span>
[(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">5</span>), (<span class="hljs-number">3</span>, <span class="hljs-number">6</span>)]
<span class="hljs-meta">&gt;&gt;&gt; </span>zip(*zipped)          <span class="hljs-comment"># 与 zip 相反，*zipped 可理解为解压，返回二维矩阵式</span>
[(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), (<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>)]</code></pre>
<h3 id="学习随即梯度下降">学习：随即梯度下降</h3>
<p>想要Network 对象做的主要事情是学习。为此我们给它们⼀个实现随即梯度下降算法的SGD ⽅法。</p>
<pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">SGD</span><span class="hljs-params">(self, training_data, epochs, mini_batch_size, eta,</span></span>
<span class="hljs-function"><span class="hljs-params">        test_data=None)</span>:</span>
    <span class="hljs-string">"""Train the neural network using mini-batch stochastic</span>
<span class="hljs-string">    gradient descent.  The ``training_data`` is a list of tuples</span>
<span class="hljs-string">    ``(x, y)`` representing the training inputs and the desired</span>
<span class="hljs-string">    outputs.  The other non-optional parameters are</span>
<span class="hljs-string">    self-explanatory.  If ``test_data`` is provided then the</span>
<span class="hljs-string">    network will be evaluated against the test data after each</span>
<span class="hljs-string">    epoch, and partial progress printed out.  This is useful for</span>
<span class="hljs-string">    tracking progress, but slows things down substantially."""</span>
    <span class="hljs-keyword">if</span> test_data: n_test = len(test_data)
    n = len(training_data)
    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> xrange(epochs):
        random.shuffle(training_data)
        mini_batches = [
            training_data[k:k+mini_batch_size]
            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> xrange(<span class="hljs-number">0</span>, n, mini_batch_size)]
        <span class="hljs-keyword">for</span> mini_batch <span class="hljs-keyword">in</span> mini_batches:
            self.update_mini_batch(mini_batch, eta)
        <span class="hljs-keyword">if</span> test_data:
            <span class="hljs-keyword">print</span> <span class="hljs-string">"Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(
                j, self.evaluate(test_data), n_test)
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">print</span> <span class="hljs-string">"Epoch &#123;0&#125; complete"</span>.format(j)</code></pre>
<p>代码功能</p>
<ol type="1">
<li>在每个迭代期，它首先随机地将训练数据打乱，</li>
<li>然后将它分成多个适当大小的小批量数据。这是一个简单的从训练数据的随机采样方法。</li>
<li>然后对于每一个 mini_batch 我们应用一次梯度下降。
<ul>
<li>这是通过代码 self.update_mini_batch(mini_batch, eta) 完成的，</li>
<li>它仅仅使用 mini_batch 中的训练数据，</li>
</ul></li>
<li>根据单次梯度下降的迭代更新网络的权重和偏置。</li>
</ol>
<p>这是 update_mini_batch 方法的代码：</p>
<h3 id="改进">改进</h3>
<h4 id="改进1-网络结构">改进1: 网络结构</h4>
<blockquote>
<p>重新运行上面的实验，将隐藏神经元数量改到 100</p>
</blockquote>
<pre><code class="hljs python">net = network.Network([<span class="hljs-number">784</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>])
net.SGD(training_data, <span class="hljs-number">30</span>, <span class="hljs-number">10</span>, <span class="hljs-number">3.0</span>, test_data=test_data)</code></pre>
<p>变化</p>
<ol type="1">
<li>执行时间更长</li>
<li>结果提升至 96.59 %</li>
</ol>
<h4 id="改进2超参数">改进2：超参数</h4>
<blockquote>
<p>学习速率改变为 <span class="math inline">\(\eta=0.01\)</span> 和 <span class="math inline">\(\eta=100.0:\)</span></p>
</blockquote>
<ul>
<li>训练的迭代期数量，</li>
<li>小批量数据大小</li>
<li>学习速率 <span class="math inline">\(\eta\)</span></li>
</ul>
<p>做特别的选择， 这些在我们的神经⽹络中被称为<strong>超参数</strong>。</p>
<p>相对而言，权重和偏置是学习算法所学到的参数</p>
<ul>
<li><span class="math inline">\(\eta=0.01\)</span> 准确率随着时间的推移慢慢地变好了，但是太慢了。</li>
</ul>
<pre><code class="hljs python">net.SGD(training_data, <span class="hljs-number">30</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.001</span>, test_data=test_data)

Epoch <span class="hljs-number">0</span>: <span class="hljs-number">1139</span> / <span class="hljs-number">10000</span>
Epoch <span class="hljs-number">1</span>: <span class="hljs-number">1136</span> / <span class="hljs-number">10000</span>
Epoch <span class="hljs-number">2</span>: <span class="hljs-number">1135</span> / <span class="hljs-number">10000</span>
...
Epoch <span class="hljs-number">27</span>: <span class="hljs-number">2101</span> / <span class="hljs-number">10000</span>
Epoch <span class="hljs-number">28</span>: <span class="hljs-number">2123</span> / <span class="hljs-number">10000</span>
Epoch <span class="hljs-number">29</span>: <span class="hljs-number">2142</span> / <span class="hljs-number">10000</span></code></pre>
<ul>
<li><span class="math inline">\(\eta=100.0\)</span> 学得很快，但是准确率太低，升不起来。</li>
</ul>
<pre><code class="hljs python">net.SGD(training_data, <span class="hljs-number">30</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100.0</span>, test_data=test_data)

Epoch <span class="hljs-number">0</span>: <span class="hljs-number">1009</span> / <span class="hljs-number">10000</span>
Epoch <span class="hljs-number">1</span>: <span class="hljs-number">1009</span> / <span class="hljs-number">10000</span>
Epoch <span class="hljs-number">2</span>: <span class="hljs-number">1009</span> / <span class="hljs-number">10000</span>
Epoch <span class="hljs-number">3</span>: <span class="hljs-number">1009</span> / <span class="hljs-number">10000</span>
...
Epoch <span class="hljs-number">27</span>: <span class="hljs-number">982</span> / <span class="hljs-number">10000</span>
Epoch <span class="hljs-number">28</span>: <span class="hljs-number">982</span> / <span class="hljs-number">10000</span>
Epoch <span class="hljs-number">29</span>: <span class="hljs-number">982</span> / <span class="hljs-number">10000</span></code></pre>
<p>对比</p>
<ul>
<li><span class="math inline">\(\eta=0.01\)</span> 准确率随着时间的推移慢慢地变好了，但是太慢了。</li>
<li><span class="math inline">\(\eta=100.0\)</span> 学得很快，但是准确率太低，升不起来。</li>
</ul>
<h2 id="反向传播">反向传播</h2>
<blockquote>
<p>反向传播的核心是一个对代价函数 <span class="math inline">\(C\)</span> 关于任何权重 <span class="math inline">\(w\)</span> (或者偏置 <span class="math inline">\(b)\)</span> 的偏 导数 <span class="math inline">\(\partial C / \partial w\)</span> 的表达式。</p>
</blockquote>
<p>这个表达式告诉我们在改变权重和偏置时，代价函数变化的快慢。</p>
<h2 id="代码2改进">代码2：改进</h2>
<ol type="1">
<li>代价函数的选择</li>
<li>“规范化"方法</li>
<li>更好的权重初始化方法</li>
<li>选择好的超参数</li>
</ol>
<h2 id="卷积神经网络">卷积神经网络</h2>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210423100725277.png" srcset="/img/loading.gif" /></p>
<h3 id="介绍">介绍</h3>
<p>卷积神经网络采用了三种基本概念：</p>
<ol type="1">
<li>局部感受野 （local receptive fields）</li>
<li>共享权重（shared weights）</li>
<li>混合（pooling）</li>
</ol>
<h4 id="局部感受野-local-receptive-fields">局部感受野 （local receptive fields）</h4>
<ol type="1">
<li><p>输入形式</p>
<ul>
<li><p>在之前看到的全连接层的网络中，输入被描绘成纵向排列的神经元。</p></li>
<li><p>但在一个 卷积网络中，把输入看作是一个 <span class="math inline">\(28 \times 28\)</span> 的方形排列的神经元更有帮助，</p></li>
<li><p>其值对应于我们用作输 入的 <span class="math inline">\(28 \times 28\)</span> 的像素光强度：</p></li>
</ul></li>
<li><p>局部连接</p>
<ul>
<li>和通常一样，我们把输入像素连接到一个隐藏神经元层。</li>
<li>但是我们不会把每个输入像素连接到每个隐藏神经元。相反，我们只是把输入图像进行小的，局部区域的连接。</li>
<li>例如一个 <span class="math inline">\(5 \times 5\)</span> 的区域，对应于 25 个输入像素。</li>
</ul></li>
</ol>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210421180612863.png" srcset="/img/loading.gif" style="zoom:50%;" /></p>
<ol start="3" type="1">
<li>隐藏层
<ul>
<li>一个 <span class="math inline">\(28 \times 28\)</span> 的输入图像， <span class="math inline">\(5 \times 5\)</span> 的局部感 受野，那么隐藏层中就会有 <span class="math inline">\(24 \times 24\)</span> 个神经元。这是因为在抵达右边（或者底部）的输入图像之</li>
</ul></li>
</ol>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210421181003267.png" srcset="/img/loading.gif" style="zoom:50%;" /></p>
<h4 id="共享权重shared-weights">共享权重（shared weights）</h4>
<p><span class="math inline">\(24 \times 24\)</span> 隐藏神经元中的每一个使用相同的权重和偏置。</p>
<p>对第 <span class="math inline">\(j, k\)</span> 个隐藏神经元，输出为： <span class="math display">\[
\sigma\left(b+\sum_{l=0}^{4} \sum_{m=0}^{4} w_{l, m} a_{j+l, k+m}\right)
\]</span> <span class="math inline">\(w_{l, m}\)</span> 是一个共享权重的 <span class="math inline">\(5 \times 5\)</span> 数组</p>
<p>从输入层到隐藏层的映射称为一个特征映射。</p>
<ul>
<li>共享权重和偏置经常被称为⼀个卷积核或者滤波器。</li>
<li>一个网络结构的卷积核只能检测一种局部特征的类型。</li>
<li>为了完成图像识别我们需要超过一个 的特征映射。所以一个完整的卷积层由几个不同的特征映射组成：</li>
</ul>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210421191650051.png" srcset="/img/loading.gif" /></p>
<ol type="1">
<li>为了让上面的图示简单些，我仅仅展示了 3 个特征映射</li>
<li>一种早期的识别 MNIST 数字的卷积网络，LeNet-5，使用 6 个 特征映射，每个关联到一个 <span class="math inline">\(5 \times 5\)</span> 的局部感受野。</li>
<li>本章后面要开发的例子里，我们将使用具有 20 和 40 个特征映射的卷积层。</li>
</ol>
<p>共享权重和偏置的一个很大的优点是，它大大减少了参与的卷积网络的参数。</p>
<h4 id="混合pooling">混合（pooling）</h4>
<p>混合层（pooling layers)紧接着在卷积层之后使用。它要做的是简化从卷积层输出的信息。</p>
<ol type="1">
<li>一个常见的混合的程序被称为最大值混合（max-pooling)
<ul>
<li>最大值混合看作一种网络询问是否有一个给定的特征在一个图像区域中的哪个地方被发现的方式</li>
</ul></li>
<li>L2 混合 (L2 pooling)
<ul>
<li>凝缩从卷积层输出的信息的方式。</li>
</ul></li>
</ol>
<h3 id="code">code</h3>
<blockquote>
<p>将卷积网络应用于 MNIST 数字分类问题</p>
</blockquote>
<ul>
<li>使用一个隐藏层，包含 100 个隐藏神经元</li>
<li>我们会训练 60 迭代期</li>
<li>使用学习速率为： <span class="math inline">\(\eta=0.1\)</span></li>
<li>小批量数据 大小为 10</li>
</ul>
<p>结果</p>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210423132225265.png" srcset="/img/loading.gif" style="zoom: 80%;" /></p>
<h4 id="改进更深的网络">改进：更深的网络</h4>
<p>有一个额外的全连接层：</p>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210423133242349.png" srcset="/img/loading.gif" /></p>
<pre><code class="hljs python">net = Network([
	ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>),
		filter_shape=(<span class="hljs-number">20</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>),
		poolsize=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)),
	FullyConnectedLayer(n_in=<span class="hljs-number">20</span>*<span class="hljs-number">12</span>*<span class="hljs-number">12</span>, n_out=<span class="hljs-number">100</span>),
		SoftmaxLayer(n_in=<span class="hljs-number">100</span>, n_out=<span class="hljs-number">10</span>)], mini_batch_size)</code></pre>
<ul>
<li><span class="math inline">\(98.78 \%\)</span> 的准确率，这是相当大的改善</li>
</ul>
<h4 id="改进第个卷积">改进：第⼆个卷积</h4>
<p>插入第二个卷积-混合层。把它插在已有的卷积-混合层和全连接隐藏层之间</p>
<pre><code class="hljs python">net = Network([
	ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>),
		filter_shape=(<span class="hljs-number">20</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>),
		poolsize=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)),
	ConvPoolLayer(image_shape=(mini_batch_size, <span class="hljs-number">20</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>),
		filter_shape=(<span class="hljs-number">40</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>),
		poolsize=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)),
	FullyConnectedLayer(n_in=<span class="hljs-number">40</span>*<span class="hljs-number">4</span>*<span class="hljs-number">4</span>, n_out=<span class="hljs-number">100</span>),
	SoftmaxLayer(n_in=<span class="hljs-number">100</span>, n_out=<span class="hljs-number">10</span>)], mini_batch_size)</code></pre>
<ul>
<li><span class="math inline">\(99.06 \%\)</span> 的分类准确率。</li>
</ul>
<h4 id="改进修改激活函数">改进：修改激活函数</h4>
<blockquote>
<p><span class="math inline">\(\tanh\)</span> 函数 <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(S\)</span> 型函数</p>
</blockquote>
<ul>
<li>tanh 网络训练得稍微快些，</li>
<li>但是最终的准确率非常相似</li>
</ul>
<h2 id="附录">附录</h2>
<h3 id="python中类和self">python中类和self</h3>
<p>面向对象最重要的概念就是类（class）和实例（instance），</p>
<ul>
<li>类是抽象的模板，比如学生这个抽象的事物，可以用一个Student类来表示。</li>
<li>而实例是根据类创建出来的一个个具体的“对象”，每一个对象都从类中继承有相同的方法，但各自的数据可能不同。</li>
</ul>
<p><a href="https://blog.csdn.net/CLHugh/article/details/75000104" target="_blank" rel="noopener">参考</a> 例子</p>
<ol type="1">
<li>以Student类为例，在Python中，定义类如下：</li>
</ol>
<pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Student</span><span class="hljs-params">(object)</span>:</span>
	<span class="hljs-keyword">pass</span></code></pre>
<ol start="2" type="1">
<li>实例：定义好了类，就可以通过Student类创建出Student的实例，创建实例是通过类名+()实现：</li>
</ol>
<pre><code class="hljs python">student = Student()</code></pre>
<ol start="3" type="1">
<li>由于类起到模板的作用，因此，可以在创建实例的时候，把我们认为必须绑定的属性强制填写进去。
<ul>
<li>这里就用到Python当中的一个内置方法<code>__init__</code>方法，</li>
<li>例如在Student类时，把name、score等属性绑上去:</li>
</ul></li>
</ol>
<pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Student</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, name, score)</span>:</span>
        self.name = name
        self.score = score</code></pre>
<ul>
<li>（1）<code>__init__</code>方法的第一参数永远是self，表示创建的类实例本身，因此，在__init__方法内部，就可以把各种属性绑定到self，因为self就指向创建的实例本身。</li>
<li>（2）有了<code>__init__</code>方法，在创建实例的时候，就不能传入空的参数了，必须传入与__init__方法匹配的参数，但self不需要传，Python解释器会自己把实例变量传进去：</li>
</ul>
<pre><code class="hljs python">&gt;&gt;&gt;student = Student(<span class="hljs-string">"Hugh"</span>, <span class="hljs-number">99</span>)
&gt;&gt;&gt;student.name
<span class="hljs-string">"Hugh"</span>
&gt;&gt;&gt;student.score
<span class="hljs-number">99</span></code></pre>
<ol type="1">
<li>这里self就是指类本身，</li>
<li>self.name就是Student类的属性变量，是Student类所有。</li>
<li>而name是外部传来的参数，不是Student类所自带的。</li>
<li>故，self.name = name的意思就是把外部传来的参数name的值赋值给Student类自己的属性变量self.name</li>
</ol>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Machine-learning/">Machine learning</a>
                    
                      <a class="hover-with-bg" href="/categories/Machine-learning/%E4%BB%A3%E7%A0%81/">代码</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Machine-learning/">Machine learning</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2021/04/12/QC03_QEC_02/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">量子处理器实现拓扑序态</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2021/04/09/ML_08_code_11_Ng/">
                        <span class="hidden-mobile">放码过来 1 吴恩达</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>







  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "放码过来之 Michael Nielsen《神经网络与深度学习》&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js" ></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  
















</body>
</html>
