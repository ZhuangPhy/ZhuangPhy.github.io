<!DOCTYPE html>
<html lang="en">





<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="JPZhuang">
  <meta name="keywords" content="">
  <title>放码过来 1 吴恩达 - JPZ</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
<meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Physics</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner intro-2" id="background" parallax=true
         style="background: url('/img/tag-bg.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fade-in-up">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2021-04-09 13:28">
      April 9, 2021 pm
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      6.1k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      85
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <p>[TOC]</p>
<h2 id="参考文献">参考文献</h2>
<ol type="1">
<li>吴恩达《机器学习》</li>
</ol>
<h2 id="概览">概览</h2>
<ol type="1">
<li>用matlab写代码
<ul>
<li>因为matlab比python熟悉，先写完matlab再迁移到python，有助于自己学习python</li>
<li>因为python容易出现一些版本问题，经常被细节卡住</li>
</ul></li>
<li>数据集吴老师已经提供，不需要自己书写。</li>
</ol>
<p>练习分成两部分</p>
<ol type="1">
<li>给你已经训练好的网络参数，让你体验一下输入图片，并且预测的过程</li>
<li>自己训练网络</li>
</ol>
<h2 id="讲义">讲义</h2>
<h3 id="逻辑回归logistic-regression">6 逻辑回归(Logistic Regression)</h3>
<p>在分类问题中，你要预测的变量 <span class="math inline">\(y\)</span> 是离散的值，逻辑回归模型 <span class="math inline">\(g(x)=\frac{1}{1+e^{-x}}\)</span> 把连续的 x 映射到 [0,1]，然后截断</p>
<ul>
<li>当<span class="math inline">\({h_\theta}\left( x \right)&gt;=0.5\)</span> 时，预测 <span class="math inline">\(y=1\)</span><br />
</li>
<li>当<span class="math inline">\({h_\theta}\left( x \right)&lt;0.5\)</span> 时，预测 <span class="math inline">\(y=0\)</span></li>
</ul>
<p><img src="http://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210412012438968.png" srcset="/img/loading.gif" /></p>
<h4 id="判定边界">判定边界</h4>
<p>一个模型： <span class="math display">\[
h_{\theta}(x)=g\left(\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}\right)
\]</span> 当参数 <span class="math inline">\(\theta\)</span> 是向量 [-3 1 1]</p>
<ul>
<li>直线<span class="math inline">\({x_1}+{x_2} = 3\)</span>，这条线便是我们模型的分界线，将预测为1的区域和预测为 0的区域分隔开。</li>
</ul>
<p>当遇到非常复杂形状的判定边界，我们可以用<strong>非常复杂的模型</strong> <span class="math display">\[
h_{\theta}(x)=g\left(\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\theta_{3} x_{1}^{2}+\theta_{4} x_{2}^{2}\right)
\]</span> 其中长串的参数可以简化 <span class="math display">\[
\boldsymbol{\theta}^{\boldsymbol{T}} \boldsymbol{x}=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\theta_{3} x_{1}^{2}+\theta_{4} x_{2}^{2}
\]</span></p>
<h4 id="代价函数">代价函数</h4>
<ul>
<li>对于线性回归模型，我们定义的代价函数是所有模型误差的平方和</li>
</ul>
<p><span class="math display">\[
J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \frac{1}{2}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}
\]</span></p>
<ul>
<li>逻辑回归的代价函数可以定义为</li>
</ul>
<p><span class="math display">\[
\boldsymbol{\theta}^{\boldsymbol{T}} \boldsymbol{x}=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\theta_{3} x_{1}^{2}+\theta_{4} x_{2}^{2}
\]</span></p>
<p><span class="math display">\[
J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \operatorname{Cost}\left(h_{\theta}\left(x^{(i)}\right), y^{(i)}\right)
\]</span></p>
<p><span class="math display">\[
\operatorname{Cost}\left(h_{\theta}(x), y\right)=\left\{\begin{aligned}
-\log \left(h_{\theta}(x)\right) &amp; \text { if } y=1 \\
-\log \left(1-h_{\theta}(x)\right) &amp; \text { if } y=0
\end{aligned}\right.
\]</span></p>
<p>逻辑回归的代价函数 的求导</p>
<p><span class="math display">\[
\begin{aligned}
\theta_{j+1} &amp; = \theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} J(\theta)
\\&amp;=  \theta_{j} - \alpha \frac{1}{m}\sum\limits_{i=1}^{m} \left[h_{\theta}\left(  {x}^{\left( i \right)} \right)- {y}^{ ( i  )} \right]   {x}_{j}^{(i)}
\end{aligned}
\]</span></p>
<h4 id="最小化-代价函数">最小化 代价函数</h4>
<p>为了拟合出参数，我们要试图找尽量让<span class="math inline">\(J\left( \theta \right)\)</span> 取得最小值的参数 $$ <span class="math display">\[
\underset{\theta}{\min }J\left( \theta  \right)
\]</span> 通常用的<strong>梯度下降法</strong>(<strong>gradient descent</strong>) <span class="math display">\[
\theta_{j+1} =\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} J(\theta)
\]</span></p>
<h4 id="多类别分类一对多">多类别分类：一对多</h4>
<p>"一对余"方法</p>
<ol type="1">
<li>将多个类中的一个类标记为正向类（<span class="math inline">\(y=1\)</span>），然后将其他所有类都标记为负向类，这个模型记作<span class="math inline">\(h_{\theta}^{(1)}\left( x \right)\)</span>。</li>
<li>接着，类似地第我们选择另一个类标记为正向类（<span class="math inline">\(y=2\)</span>），再将其它类都标记为负向类，将这个模型记作 <span class="math inline">\(h_{\theta}^{(2)}\left( x \right)\)</span>,</li>
<li>依此类推。最后我们得到一系列的模型简记为： <span class="math inline">\(h_{\theta}^{\left( i \right)}\left( x \right)=p\left( y=i|x;\theta \right)\)</span> 其中：<span class="math inline">\(i=\left( 1,2,3....k \right)\)</span></li>
</ol>
<h3 id="正则化regularization">7 正则化(Regularization)</h3>
<h4 id="过拟合over-fitting的问题">过拟合(<strong>over-fitting</strong>)的问题</h4>
<blockquote>
<ol type="1">
<li><p>解释什么是过度拟合问题，</p></li>
<li><p>谈论一种称为正则化(<strong>regularization</strong>)的技术，它可以改善或者减少过度拟合问题。</p></li>
</ol>
</blockquote>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/72f84165fbf1753cd516e65d5e91c0d3.jpg" srcset="/img/loading.gif" /></p>
<ul>
<li>什么是过度拟合问题</li>
</ul>
<p>第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质：预测新数据。我们可以看出，若给出一个新的值使之预测，它将表现的很差，是过拟合。</p>
<ul>
<li>如果我们发现了过拟合问题，应该如何处理？</li>
</ul>
<ol type="1">
<li><p>丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如<strong>PCA</strong>）</p></li>
<li><p>正则化。 保留所有的特征，但是减少参数的大小（<strong>magnitude</strong>）。</p></li>
</ol>
<h4 id="例子">例子</h4>
<p>如果我们的模型是： <span class="math display">\[
{h_\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}+{\theta_{3}}{x_{3}^3}+{\theta_{4}}{x_{4}^4}
\]</span> 那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于0的话，我们就能很好的拟合了。</p>
<ul>
<li><strong>修改代价函数</strong>，在其中<span class="math inline">\({\theta_{3}}\)</span>和<span class="math inline">\({\theta_{4}}\)</span> 设置一点惩罚。</li>
</ul>
<p><span class="math display">\[
\min _{\theta} \frac{1}{2 m}\left[\sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+1000 \theta_{3}^{2}+10000 \theta_{4}^{2}\right]
\]</span></p>
<ol type="1">
<li>最小化代价时，高次项的系数 <span class="math inline">\({\theta_{3}}\)</span> 和 <span class="math inline">\({\theta_{4}}\)</span> 需要小一些才能降低代价函数</li>
<li>因为高次项导致了过拟合的产生，所以小的高次项的系数 <span class="math inline">\({\theta_{3}}\)</span> 和 <span class="math inline">\({\theta_{4}}\)</span> 能防止过拟合问题</li>
</ol>
<h4 id="选择惩罚的程度">选择惩罚的程度</h4>
<p>正则化后的代价函数 <span class="math display">\[
J(\theta)=\frac{1}{2 m}\left[\sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n} \theta_{j}^{2}\right]
\]</span> 其中</p>
<ul>
<li>$$又称为正则化参数（<strong>Regularization Parameter</strong>）</li>
</ul>
<p>关键问题</p>
<ol type="1">
<li>如果选择的正则化参数<span class="math inline">\(\lambda\)</span> 过大，则会把所有的参数都最小化了，导致模型变成 <span class="math inline">\({h_\theta}\left( x \right)={\theta_{0}}\)</span>，造成欠拟合。</li>
<li>假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。</li>
</ol>
<h3 id="神经网络表述neural-networks-representation">8 神经网络：表述(Neural Networks: Representation)</h3>
<h4 id="动机">动机</h4>
<blockquote>
<p>当特征太多时，计算的负荷会非常大。</p>
</blockquote>
<ul>
<li>例如大于100个变量</li>
</ul>
<p>我们希望用这100个特征来构建一个非线性的多项式模型，即便我们只采用两两特征的组合 <span class="math display">\[
\left(x_{1} x_{2}+x_{1} x_{3}+x_{1} x_{4}+\ldots+x_{2} x_{3}+x_{2} x_{4}+\ldots+x_{99} x_{100}\right)
\]</span> 我们也会有接近5000个组合而成的特征。这对于一般的逻辑回归来说需要计算的特征太多了。</p>
<ul>
<li>例如识别视觉对象</li>
</ul>
<p>假如我们只选用灰度图片，每个像素则只有一个值（而非 <strong>RGB</strong>值），50x50像素的小图片</p>
<ol type="1">
<li>选取图片上的两个不同位置上的两个像素，然后训练一个逻辑回归算法利用这两个像素的值来判断图片上是否是汽车，则会有 2500个特征</li>
<li>如果我们要进一步将两两特征组合构成一个多项式模型，则会有约<span class="math inline">\(2500^{2}/2\)</span>个（接近3百万个）特征。</li>
</ol>
<p>普通的逻辑回归模型，不能有效地处理这么多的特征，这时候我们需要神经网络。</p>
<h4 id="模型表示">模型表示</h4>
<p>其中</p>
<ul>
<li><span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, <span class="math inline">\(x_3\)</span>是输入单元（<strong>input units</strong>）</li>
<li><span class="math inline">\(a_1\)</span>, <span class="math inline">\(a_2\)</span>, <span class="math inline">\(a_3\)</span>是中间单元</li>
</ul>
<figure>
<img src="http://jptanjing.oss-cn-beijing.aliyuncs.com/img/fbb4ffb48b64468c384647d45f7b86b5.png" srcset="/img/loading.gif" alt="" /><figcaption>fbb4ffb48b64468c384647d45f7b86b5</figcaption>
</figure>
<p>向量化的方法会使得计算更为简便</p>
<ol type="1">
<li>输入单元</li>
</ol>
<p><span class="math display">\[
x=\left[\begin{array}{l}
x_{0} \\
x_{1} \\
x_{2} \\
x_{3}
\end{array}\right]
\]</span></p>
<ol start="2" type="1">
<li>第一层神经元计算</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
z^{(2)}&amp;=\Theta^{(1)} x
\\&amp;= \left[\begin{array}{llll}
\theta_{10}^{(1)} &amp; \theta_{11}^{(1)} &amp; \theta_{12}^{(1)} &amp; \theta_{13}^{(1)} \\
\theta_{20}^{(1)} &amp; \theta_{21}^{(1)} &amp; \theta_{22}^{(1)} &amp; \theta_{23}^{(1)} \\
\theta_{30}^{(1)} &amp; \theta_{31}^{(1)} &amp; \theta_{32}^{(1)} &amp; \theta_{33}^{(1)}
\end{array}\right] \times\left[\begin{array}{c}
x_{0} \\
x_{1} \\
x_{2} \\
x_{3}
\end{array}\right]
\\&amp;= \left[\begin{array}{l}
\theta_{10}^{(1)} x_{0}+\theta_{11}^{(1)} x_{1}+\theta_{12}^{(1)} x_{2}+\theta_{13}^{(1)} x_{3} \\
\theta_{20}^{(1)} x_{0}+\theta_{21}^{(1)} x_{1}+\theta_{22}^{(1)} x_{2}+\theta_{23}^{(1)} x_{3} \\
\theta_{30}^{(1)} x_{0}+\theta_{31}^{(1)} x_{1}+\theta_{32}^{(1)} x_{2}+\theta_{33}^{(1)} x_{3}
\end{array}\right]
\end{aligned}
\]</span></p>
<ol start="3" type="1">
<li>第一层输出经过非线性函数 <span class="math inline">\(g\)</span> 成为第二层的输入<span class="math inline">\(a^{(2)}\)</span></li>
</ol>
<p><span class="math display">\[
\begin{aligned}
a^{(2)}&amp;=g\left(z^{(2)}\right)
\\&amp;=g\left(\left[\begin{array}{l}
\theta_{10}^{(1)} x_{0}+\theta_{11}^{(1)} x_{1}+\theta_{12}^{(1)} x_{2}+\theta_{13}^{(1)} x_{3} \\
\theta_{20}^{(1)} x_{0}+\theta_{21}^{(1)} x_{1}+\theta_{22}^{(1)} x_{2}+\theta_{23}^{(1)} x_{3} \\
\theta_{30}^{(1)} x_{0}+\theta_{31}^{(1)} x_{1}+\theta_{32}^{(1)} x_{2}+\theta_{33}^{(1)} x_{3}
\end{array}\right]\right)\\&amp;=\left[\begin{array}{c}
a_{1}^{(2)} \\
a_{2}^{(2)} \\
a_{3}^{(2)}
\end{array}\right]
\end{aligned}
\]</span></p>
<ol start="4" type="1">
<li>第二层的输入$a^{(2)} $ 的计算输出</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
z^{(3)}&amp;=\theta^{(2)} a^{(2)}
\\&amp;=\left[\begin{array}{llll}
\theta_{10}^{(2)} &amp; \theta_{11}^{(2)} &amp; \theta_{12}^{(2)} &amp; \theta_{13}^{(2)}
\end{array}\right] \times\left[\begin{array}{l}
u_{0} \\
a_{1}^{(2)} \\
a_{2}^{(2)} \\
a_{3}^{(2)}
\end{array}\right]
\\&amp;=\theta_{10}^{(2)} a_{0}^{(2)}+\theta_{11}^{(2)} a_{1}^{(2)}+\theta_{12}^{(2)} a_{2}^{(2)}+\theta_{13}^{(2)} a_{3}^{(2)}
\end{aligned}
\]</span></p>
<ol start="5" type="1">
<li>非线性输出</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
h_{\theta}(x) &amp;=a^{(3)}=g\left(z^{(3)}\right) \\
&amp;=g\left(\theta_{10}^{(2)} a_{0}^{(2)}+\theta_{11}^{(2)} a_{1}^{(2)}+\theta_{12}^{(2)} a_{2}^{(2)}+\theta_{13}^{(2)} a_{3}^{(2)}\right)
\end{aligned}
\]</span></p>
<h4 id="多类分类">多类分类</h4>
<p>将神经网络的分类定义为两种情况：</p>
<ul>
<li><p>二类分类：<span class="math inline">\(S_L=0, y=0\, or\, 1\)</span>表示哪一类；</p></li>
<li><p><span class="math inline">\(K\)</span>类分类：<span class="math inline">\(S_L=k, y_i = 1\)</span>表示分到第<span class="math inline">\(i\)</span>类；<span class="math inline">\((k&gt;2)\)</span></p></li>
</ul>
<p>其中 <span class="math inline">\(S_L\)</span>代表最后一层中处理单元的个数。</p>
<p>输出层4个神经元分别用来表示4类</p>
<figure>
<img src="http://jptanjing.oss-cn-beijing.aliyuncs.com/img/f3236b14640fa053e62c73177b3474ed.jpg" srcset="/img/loading.gif" alt="" /><figcaption>f3236b14640fa053e62c73177b3474ed</figcaption>
</figure>
<p>输出结果为四种可能情形之一： <span class="math display">\[
h_{\Theta}(x) \approx\left[\begin{array}{l}
1 \\
0 \\
0 \\
0
\end{array}\right], \quad h_{\Theta}(x) \approx\left[\begin{array}{l}
0 \\
1\\
0 \\
0
\end{array}\right], \quad h_{\Theta}(x) \approx\left[\begin{array}{l}
0 \\
0 \\
1 \\
0
\end{array}\right]\quad h_{\Theta}(x) \approx\left[\begin{array}{l}
0 \\
0 \\
0 \\
1
\end{array}\right]
\]</span></p>
<h3 id="神经网络的学习neural-networks-learning">9 神经网络的学习(Neural Networks: Learning)</h3>
<h4 id="多分类代价函数">多分类代价函数</h4>
<ul>
<li>在逻辑回归中，我们只有一个输出变量，又称标量（<strong>scalar</strong>）</li>
<li>在神经网络中，我们可以有很多输出变量，我们的<span class="math inline">\(h_\theta(x)\)</span>是一个维度为<span class="math inline">\(K\)</span>的向量</li>
</ul>
<p>因此神经网络中的代价函数会比逻辑回归更加复杂一些</p>
<ol type="1">
<li>逻辑回归中的代价函数</li>
</ol>
<p><span class="math display">\[
  J\left(\theta \right)=-\frac{1}{m}\left[\sum_\limits{i=1}^{m}{y}^{(i)}\log{h_\theta({x}^{(i)})}+\left(1-{y}^{(i)}\right)log\left(1-h_\theta\left({x}^{(i)}\right)\right)\right]+\frac{\lambda}{2m}\sum_\limits{j=1}^{n}{\theta_j}^{2}  
\]</span></p>
<ol start="2" type="1">
<li>神经网络中的代价函数</li>
</ol>
<p><span class="math display">\[
J(\Theta) = -\frac{1}{m} \left[ \sum\limits_{i=1}^{m} \sum\limits_{k=1}^{k} {y_k}^{(i)} \log {(h_\Theta(x^{(i)}))_k} + \left( 1 - y_k^{(i)} \right) \log \left( 1-  {\left( h_\Theta \left( x^{(i)} \right) \right)_k} \right) \right] + \frac{\lambda}{2m} \sum\limits_{l=1}^{L-1} \sum\limits_{i=1}^{s_l} \sum\limits_{j=1}^{s_l+1} \left( \Theta_{ji}^{(l)} \right)^2
\]</span></p>
<h4 id="反向传播算法">反向传播算法</h4>
<blockquote>
<p>为了拟合出参数，我们要试图找尽量让<span class="math inline">\(J\left( \theta \right)\)</span> 取得最小值的参数 $$ <span class="math display">\[
\underset{\theta}{\min }J\left( \theta  \right)
\]</span> 通常用的<strong>梯度下降法</strong>(<strong>gradient descent</strong>) <span class="math display">\[
\theta_{j+1}:=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} J(\theta)
\]</span> 为了计算代价函数的偏导数<span class="math inline">\(\frac{\partial}{\partial\Theta^{(l)}_{ij}}J\left(\Theta\right)\)</span>，我们需要采用一种反向传播算法</p>
<ol type="1">
<li>首先计算最后一层的误差</li>
<li>然后再一层一层反向求出各层的误差</li>
<li>直到倒数第二层</li>
</ol>
</blockquote>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/6a0954ad41f959d7f272e8f53d4ee2de.jpg" srcset="/img/loading.gif" /></p>
<p>误差计算：</p>
<ol start="4" type="1">
<li>第4层：激活单元的预测（<span class="math inline">\({a^{(4)}}\)</span>）与实际值（<span class="math inline">\(y^k\)</span>）之间的误差，（<span class="math inline">\(k=1:k\)</span>）。</li>
</ol>
<p><span class="math display">\[
\delta^{(4)}=a^{(4)}-y
\]</span></p>
<ol start="3" type="1">
<li></li>
</ol>
<h4 id="梯度检验">梯度检验</h4>
<ul>
<li>动机</li>
</ul>
<p>对一个较为复杂的模型 使用梯度下降算法时，可能会存在一些不容易察觉的错误（局部最优）</p>
<ul>
<li>指导思想</li>
</ul>
<p>为了避免这样的问题，我们采取一种叫做梯度的数值检验（<strong>Numerical Gradient Checking</strong>）方法。</p>
<h4 id="小结">小结</h4>
<p>网络结构：</p>
<ol type="1">
<li>第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。</li>
</ol>
<p>训练神经网络：</p>
<ol type="1">
<li><p>参数的随机初始化</p></li>
<li><p>利用正向传播方法计算所有的<span class="math inline">\(h_{\theta}(x)\)</span></p></li>
<li><p>编写计算代价函数 <span class="math inline">\(J\)</span> 的代码</p></li>
<li><p>利用反向传播方法计算所有偏导数</p></li>
<li><p>利用数值检验方法检验这些偏导数</p></li>
<li><p>使用优化算法来最小化代价函数</p></li>
</ol>
<h2 id="代码练习概述">代码练习概述</h2>
<p>代码练习-3.1</p>
<ul>
<li>"一对余"方法</li>
<li>Logistic Regression</li>
<li>计算代价函数和梯度值</li>
</ul>
<p>代码练习-3.2</p>
<ul>
<li></li>
</ul>
<h2 id="代码练习3.1">代码练习3.1</h2>
<p>"一对余"方法</p>
<pre><code class="hljs matlab">close all;clc
<span class="hljs-comment">%% 1.读取数据和初始化，</span>
fprintf(<span class="hljs-string">'Load dataSet:ex3data1.mat\n'</span>);
load(<span class="hljs-string">'ex3data1.mat'</span>);
<span class="hljs-comment">% 从数据中随机选择100个样本，显示成10*10的数字表格</span>
randIndex = randperm(<span class="hljs-built_in">size</span>(X,<span class="hljs-number">1</span>));
seldata = X(randIndex(<span class="hljs-number">1</span>:<span class="hljs-number">100</span>),:);
<span class="hljs-comment">% 数据可视化</span>
<span class="hljs-built_in">figure</span>(<span class="hljs-number">1</span>);
displayData(seldata);

<span class="hljs-comment">%% 2.向量化Logistic Regression，并计算代价函数和梯度值</span>
fprintf(<span class="hljs-string">'Training data with one-Vs-All logistic regression\n'</span>);
<span class="hljs-comment">% 设置正则化系数lambda，和输出的样本类别数labels</span>
lambda = <span class="hljs-number">0.05</span>;
num_labels=<span class="hljs-number">10</span>;
[all_theta] = oneVsAll(X,y,num_labels,lambda);

<span class="hljs-comment">%% 3.使用OneVsAll来预测</span>
P=predictOneVsAll(X,all_theta);
fprintf(<span class="hljs-string">'Training set Accuracy is %f\n'</span>, <span class="hljs-built_in">mean</span>(double(P==y)) * <span class="hljs-number">100</span>);</code></pre>
<h3 id="数据集">数据集</h3>
<p><code>ex3data1.mat</code> 包含5000个手写数据</p>
<p>把文件加载到程序中</p>
<pre><code class="hljs matlab">load(<span class="hljs-string">'ex3data1.mat'</span>);</code></pre>
<ol type="1">
<li>每个图像是一个 <span class="math inline">\(20\times 20\)</span> 的矩阵</li>
<li><span class="math inline">\(20\times 20\)</span> 的矩阵拉长为长度 400 的向量</li>
<li>所以数据集是 <span class="math inline">\(5000\times 400\)</span> 的矩阵</li>
</ol>
<p><span class="math display">\[
X=\left[\begin{array}{c}
-\left(x^{(1)}\right)^{T}- \\
-\left(x^{(2)}\right)^{T}- \\
\vdots \\
-\left(x^{(m)}\right)^{T}-
\end{array}\right]
\]</span></p>
<pre><code class="hljs matlab">close all;clc
<span class="hljs-comment">%% 1.读取数据和初始化，</span>
fprintf(<span class="hljs-string">'Load dataSet:ex3data1.mat\n'</span>);
load(<span class="hljs-string">'ex3data1.mat'</span>);
<span class="hljs-comment">% 从数据中随机选择100个样本，显示成10*10的数字表格</span>
randIndex = randperm(<span class="hljs-built_in">size</span>(X,<span class="hljs-number">1</span>));
<span class="hljs-comment">% size(X,1)=5000</span>
<span class="hljs-comment">% p = randperm(n) 返回行向量，其中包含从 1 到 n 没有重复元素的整数随机排列</span>
seldata = X(randIndex(<span class="hljs-number">1</span>:<span class="hljs-number">100</span>),:);
<span class="hljs-comment">% 数据可视化</span>
<span class="hljs-built_in">figure</span>(<span class="hljs-number">1</span>);
displayData(seldata);</code></pre>
<p>输出结果</p>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210418234842163.png" srcset="/img/loading.gif" /></p>
<p><code>displayData.m</code> 函数说明：把输入的图像数据X进行重新排列，显示在一个面板figurePane中，面板中有多个小imge用来显示每一行数据</p>
<pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-params">[figurePane , display_array]</span> = <span class="hljs-title">displayData</span><span class="hljs-params">(X,image_width)</span></span>
<span class="hljs-comment">%% 函数说明：把输入的图像数据X进行重新排列，显示在一个面板figurePane中，面板中有多个小imge用来显示每一行数据</span>
<span class="hljs-comment">% image_width：每一个小imgae的宽度</span>
<span class="hljs-comment">% 获取X的具体尺寸</span>
[m,n] =<span class="hljs-built_in">size</span>(X);

<span class="hljs-comment">%% 设置每一个image的尺寸</span>
<span class="hljs-comment">% 如果没有设置image_width，就默认为是数据维度的开平方，并四舍五入</span>
<span class="hljs-keyword">if</span> ~exist(<span class="hljs-string">'image_width'</span>,<span class="hljs-string">'var'</span>) || <span class="hljs-built_in">isempty</span>(image_width)
    image_width = <span class="hljs-built_in">round</span>(<span class="hljs-built_in">sqrt</span>(n));
<span class="hljs-keyword">end</span>
<span class="hljs-comment">% 每一个image的高度</span>
image_height = n / image_width;

<span class="hljs-comment">%% 设置figurePane（figure）的参数</span>
<span class="hljs-comment">% 设置面板figurePane图片的色彩为灰度图</span>
colormap(gray);
<span class="hljs-comment">% 设置面板figurePane中image的行数和列数</span>
figure_rows = <span class="hljs-built_in">floor</span>(<span class="hljs-built_in">sqrt</span>(m));   <span class="hljs-comment">% floor ---- 向负无穷取整，取小于等于它的最大整数</span>
figure_cols = <span class="hljs-built_in">ceil</span>(m / figure_rows);    <span class="hljs-comment">% ceil ---- 向正无穷取整，取大于等于它的最小整数</span>

<span class="hljs-comment">%% 设置面板figurePane对应的数组，用来保存X中的象素值</span>
<span class="hljs-comment">% 每一个image之间的间距</span>
pad = <span class="hljs-number">1</span>;
<span class="hljs-comment">% 初始值都是-1，显示为黑色</span>
display_array = -<span class="hljs-built_in">ones</span>( pad + (image_width+pad) * figure_rows, ...
                                     pad+(image_height +pad) * figure_cols );
                                 
<span class="hljs-comment">%% 把X中的每一个象素值，复制进display_array的对应位置</span>
current_image=<span class="hljs-number">1</span>;
<span class="hljs-keyword">for</span> row =<span class="hljs-number">1</span> : figure_rows
    <span class="hljs-keyword">for</span> col =<span class="hljs-number">1</span>:figure_cols
        <span class="hljs-comment">% 判断current_image的大小，大于m就表明遍历结束了</span>
        <span class="hljs-keyword">if</span> current_image &gt; m
            <span class="hljs-keyword">break</span>;
        <span class="hljs-keyword">end</span>
        <span class="hljs-comment">% 找到每一行的最大值，用来把这一行的数据归一化到[-1,1]之间</span>
        max_val = <span class="hljs-built_in">max</span>( <span class="hljs-built_in">max</span>( X(current_image,:) ) );
        <span class="hljs-comment">% 按照数据块进行重新放置数据，使用 reshape函数进行位置重排</span>
        <span class="hljs-comment">% 一个图像数据在重排成一行的时候，也是用reshape方法进行的，再使用reshape方法来恢复</span>
        display_array(pad + (row<span class="hljs-number">-1</span>)*(image_height+pad)+(<span class="hljs-number">1</span>:image_height) , pad + (col<span class="hljs-number">-1</span>)*(image_width+pad)+(<span class="hljs-number">1</span>:image_width))=...
            <span class="hljs-built_in">reshape</span>(X(current_image,:),image_height,image_width) / max_val;
       current_image=current_image+<span class="hljs-number">1</span>;
    <span class="hljs-keyword">end</span>
    <span class="hljs-keyword">if</span> current_image &gt; m
            <span class="hljs-keyword">break</span>;
    <span class="hljs-keyword">end</span>
<span class="hljs-keyword">end</span>
<span class="hljs-comment">% 显示图像，并且把色彩（这里是灰度）范围设置为[-1,1]</span>
figurePane = imagesc(display_array,[<span class="hljs-number">-1</span> <span class="hljs-number">1</span>]);
title(<span class="hljs-string">'Random handwritten digits'</span>);
axis image off
drawnow;
<span class="hljs-keyword">end</span></code></pre>
<h3 id="线性化cost-function">线性化cost function</h3>
<p>第一章节</p>
<h2 id="代码练习3.2">代码练习3.2</h2>
<p>代码模块</p>
<ol type="1">
<li>主函数 <code>a_main_ex3_nn.m</code></li>
<li>加载 <code>ex3data1.mat</code> 包含5000个手写数据</li>
<li>加载神经网络参数 <code>ex3weights.mat</code> 包含两层参数 <code>Theta1</code> 和 <code>Thetha2</code></li>
<li>函数 <code>predict_nn(X,Theta1,Theta2)</code> 读入数据和神经网络参数</li>
<li>函数里面调用了 <code>sigmoid.m</code></li>
</ol>
<pre><code class="hljs matlab"><span class="hljs-comment">%% Machine Learning Online Class - Exercise 3 : Neural Networks</span>
<span class="hljs-comment">% 使用简单的神经网络来实现“手写数字识别“，网络只有3层：输入层、中间隐藏层、输出层</span>
<span class="hljs-comment">% 中间隐藏层有25个unit，外加1个值为1的偏置unit</span>
<span class="hljs-comment">% 输出层有10个输出unit，对应数字的10个类别</span>
close all;clc

<span class="hljs-comment">%% 1.读取数据，显示随机样例</span>
fprintf(<span class="hljs-string">'Load data and visulizing .... \n'</span>);
load(<span class="hljs-string">'ex3data1.mat'</span>);

index = randperm(<span class="hljs-built_in">size</span>(X,<span class="hljs-number">1</span>));
<span class="hljs-built_in">figure</span>(<span class="hljs-number">1</span>);
displayData(X(index(<span class="hljs-number">1</span>:<span class="hljs-number">100</span>),:));
title(<span class="hljs-string">'Random Examples'</span>);
<span class="hljs-built_in">hold</span> off;

<span class="hljs-comment">%% 2.读取训练好的参数</span>
<span class="hljs-comment">% 载入训练好的参数：</span>
<span class="hljs-comment">% Thtea1：对应隐藏层每一个节点的输入参数</span>
<span class="hljs-comment">% Theta2：对应输出层每一个unit的参数</span>
fprintf(<span class="hljs-string">'Load parameters Theta1 and Thetha2 ....\n'</span>);
load(<span class="hljs-string">'ex3weights.mat'</span>);

<span class="hljs-comment">%% 3.进行预测</span>
fprintf(<span class="hljs-string">'Implement prediction ....\n'</span>);
<span class="hljs-comment">% 这里的参数是训练好的，所以可以直接用来预测，造成了不需要进行训练的假象</span>
P = predict_nn(X,Theta1,Theta2);

fprintf(<span class="hljs-string">'The training set accuracy is %f\n'</span>,<span class="hljs-built_in">mean</span>(double(P==y))*<span class="hljs-number">100</span>);

<span class="hljs-comment">% 随机取出X中的数据，参看对它的预测结果</span>
<span class="hljs-built_in">figure</span>(<span class="hljs-number">2</span>);
<span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span> =<span class="hljs-number">1</span> : <span class="hljs-built_in">size</span>(X,<span class="hljs-number">1</span>)
    fprintf(<span class="hljs-string">'Displaying the random example ... \n'</span>);
    displayData(X(index(<span class="hljs-built_in">i</span>),:));
    
    fprintf(<span class="hljs-string">'The ture digit is %d ,the Neural Networks prediction : %d (digit %d)'</span>,<span class="hljs-built_in">mod</span>(y(index(<span class="hljs-built_in">i</span>)),<span class="hljs-number">10</span>),<span class="hljs-built_in">mod</span>(P(index(<span class="hljs-built_in">i</span>)),<span class="hljs-number">10</span>));
    
    fprintf(<span class="hljs-string">'Program paused. Pleasr enter any key to continue.\n'</span>);
    pause;
<span class="hljs-keyword">end</span></code></pre>
<h3 id="神经网络参数">神经网络参数</h3>
<p>训练好的神经网络参数 <code>ex3weights.mat</code> 包含两层参数 <code>Theta1</code> 和 <code>Thetha2</code></p>
<p><code>Theta1</code></p>
<ol type="1">
<li><p>输入图像是 <span class="math inline">\(20\times 20\)</span> 的矩阵，外加一个偏置 b，所以输入层是 401个神经元</p></li>
<li><p>第二层是25个神经元</p></li>
<li><p>所以 <code>Theta1</code> 是一个 <span class="math inline">\(25 \times 401\)</span> 的矩阵</p></li>
</ol>
<p><code>Theta2</code></p>
<ol type="1">
<li>输出10个字符</li>
<li>上一层25个神经元，外加一个偏置 b</li>
<li>所以 <code>Theta2</code> 是 <span class="math inline">\(10 \times 26\)</span> 的矩阵</li>
</ol>
<p>本次习题不需要训练数据，把神经网络参数训练好给你</p>
<h3 id="神经网络计算">神经网络计算</h3>
<p>神经元计算 <span class="math display">\[
\begin{aligned}
z^{(2)}&amp;=\Theta^{(1)} x
\\&amp;= \left[\begin{array}{llll}
\theta_{10}^{(1)} &amp; \theta_{11}^{(1)} &amp; \theta_{12}^{(1)} &amp; \theta_{13}^{(1)} \\
\theta_{20}^{(1)} &amp; \theta_{21}^{(1)} &amp; \theta_{22}^{(1)} &amp; \theta_{23}^{(1)} \\
\theta_{30}^{(1)} &amp; \theta_{31}^{(1)} &amp; \theta_{32}^{(1)} &amp; \theta_{33}^{(1)}
\end{array}\right] \times\left[\begin{array}{c}
x_{0} \\
x_{1} \\
x_{2} \\
x_{3}
\end{array}\right]
\\&amp;= \left[\begin{array}{l}
\theta_{10}^{(1)} x_{0}+\theta_{11}^{(1)} x_{1}+\theta_{12}^{(1)} x_{2}+\theta_{13}^{(1)} x_{3} \\
\theta_{20}^{(1)} x_{0}+\theta_{21}^{(1)} x_{1}+\theta_{22}^{(1)} x_{2}+\theta_{23}^{(1)} x_{3} \\
\theta_{30}^{(1)} x_{0}+\theta_{31}^{(1)} x_{1}+\theta_{32}^{(1)} x_{2}+\theta_{33}^{(1)} x_{3}
\end{array}\right]
\end{aligned}
\]</span></p>
<p>输入的图像(矩阵拉直的向量)和神经网络参数的乘积</p>
<pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">P</span> = <span class="hljs-title">predict_nn</span><span class="hljs-params">(X,Theta1,Theta2)</span></span>
<span class="hljs-comment">%% 函数说明：根据隐藏层的参数Thtea1和输出层的参数Theta2，对输入数据X进行预测，获得X在该Neural Networks上的预测结果</span>
<span class="hljs-comment">% m：数据样本的个数</span>
<span class="hljs-comment">% n：每一个数据的维度</span>
[m,~] = <span class="hljs-built_in">size</span>(X);
X = [<span class="hljs-built_in">ones</span>(m,<span class="hljs-number">1</span>),X];
<span class="hljs-comment">% m1：隐藏层的unit个数</span>
<span class="hljs-comment">% n1：隐藏层的每一个unit的θ的维数，大小与输入层数据的维度大小相同</span>
<span class="hljs-comment">% [m1,n1] = size(Theta1);</span>
<span class="hljs-comment">% m1：输出层的unit个数</span>
<span class="hljs-comment">% n1：输出层的每一个unit的θ的维数，大小与隐藏层的输出个数相同</span>
<span class="hljs-comment">% [m2,n2] = size(Theta2);</span>

<span class="hljs-comment">% 隐藏层的输入Z1，增加一个值为1的unit</span>
<span class="hljs-comment">% Z1 的尺寸为m1*m，表示某一个隐藏层unit第一组参数Thteta1下对每一个数据的预测</span>
Z1 = Theta1 * X.' ;
Z1 = [<span class="hljs-built_in">ones</span>(<span class="hljs-number">1</span>,m) ; Z1];
<span class="hljs-comment">% 隐藏层的输出 A1</span>
A1 = sigmoid(Z1);

<span class="hljs-comment">% 输出层的输入Z2</span>
Z2 = Theta2 * A1;
Z2 = Z2.';

<span class="hljs-comment">% 输出层的输出，其实没有这个sigmoid函数也行，因为sigmoid是单调增加的函数</span>
A2 = sigmoid(Z2);

[~,P] = <span class="hljs-built_in">max</span>(A2 ,[],<span class="hljs-number">2</span>);

<span class="hljs-keyword">end</span></code></pre>
<p>代码解释 <code>[~,P] = max(A2 ,[],2);</code></p>
<blockquote>
<p>返回矩阵 <code>A2</code> 上的每一行的最大值对应索引。就是数值 <span class="math inline">\(0 \sim 9\)</span></p>
</blockquote>
<p>参考</p>
<ol type="1">
<li><a href="https://ww2.mathworks.cn/help/matlab/ref/max.html" target="_blank" rel="noopener">max</a></li>
</ol>
<p>创建一个矩阵 <code>A</code>，并返回矩阵 <code>M</code> 中每行的最大值。</p>
<ul>
<li><code>M = max(A,[],dim)</code> 返回维度 <code>dim</code> 上的最大元素。例如，如果 <code>A</code> 为矩阵，则 <code>max(A,[],2)</code> 是包含每一行的最大值的列向量。</li>
<li><code>[M,I] = max(___)</code> 在上述语法基础上，还返回 <code>A</code> 中最大值在运算维度上的对应索引。</li>
</ul>
<pre><code class="hljs matlab">A = [<span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span>; <span class="hljs-number">4</span> <span class="hljs-number">5</span> <span class="hljs-number">6</span>]
[M,I] = <span class="hljs-built_in">max</span>(A,[],<span class="hljs-number">2</span>,<span class="hljs-string">'linear'</span>)

<span class="hljs-comment">% M = 2×1</span>

<span class="hljs-comment">%     3</span>
<span class="hljs-comment">%     6</span>

<span class="hljs-comment">% I = 2×1</span>

<span class="hljs-comment">%     5</span>
<span class="hljs-comment">%     6</span>

maxvals = A(I)</code></pre>
<p>参考</p>
<ol type="1">
<li><a href="https://ww2.mathworks.cn/help/matlab/matlab_prog/matlab-operators-and-special-characters.html" target="_blank" rel="noopener">波浪号</a> <code>~</code></li>
</ol>
<p><strong>用法</strong>：</p>
<ul>
<li>逻辑非</li>
<li>参数占位符</li>
</ul>
<p><strong>说明</strong>：使用波浪号可表示逻辑非，也可禁止特定输入或输出参数。</p>
<p><strong>示例</strong></p>
<p>计算矩阵的逻辑 NOT：</p>
<pre><code class="hljs dns"><span class="hljs-keyword">A</span> = eye(<span class="hljs-number">3</span>)<span class="hljs-comment">;</span>
~<span class="hljs-keyword">A</span></code></pre>
<p>确定 <code>A</code> 的元素在哪些位置不等于 <code>B</code> 的元素：</p>
<pre><code class="hljs angelscript">A = [<span class="hljs-number">1</span> <span class="hljs-number">-1</span>; <span class="hljs-number">0</span> <span class="hljs-number">1</span>]
B = [<span class="hljs-number">1</span> <span class="hljs-number">-2</span>; <span class="hljs-number">3</span> <span class="hljs-number">2</span>]
A~=B</code></pre>
<p>仅返回 <code>union</code> 的第三个输出值：</p>
<pre><code class="hljs crystal">[~,~,iB] = <span class="hljs-class"><span class="hljs-keyword">union</span>(<span class="hljs-title">A</span>,<span class="hljs-title">B</span>)</span></code></pre>
<p>mod(P(index(i)),10))</p>
<h3 id="输出">输出</h3>
<p>代码解释 <code>mean(double(p==y))\*100</code> 计算预测的精确度(Accuracy)。</p>
<blockquote>
<ol type="1">
<li>double(p==y)将预测结果向量p与真实值向量y逐一对比，相同则置为1，不同则置为0，</li>
<li>然后通过mean()函数计算一下均值，精确度就计算出来了。</li>
<li>double(p~=y) 向量p与真实值向量y逐一对比，相同则置为0，不同则置为1。与上述对比正好相反。</li>
</ol>
</blockquote>
<p>当数组 <code>A</code> 和 <code>B</code> 相等时，<code>A == B</code> 返回一个逻辑数组，</p>
<ul>
<li>其各元素设置为逻辑值 <code>1</code> (<code>true</code>)；</li>
<li>否则，元素是逻辑值 <code>0</code> (<code>false</code>)。</li>
</ul>
<p>代码解释 <code>mod(y(index(i)),10)</code></p>
<p>参考 <a href="https://ww2.mathworks.cn/help/matlab/ref/mod.html?searchHighlight=mod&amp;s_tid=srchtitle" target="_blank" rel="noopener">mod</a></p>
<p><code>b = mod(a,m)</code> 返回 <code>a</code> 除以 <code>m</code> 后的余数，其中 <code>a</code> 是被除数，<code>m</code> 是除数。此函数通常称为取模运算，表达式为 <code>b = a - m.*floor(a./m)</code>。<code>mod</code> 函数遵从 <code>mod(a,0)</code> 返回 <code>a</code> 的约定。</p>
<h2 id="代码练习4">代码练习4</h2>
<blockquote>
<p>反向反馈法训练神经网络参数</p>
</blockquote>
<h3 id="运行结果">运行结果</h3>
<p>load data and visualizing ...</p>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210418234842163.png" srcset="/img/loading.gif" style="zoom:67%;" /></p>
<p>load parameters ...</p>
<ul>
<li>The cost at weights is (without lambda=0): 0.2876293.2</li>
<li>The cost at weights is (without lambda=1): 0.3837703.2</li>
</ul>
<p>简化</p>
<pre><code class="hljs matlab"><span class="hljs-comment">%%% 1 加载训练数据</span>
load(<span class="hljs-string">'ex4data1.mat'</span>);

<span class="hljs-comment">%%% 2.读取参数</span>
load(<span class="hljs-string">'ex4weights.mat'</span>);
nn_parameter = [Theta1(:);Theta2(:)];
<span class="hljs-comment">% 获得输入层、隐藏层、输出层的节点个数（不包括每一层的偏置维度）</span>
input_layer_size = <span class="hljs-built_in">size</span>(X,<span class="hljs-number">2</span>);
hidden_layer_size = <span class="hljs-number">25</span>;
label_nums = <span class="hljs-number">10</span>;

<span class="hljs-comment">%% 计算代价：</span>
<span class="hljs-comment">% nnCostFunction函数还可以计算BP算法得到梯度值grad，但是第一次呼叫nnCostFunction的时候不需要grad只要J，所以[J,~]</span>
 <span class="hljs-comment">% 有正则项 </span>
[J,~] = nnCostFunction(X,y,nn_parameter ,input_layer_size,hidden_layer_size,label_nums);
fprintf(<span class="hljs-string">'The cost at weights is (without lambda=0): %f3.2\n'</span>,J);
 <span class="hljs-comment">%没有正则项</span>
lambda =<span class="hljs-number">1</span>;
[J,~] = nnCostFunction(X,y,nn_parameter ,input_layer_size,hidden_layer_size,label_nums,lambda);
fprintf(<span class="hljs-string">'The cost at weights is (without lambda=1): %f3.2\n'</span>,J);

<span class="hljs-comment">%%%% 3.随机初始化参数</span>
fprintf(<span class="hljs-string">'Initializing parameters ....\n'</span>);
theta1 = randInitializeWeights(input_layer_size,hidden_layer_size);
theta2 = randInitializeWeights(hidden_layer_size,label_nums);
initial_theta = [theta1(:) ; theta2(:)];

fprintf(<span class="hljs-string">'\n Checking Backpropagation... \n'</span>);
<span class="hljs-comment">% 作者提供的检测梯度函数是否正确的函数，意义不明</span>
checkNNGradients;
fprintf(<span class="hljs-string">'\nProgram paused. Press enter to continue.\n'</span>);
pause;

<span class="hljs-comment">%%%% 4.训练神经网络</span>
fprintf(<span class="hljs-string">'\nTraining Neural Network... \n'</span>);
lambda = <span class="hljs-number">1</span>;
costFunction = @(p) nnCostFunction(X,y,p,input_layer_size, ...
                                   hidden_layer_size, ...
                                   label_nums,lambda);
options = optimset(<span class="hljs-string">'MaxIter'</span>, <span class="hljs-number">50</span>);
[nn_params, cost] = fmincg(costFunction, initial_theta , options);
Theta1 = <span class="hljs-built_in">reshape</span>(nn_params(<span class="hljs-number">1</span>:hidden_layer_size * (input_layer_size + <span class="hljs-number">1</span>)), ...
                 hidden_layer_size, (input_layer_size + <span class="hljs-number">1</span>));
Theta2 = <span class="hljs-built_in">reshape</span>(nn_params((<span class="hljs-number">1</span> + (hidden_layer_size * (input_layer_size + <span class="hljs-number">1</span>))):<span class="hljs-keyword">end</span>), ...
                 label_nums, (hidden_layer_size + <span class="hljs-number">1</span>));             

<span class="hljs-comment">%%%% 5.可视化中间层参数和预测</span>
fprintf(<span class="hljs-string">'\nVisualizing Neural Network... \n'</span>)
<span class="hljs-built_in">figure</span>(<span class="hljs-number">2</span>);
<span class="hljs-comment">% 因为Theta1的列数与数据样本的列数相同，所以可以使用相同的显示函数绘制图像</span>
displayData(Theta1(:, <span class="hljs-number">2</span>:<span class="hljs-keyword">end</span>));
pred = predict_nn(X, Theta1, Theta2);
title(sprintf(<span class="hljs-string">'\n The wights with training set accuracy: %f\n'</span>, <span class="hljs-built_in">mean</span>(double(pred == y)) * <span class="hljs-number">100</span>));
<span class="hljs-built_in">hold</span> off;</code></pre>
<h3 id="计算代价">计算代价</h3>
<p><code>nnCostFunction.m</code> 函数功能：根据输入的数据和对应的输出，以及神经网络相应的参数，计算代价函数</p>
<ul>
<li>The cost at weights is (without lambda=0): 0.2876293.2</li>
<li>The cost at weights is (without lambda=1): 0.3837703.2</li>
</ul>
<pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span>  <span class="hljs-params">[J,grad]</span> = <span class="hljs-title">nnCostFunction</span><span class="hljs-params">(X,y,nn_parameter, ...</span></span>
<span class="hljs-function"><span class="hljs-params">                                                 input_layer_size, ...</span></span>
<span class="hljs-function"><span class="hljs-params">                                                 hidden_layer_size, ...</span></span>
<span class="hljs-function"><span class="hljs-params">                                                 label_num, ...</span></span>
<span class="hljs-function"><span class="hljs-params">                                                 lambda)</span></span>
<span class="hljs-comment">%% 函数功能：根据输入的数据和对应的输出，以及神经网络相应的参数，计算代价函数</span>
<span class="hljs-keyword">if</span> nargin == <span class="hljs-number">6</span>
    <span class="hljs-comment">%只有6个参数的时候，表示没有正则项，把系数lambda设置为0</span>
    lambda =<span class="hljs-number">0</span>;
<span class="hljs-keyword">end</span>

<span class="hljs-comment">%% 第一部分，计算输出</span>
<span class="hljs-comment">% 重拍参数</span>
theta1 = <span class="hljs-built_in">reshape</span>(nn_parameter(<span class="hljs-number">1</span>:hidden_layer_size * (input_layer_size + <span class="hljs-number">1</span> )),hidden_layer_size,input_layer_size+<span class="hljs-number">1</span>); 
theta2 = <span class="hljs-built_in">reshape</span>(nn_parameter(hidden_layer_size * (input_layer_size + <span class="hljs-number">1</span> )+<span class="hljs-number">1</span>:<span class="hljs-keyword">end</span>),label_num,hidden_layer_size+<span class="hljs-number">1</span>);
[m,~] = <span class="hljs-built_in">size</span>(X);
X = [<span class="hljs-built_in">ones</span>(m,<span class="hljs-number">1</span>),X];

<span class="hljs-comment">% 输入层的输出等于输入</span>
a1 = X;     
<span class="hljs-comment">% 第二层输入和输出</span>
z2 = a1 * theta1.' ;
a2 = sigmoid(z2);
a2 = [<span class="hljs-built_in">ones</span>(<span class="hljs-built_in">size</span>(a2,<span class="hljs-number">1</span>),<span class="hljs-number">1</span>) , a2];
<span class="hljs-comment">% 第三层输入和输出</span>
z3 = a2 * theta2.';
a3 = sigmoid(z3);
<span class="hljs-comment">% 此时a3是一个m*K的数组，a3(m,K)的值是第m个样本属于第K个标签的预测输出</span>
<span class="hljs-comment">% 根据已有的真值y，转换成与a3格式相同的数组，如果yk(m,k)==1，表示第m个样本的真实标签是K</span>
yk = <span class="hljs-built_in">zeros</span>(<span class="hljs-built_in">length</span>(y) , label_num);
<span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span> =<span class="hljs-number">1</span>:m
    yk(<span class="hljs-built_in">i</span>,y(<span class="hljs-built_in">i</span>)) =<span class="hljs-number">1</span>;
<span class="hljs-keyword">end</span>
<span class="hljs-comment">% 计算代价，正则项中，没有带入theta1和theta2的偏置部分</span>
logisf = (-yk) .* <span class="hljs-built_in">log</span>(a3) - (<span class="hljs-number">1</span>- yk) .* <span class="hljs-built_in">log</span>(<span class="hljs-number">1</span> - a3);
J = (<span class="hljs-number">1</span>/m) * sum(sum(logisf)) + (lambda) * ( sum(sum(theta1(:,<span class="hljs-number">2</span>:<span class="hljs-keyword">end</span>) .^ <span class="hljs-number">2</span> )) + sum(sum(theta2(:,<span class="hljs-number">2</span>:<span class="hljs-keyword">end</span>) .^<span class="hljs-number">2</span>)) ) /(<span class="hljs-number">2</span>*m);

<span class="hljs-comment">%% 第二部分，BP算法--这个BP只适用于这个案例中，对于更高层数更多，更复杂的模型就不适用了</span>
delta3 = a3 - yk;

delta2 = delta3 * theta2 .* sigmoidGradient( [<span class="hljs-built_in">ones</span>(<span class="hljs-built_in">size</span>(z2,<span class="hljs-number">1</span>), <span class="hljs-number">1</span>), z2] );
delta2 = delta2(:,<span class="hljs-number">2</span>:<span class="hljs-keyword">end</span>);
<span class="hljs-comment">% theta1的梯度</span>
tridelta_1 = <span class="hljs-number">0</span>;
tridelta_1 = tridelta_1 + delta2.' * a1;
nn_parameter1_grad = (<span class="hljs-number">1</span>/m) .* tridelta_1 + (lambda/m) *[<span class="hljs-built_in">zeros</span>(<span class="hljs-built_in">size</span>(theta1,<span class="hljs-number">1</span>),<span class="hljs-number">1</span>),theta1(:,<span class="hljs-number">2</span>:<span class="hljs-keyword">end</span>)]  ;
<span class="hljs-comment">% theta2的梯度</span>
tridelta_2 = <span class="hljs-number">0</span>;
tridelta_2 = tridelta_2 + delta3.' * a2;
nn_parameter2_grad = (<span class="hljs-number">1</span>/m) .* tridelta_2 +(lambda/m) * [<span class="hljs-built_in">zeros</span>(<span class="hljs-built_in">size</span>(theta2,<span class="hljs-number">1</span>),<span class="hljs-number">1</span>),theta2(:,<span class="hljs-number">2</span>:<span class="hljs-keyword">end</span>)] ;

grad = [nn_parameter1_grad(:);nn_parameter2_grad(:)];

<span class="hljs-keyword">end</span></code></pre>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Machine-learning/">Machine learning</a>
                    
                      <a class="hover-with-bg" href="/categories/Machine-learning/%E4%BB%A3%E7%A0%81/">代码</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Machine-learning/">Machine learning</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2021/04/10/ML_08_code_21_NNDL/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">放码过来之 Michael Nielsen《神经网络与深度学习》</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2021/04/07/CMP_15_Transport04/">
                        <span class="hidden-mobile">量子输运-以Majorana为例子</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>







  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "放码过来 1 吴恩达&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js" ></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  
















</body>
</html>
