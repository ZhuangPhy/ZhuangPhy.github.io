<!DOCTYPE html>
<html lang="en">





<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="JPZhuang">
  <meta name="keywords" content="">
  <title>随机过程基础 - JPZ</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
<meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Physics</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner intro-2" id="background" parallax=true
         style="background: url('https://jptanjing.oss-cn-beijing.aliyuncs.com/blog_QuantumAI/Math_stochastic_processes_for_physicists_01.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fade-in-up">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2021-02-23 17:23">
      February 23, 2021 pm
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      8.8k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      173
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <p>[TOC]</p>
<h2 id="references">References</h2>
<ol type="1">
<li>《Stochastic Processes for Physicists: Understanding Noisy Systems》</li>
</ol>
<h2 id="概率论基础">概率论基础</h2>
<h4 id="probability-density">probability density</h4>
<p>probability density is usually denoted by <span class="math inline">\(P_{X}(x)\)</span> (or just <span class="math inline">\(P(x)\)</span> ). The probability for <span class="math inline">\(X\)</span> to be in the range from <span class="math inline">\(x=a\)</span> to <span class="math inline">\(x=b\)</span> is now the area under <span class="math inline">\(P(x)\)</span> from <span class="math inline">\(x=a\)</span> to <span class="math inline">\(x=b\)</span>. That is <span class="math display">\[
\operatorname{Prob}(a&lt;X&lt;b)=\int_{a}^{b} P(x) d x
\]</span></p>
<h4 id="average">average</h4>
<p>The average of <span class="math inline">\(X,\)</span> also known as the mean, or expectation value, of <span class="math inline">\(X\)</span> is defined by <span class="math display">\[
\langle X\rangle \equiv \int_{-\infty}^{\infty} P(x) x d x
\]</span></p>
<h4 id="variance">variance</h4>
<p>The variance of <span class="math inline">\(X\)</span> is defined as <span class="math display">\[
V_{X} \equiv \int_{-\infty}^{\infty} P(x)(x-\langle X\rangle)^{2} d x=\int_{-\infty}^{\infty} P(x) x^{2} d x-\langle X\rangle^{2}=\left\langle X^{2}\right\rangle-\langle X\rangle^{2}
\]</span> #### standard deviation</p>
<p>The standard deviation of <span class="math inline">\(X,\)</span> denoted by <span class="math inline">\(\sigma_{X}\)</span> and defined as the square root of the variance, <span class="math display">\[
\sigma_{X}=\sqrt{V_{X}}
\]</span> is a measure of how <strong>broad</strong> the probability density for <span class="math inline">\(X\)</span> is - that is, how much we can expect <span class="math inline">\(X\)</span> to deviate from its mean value.</p>
<h4 id="example">example</h4>
<p>An important example of a probability density is the Gaussian, given by <span class="math display">\[
P(x)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
\]</span> - The mean of this Gaussian probability density is <span class="math inline">\(\langle X\rangle=\mu\)</span> - the variance is <span class="math inline">\(V(x)=\sigma^{2} .\)</span></p>
<p>A plot of this probability density in given in Figure</p>
<figure>
<img src="http://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210223161040169.png" srcset="/img/loading.gif" alt="" /><figcaption>image-20210223161040169</figcaption>
</figure>
<h3 id="independence">Independence</h3>
<p>Two random variables are referred to as being independent if neither of their probability densities depends on the value of the other variable.</p>
<h4 id="joint-probability-density">joint probability density</h4>
<p>The joint probability density is the product of the probability densities for each of the two independent random variables, and we write this as <span class="math display">\[
P(x, y)=P_{X}(x) P_{Y}(y)
\]</span> The probability that <span class="math inline">\(X\)</span> falls within the interval <span class="math inline">\([a, b]\)</span> and <span class="math inline">\(Y\)</span> falls in the interval <span class="math inline">\([c, d]\)</span> is then <span class="math display">\[
\begin{aligned}
\operatorname{Prob}(X \in[a, b] \text { and } Y \in[c, d]) &amp;=\int_{a}^{b} \int_{c}^{d} P(x, y) d y d x \\
&amp;= \int_{a}^{b} \int_{c}^{d} P_{X}(x) P_{Y}(y) d y d x\\&amp;=\left(\int_{a}^{b} P_{X}(x) d x\right)\left(\int_{c}^{d} P_{Y}(y) d y\right) \\
&amp;=\operatorname{Prob}(X \in[a, b]) \times \operatorname{Prob}(Y \in[c, d]) .
\end{aligned}
\]</span></p>
<h4 id="expectation-values">expectation values</h4>
<p>when two variables are independent, then the expectation value of their product is simply the product of their individual expectation values. That is <span class="math display">\[
\langle X Y\rangle=\langle X\rangle\langle Y\rangle
\]</span></p>
<h3 id="dependent-random-variables">Dependent random variables</h3>
<p>Random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y,\)</span> are said to be dependent if their joint probability density, <span class="math inline">\(P(x, y),\)</span> does not factor into the product of their respective probability densities.</p>
<h4 id="marginal">marginal</h4>
<p>To obtain the probability density for one of the variables alone (say <span class="math inline">\(X\)</span> ), we integrate the joint probability density over all values of the other variable (in this case <span class="math inline">\(Y\)</span> ). <span class="math display">\[
P_{X}(x)=\int_{-\infty}^{\infty} P(x, y) d y
\]</span> This is because, for each value of <span class="math inline">\(X\)</span>, we want to know the total probability summed over all the mutually exclusive values that <span class="math inline">\(Y\)</span> can take. In this context, the probability densities for the single variables are referred to as the <strong>marginals</strong> of the joint density.</p>
<p>If we know nothing about the value of <span class="math inline">\(Y,\)</span> then our probability density for <span class="math inline">\(X\)</span> is just the marginal</p>
<h4 id="conditional-probability">conditional probability</h4>
<p>conditional probability density for <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span>. <span class="math display">\[
\begin{aligned}
P(x \mid y)&amp;=\frac{P(x, y)}{\int_{-\infty}^{\infty} P(x, y) d x} \\
&amp;=\frac{P(x, y)}{P_{Y}(y)}
\end{aligned}
\]</span> where</p>
<ul>
<li><span class="math inline">\(P_{Y}(y)=\int_{-\infty}^{\infty} P(x, y) d x\)</span></li>
<li><span class="math inline">\(P(x, y)=P(x \mid y) P_{Y}(y)\)</span></li>
</ul>
<h4 id="example-1">example</h4>
<p>As an example of a conditional probability density consider a joint probability density for <span class="math inline">\(X\)</span> and <span class="math inline">\(Y,\)</span> where the probability density for <span class="math inline">\(Y\)</span> is a Gaussian with zero mean, and that for <span class="math inline">\(X\)</span> is a Gaussian whose mean is given by the value of <span class="math inline">\(Y .\)</span> In this case <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are not independent, and we have <span class="math display">\[
P(x, y)=P(x \mid y) P(y)=\frac{e^{-(1 / 2)(x-y)^{2}}}{\sqrt{2 \pi}} \times \frac{e^{-(1 / 2) y^{2}}}{\sqrt{2 \pi}}=\frac{e^{-(1 / 2)(x-y)^{2}-(1 / 2) y^{2}}}{2 \pi}
\]</span> where we have chosen the variance of <span class="math inline">\(Y,\)</span> and of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span> to be unity. Generally, when two random variables are dependent <span class="math display">\[
\langle X Y\rangle \neq\langle X\rangle\langle Y\rangle
\]</span></p>
<h3 id="correlations">Correlations</h3>
<p>The expectation value of the product of two random variables is called the correlation of the two variables. <span class="math display">\[
\langle X Y\rangle
\]</span> correlation coefficients <span class="math display">\[
\begin{aligned}
C_{X Y} 
&amp;=\frac{\langle(X-\langle X\rangle)(Y-\langle Y\rangle)\rangle}{\sqrt{V(X) V(Y)}}\\
&amp;=\frac{\langle X Y\rangle-\langle X\rangle\langle Y\rangle}{\sqrt{V(X) V(Y)}}  
\end{aligned}
\]</span> where</p>
<ul>
<li><span class="math inline">\(\langle X Y\rangle-\langle X\rangle\langle Y\rangle\)</span> is called the covariance of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and is zero if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</li>
</ul>
<blockquote>
<p>为什么要除以 <span class="math inline">\(\sqrt{V(X) V(Y)}\)</span> ？</p>
<p>Of course, if we increase the variance of either of the two variables then the correlation will also increase. We can remove this dependence, and obtain a quantity that is a clearer indicator of the mutual dependence between the two variables by dividing the correlation by <span class="math inline">\(\sqrt{V(X) V(Y)}\)</span>.</p>
</blockquote>
<h3 id="adding-random-variables">Adding random variables</h3>
<p>When we have two continuous random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, with probability densities <span class="math inline">\(P_{X}\)</span> and <span class="math inline">\(P_{Y},\)</span> it is often useful to be able to calculate the probability density of the random variable whose value is the sum of them: <span class="math inline">\(Z=X+Y .\)</span></p>
<p>It turns out that the probability density for <span class="math inline">\(Z\)</span> is given by <span class="math display">\[
P_{Z}(z)=\int_{-\infty}^{\infty} P_{X}(s-z) P_{Y}(s) d s \equiv P_{X} * P_{Y}
\]</span> - which is called the convolution of <span class="math inline">\(P_{X}\)</span> and <span class="math inline">\(P_{Y}[1] .\)</span> - Note that the convolution of two functions, denoted by "*", is another function.</p>
<blockquote>
<p>为什么 <span class="math inline">\(Z=X+Y\)</span> 的概率是 <span class="math inline">\(P_{X} * P_{Y}\)</span> 卷积？</p>
<p>It is, in fact, quite easy to see directly why the above expression for <span class="math inline">\(P_{Z}(z)\)</span> is true. For <span class="math inline">\(Z\)</span> to equal <span class="math inline">\(z,\)</span> then if <span class="math inline">\(Y=y,\)</span> $ X$ must be equal to <span class="math inline">\(z-y\)</span>. The probability (density) for that to occur is <span class="math inline">\(P_{Y}(y) P_{X}(z-y)\)</span>. To obtain the total probability (density) that <span class="math inline">\(Z=z,\)</span> we need to sum this product over all possible values of <span class="math inline">\(Y,\)</span> and this gives the expression for <span class="math inline">\(P_{Z}(z)\)</span> above.</p>
</blockquote>
<h4 id="mean-and-variance">mean and variance</h4>
<p>It will be useful to know the mean and variance of a random variable that is the sum of two or more random variables.</p>
<p>It turns out that if <span class="math inline">\(X=X_{1}+X_{2},\)</span> then the mean of <span class="math inline">\(X\)</span> is <span class="math display">\[
\langle X\rangle=\left\langle X_{1}\right\rangle+\left\langle X_{2}\right\rangle
\]</span> and if <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span> are independent, then <span class="math display">\[
V_{X}=V_{X_{1}}+V_{X_{2}}
\]</span></p>
<h4 id="example-2">example</h4>
<p>averaging the results of a number of independent measurements produces a more accurate result.</p>
<blockquote>
<p>为什么多次测量结果平均会更加准确？</p>
</blockquote>
<p>measurement results add together. If all the measurements are made using the same method, we can assume the results of all the measurements have the same mean, <span class="math inline">\(\mu,\)</span> and variance, <span class="math inline">\(V .\)</span> If we average the results, <span class="math inline">\(X_{n},\)</span> of <span class="math inline">\(N\)</span> of these independent measurements, then the mean of the average is <span class="math display">\[
\mu_{ av }=\left\langle\sum_{n=1}^{N} \frac{X_{n}}{N}\right\rangle=\sum_{n=1}^{N} \frac{\mu}{N}=\mu
\]</span></p>
<p><span class="math display">\[
V_{ av }=V\left[\sum_{n=1}^{N} \frac{X_{n}}{N}\right]=\sum_{n=1}^{N} \frac{V}{N^{2}}=\frac{V}{N}
\]</span> Thus the variance gets smaller as we add more results together. Of course, it is not the variance that quantifies the uncertainty in the final value, but the standard deviation. The standard deviation of each measurement result is <span class="math inline">\(\sigma=\sqrt{V},\)</span> and hence the standard deviation of the average is <span class="math display">\[
\sigma_{ av }=\sqrt{\frac{V}{N}}=\frac{\sigma}{\sqrt{N}}
\]</span> The accuracy of the average therefore increases as the square root of the number of measurements.</p>
<h3 id="transformations-of-a-random-variable">Transformations of a random variable</h3>
<p>transformation of a random variable <span class="math display">\[
Y=a X+b
\]</span> To calculate the probability density <span class="math display">\[
\langle f(Y)\rangle=\int_{-\infty}^{\infty} P(y) f(y) d y
\]</span></p>
<h3 id="characteristic-function">Characteristic function</h3>
<p>The characteristic function is defined as the Fourier transform of the probability density. <span class="math display">\[
\chi(s)=\int_{-\infty}^{\infty} P(x) e^{i s x} d x
\]</span> This inverse transform is <span class="math display">\[
P(x)=\frac{1}{2 \pi} \int_{-\infty}^{\infty} \chi(s) e^{-i s x} d s
\]</span></p>
<p>Another very useful property is the following. If we have two functions <span class="math inline">\(F(x)\)</span> and <span class="math inline">\(G(x),\)</span> then the Fourier transform of their convolution is simply the product of their respective Fourier transforms! This can be very useful because a product is always easy to calculate, but a convolution is not. Because the density for the sum of two random variables in the convolution of their respect densities, we now have an alternate way to find the probability density of the sum of two random variables: we can either convolve their two densities, or we can calculate the characteristic functions for each, multiply these together, and then take the inverse Fourier transform.</p>
<p>Showing that the Fourier transform of the convolution of two densities is the product of their respective Fourier transforms is not difficult, but we do need to use the Dirac <span class="math inline">\(\delta\)</span> -function, denoted by <span class="math inline">\(\delta(x)\)</span>. The Dirac <span class="math inline">\(\delta\)</span> -function is zero everywhere except at <span class="math inline">\(t=0,\)</span> where it is infinite. It is defined in such a way that it integrates to</p>
<h2 id="微分方程">微分方程</h2>
<p>We now have two first-order differential equations for the two functions <span class="math inline">\(x(t)\)</span> and <span class="math inline">\(p(t)\)</span> <span class="math display">\[
\frac{d x}{d t}=\frac{p}{m} \quad \text { and } \quad \frac{d p}{d t}=-\left(\frac{k}{m}\right) x
\]</span> We can now write this set of first-order differential equations in the "vector form" <span class="math display">\[
\begin{aligned}
\left(\begin{array}{l}
d x / d t \\
d p / d t
\end{array}\right)&amp;=\left(\begin{array}{c}
p / m \\
-k x / m
\end{array}\right)\\
\frac{d}{d t}\left(\begin{array}{l}
x \\
p
\end{array}\right)&amp;=\frac{1}{m}\left(\begin{array}{cc}
0 &amp; 1 \\
-k &amp; 0
\end{array}\right)\left(\begin{array}{l}
x \\
p
\end{array}\right) 
\end{aligned}
\]</span> Defining <span class="math inline">\(x =(x, p)^{ T }\)</span> and <span class="math inline">\(A\)</span> as the matrix <span class="math display">\[
A=\frac{1}{m}\left(\begin{array}{cc}
0 &amp; 1 \\
-k &amp; 0
\end{array}\right)
\]</span> we can write the set of equations in the compact form <span class="math display">\[
\dot{ x } \equiv \frac{d x }{d t}=A x
\]</span> We now introduce another way of writing differential equations, as this will be most useful for the stochastic differential equations that we will encounter later. <span class="math display">\[
d x=\frac{d x}{d t} d t
\]</span> We can write our differential equations in terms of <span class="math inline">\(d x\)</span> and <span class="math inline">\(d t\)</span> <span class="math display">\[
d\left(\begin{array}{l}
x \\
p
\end{array}\right) \equiv\left(\begin{array}{l}
d x \\
d p
\end{array}\right)=\frac{1}{m}\left(\begin{array}{c}
p d t \\
-k x d t
\end{array}\right)=\frac{1}{m}\left(\begin{array}{cc}
0 &amp; 1 \\
-k &amp; 0
\end{array}\right)\left(\begin{array}{l}
x \\
p
\end{array}\right) d t
\]</span> or in the more compact form <span class="math display">\[
d x =A x d t
\]</span></p>
<h3 id="solving-differential-equations">solving differential equations</h3>
<h4 id="separation-of-variables">separation of variables</h4>
<p>solving a first-order differential equation of the form <span class="math display">\[
\frac{d x}{d t}=g(t) f(x)
\]</span> is to divide by <span class="math inline">\(f(x),\)</span> multiply by <span class="math inline">\(d t,\)</span> and then integrate both sides: <span class="math display">\[
\int \frac{d x}{f(x)}=\int g(t) d t+C
\]</span> This method is called "separation of variables'</p>
<h4 id="linear-differential-equation">linear differential equation</h4>
<p><span class="math display">\[
d x=-\gamma x d t
\]</span></p>
<p>This tells us that the value of <span class="math inline">\(x\)</span> at time <span class="math inline">\(t+d t\)</span> is the value at time <span class="math inline">\(t\)</span> plus <span class="math inline">\(d x\)</span>. That is <span class="math display">\[
x(t+d t)=x(t)-\gamma x(t) d t=(1-\gamma d t) x(t)
\]</span> To solve this we note that to first order in <span class="math inline">\(d t\)</span> (that is, when <span class="math inline">\(d t\)</span> is very small) <span class="math inline">\(e^{\alpha d t} \approx 1+\alpha d t .\)</span> We can therefore write the equation for <span class="math inline">\(x(t+d t)\)</span> as <span class="math display">\[
\begin{aligned}
x(t+d t)&amp;=x(t)-\gamma x(t) d t\\&amp;=e^{-\gamma d t} x(t)
\end{aligned}
\]</span> This tells us that to move <span class="math inline">\(x\)</span> from time <span class="math inline">\(t\)</span> to <span class="math inline">\(t+d t\)</span> we merely have to multiply <span class="math inline">\(x(t)\)</span> by the factor <span class="math inline">\(e^{-\gamma d t} .\)</span> So to move by two lots of <span class="math inline">\(d t\)</span> we simply multiply by this factor twice: <span class="math display">\[
\begin{aligned}
x(t+2 d t)&amp;=e^{-\gamma d t} x(t+d t)
\\&amp;=e^{-\gamma d t}\left[e^{-\gamma d t} x(t)\right]\\&amp;=e^{-\gamma 2 d t} x(t)
\end{aligned}
\]</span> To obtain <span class="math inline">\(x(t+\tau)\)</span> all we have to do is apply this relation repeatedly. Let us say that <span class="math inline">\(d t=\tau / N\)</span> for <span class="math inline">\(N\)</span> as large as we want. Thus <span class="math inline">\(d t\)</span> is a small but finite time-step, and we can make it as small as we want. That means that to evolve <span class="math inline">\(x\)</span> from time <span class="math inline">\(t\)</span> to <span class="math inline">\(t+\tau\)</span> <span class="math display">\[
x(t+\tau)=\left(e^{-\gamma d t}\right)^{N} x(t)=e^{-\gamma \sum_{n=1}^{N} d t} x(t)=e^{-\gamma N d t} x(t)=e^{-\gamma \tau} x(t)
\]</span> is the solution to the differential equation. If <span class="math inline">\(\gamma\)</span> is a function of time, so that the equation becomes <span class="math display">\[
d x=-\gamma(t) x d t
\]</span> we can still use the above technique. As before we set <span class="math inline">\(d t=\tau / N\)</span> so that it is a small finite time-step. But this time we have to explicitly take the limit as <span class="math inline">\(N \rightarrow \infty\)</span> to obtain the solution to the differential equation: <span class="math display">\[
\begin{aligned}
x(t+\tau) &amp;=\lim _{N \rightarrow \infty} \Pi_{n=1}^{N} e^{-\gamma(t+n d t) d t} x(t) \\
&amp;=\lim _{N \rightarrow \infty} e^{-\sum_{n=1}^{N} \gamma(t+n d t) d t} x(t) \\
&amp;=e^{-\int_{t}^{t+\tau} \gamma(t) d t} x(t)
\end{aligned}
\]</span> The equation we have just solved is the simplest linear differential equation. All linear differential equations can be written in the form of <span class="math display">\[
d x=A x d t
\]</span></p>
<h3 id="linear-differential-equation-with-driving">linear differential equation with driving</h3>
<blockquote>
<p>解题思路：把 <span class="math inline">\(x(t)\)</span> 的微分方程转换成 <span class="math inline">\(y(t)\)</span> 的微分方程</p>
</blockquote>
<p>we consider the simple linear differential equation with the addition of a driving term. <span class="math display">\[
\frac{d x}{d t}=-\gamma x+f(t)
\]</span> To solve this we first transform to a new variable. <span class="math display">\[
y(t)=x(t) e^{\gamma t}
\]</span> differential <span class="math display">\[
\frac{d y}{d t}=\left(\frac{\partial y}{\partial x}\right) \frac{d x}{d t}+\frac{\partial y}{\partial t}=e^{\gamma t} f(t)
\]</span> The equation for <span class="math inline">\(y\)</span> is solved merely by integrating both sides, and the solution is <span class="math display">\[
y(t)=y_{0}+\int_{0}^{t} e^{\gamma s} f(s) d s
\]</span> where we have defined <span class="math inline">\(y_{0}\)</span> as the value of <span class="math inline">\(y\)</span> at time <span class="math inline">\(t=0 .\)</span> Now we can easily obtain <span class="math inline">\(x(t)\)</span> from <span class="math inline">\(y(t)\)</span> by inverting Eq. <span class="math inline">\((2.21) .\)</span> This gives us the solution<br />
<span class="math display">\[
\begin{aligned}
x(t)&amp;=x_{0} e^{-\gamma t}+e^{-\gamma t} \int_{0}^{t} e^{\gamma s} f(s) d s\\
&amp;=x_{0} e^{-\gamma t}+\int_{0}^{t} e^{-\gamma(t-s)} f(s) d s
\end{aligned}
\]</span> 如果 <span class="math inline">\(\frac{d x}{d t}=-\gamma x+f(t)\)</span> 的<span class="math inline">\(\gamma(t)\)</span> 包含时间</p>
<p>In this case we transform to <span class="math inline">\(y(t)=x(t) \exp [\Gamma(t)]\)</span>, where <span class="math display">\[
\Gamma(t) \equiv \int_{0}^{t} \gamma(s) d s
\]</span> the solution is <span class="math display">\[
x(t)=x_{0} e^{-\Gamma(t)}+e^{-\Gamma(t)} \int_{0}^{t} e^{\Gamma(s)} f(s) d s
\]</span></p>
<h3 id="vector-linear-differential-equations">vector linear differential equations</h3>
<p>We can usually solve a linear equation with more that one variable, <span class="math display">\[
\dot{ x }=A x
\]</span> by transforming to a new set of variables, <span class="math inline">\(y =U x ,\)</span> where <span class="math inline">\(U\)</span> is a matrix chosen so that the equations for the new variables are decoupled from each other. That is, the equation for the vector <span class="math inline">\(y\)</span> is <span class="math display">\[
\dot{ y }=D y
\]</span> where <span class="math inline">\(D\)</span> is a diagonal matrix. <span class="math display">\[
D=\left(\begin{array}{cccc}
\lambda_{1} &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \lambda_{2} &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \lambda_{N}
\end{array}\right)
\]</span></p>
<blockquote>
<p>上面的主体思路，把解微分方程变成了求解矩阵 <span class="math inline">\(A\)</span> 对角化的幺正变换 <span class="math inline">\(U\)</span> 。那么 <span class="math inline">\(U\)</span> 存在吗？</p>
</blockquote>
<ol type="1">
<li><p>For many square matrices <span class="math inline">\(A\)</span>, there exists a matrix <span class="math inline">\(U\)</span> so that <span class="math inline">\(D\)</span> is diagonal. This is only actually guaranteed if <span class="math inline">\(A\)</span> is normal, which means that <span class="math inline">\(A^{\dagger} A=A A^{\dagger}\)</span>. Here <span class="math inline">\(A^{\dagger}\)</span> is the Hermitian conjugate of <span class="math inline">\(A,\)</span> defined as the transpose of the complex conjugate of <span class="math inline">\(A\)</span>.</p></li>
<li><p>If there is no <span class="math inline">\(U\)</span> that gives a diagonal <span class="math inline">\(D,\)</span> then one must solve the differential equation using Laplace transforms instead, a method that is described in most textbooks on differential equations.</p></li>
</ol>
<h4 id="diagonal">Diagonal</h4>
<p>If <span class="math inline">\(U\)</span> exists then it is unitary, which means that <span class="math inline">\(U^{\dagger} U=U U^{\dagger}=I .\)</span> The diagonal elements of <span class="math inline">\(D\)</span> are called the eigenvalues of <span class="math inline">\(A\)</span>.</p>
<p>for each element of <span class="math inline">\(y\)</span> (each variable), <span class="math inline">\(y_{n},\)</span> we have the simple equation <span class="math display">\[
\dot{y}_{n}=\lambda_{n} y_{n}
\]</span> and this has the solution <span class="math display">\[
y_{n}(t)=y_{n}(0) e^{\lambda_{n} t}
\]</span> The solution for <span class="math inline">\(y\)</span> is thus <span class="math display">\[
y (t)=\left(\begin{array}{cccc}
e^{\lambda_{1} t} &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; e^{\lambda_{2} t} &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; e^{\lambda_{N} t}
\end{array}\right) y (0) \equiv e^{D t} y (0)
\]</span> since <span class="math inline">\(y=U x\)</span> then <span class="math display">\[
\begin{aligned}
x &amp;=U^{\dagger} y\\
&amp;=U^{\dagger}e^{D t} y(0) \\
&amp;=U^{\dagger} e^{D t} U x (0)
\end{aligned}
\]</span></p>
<h3 id="matrix-function">matrix function</h3>
<p>define the exponential of any square matrix <span class="math inline">\(A\)</span> as <span class="math display">\[
e^{A t}=U^{\dagger} e^{D t} U
\]</span> Proof:</p>
<p>the equation for the vector <span class="math inline">\(y\)</span> is <span class="math display">\[
\dot{ y }=D y
\]</span> by substituting <span class="math inline">\(x\)</span> into the differential equation for <span class="math inline">\(y\)</span> (where <span class="math inline">\(y=U x\)</span>) <span class="math display">\[
\begin{aligned}
\dot{ y }&amp;=D y\\
 \dot{ x }U&amp;=D U x\\
\dot{ x }&amp;=U^{\dagger} D U x
\end{aligned}
\]</span> and thus <span class="math inline">\(A t=U^{\dagger} D t U .\)</span> Because of this, the power series <span class="math display">\[
\begin{aligned}
\sum_{n=0}^{\infty} \frac{(A t)^{n}}{n !} &amp;=1+A t+\frac{(A t)^{2}}{2}+\frac{(A t)^{3}}{3 !}+\cdots \\
&amp;=1+U^{\dagger} D t U+\frac{U^{\dagger} D t U U^{\dagger} D t U}{2}+\frac{\left(U^{\dagger} D t U\right)^{3}}{3 !}+\cdots \\
&amp;=1+U^{\dagger} D t U+\frac{U^{\dagger}(D t)^{2} U}{2}+\frac{U^{\dagger}(D t)^{3} U}{3 !}+\cdots \\
&amp;=U^{\dagger}\left(1+D t+\frac{(D t)^{2}}{2}+\frac{(D t)^{3}}{3 !}+\cdots\right) U \\
&amp;=U^{\dagger} e^{D t} U
\end{aligned}
\]</span> So <span class="math inline">\(U^{\dagger} e^{D t} U\)</span> corresponds precisely to the power series <span class="math inline">\(\sum_{n}^{\infty}(A t)^{n} / n !\)</span>, which is the natural generalization of the exponential function for a matrix <span class="math inline">\(A t .\)</span> Since the above relationship holds for all power series, the natural definition of any function of a square matrix <span class="math inline">\(A\)</span> is <span class="math display">\[
f(A) \equiv U^{\dagger} f(D) U \equiv U^{\dagger}\left(\begin{array}{cccc}
f\left(\lambda_{1}\right) &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; f\left(\lambda_{2}\right) &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; f\left(\lambda_{N}\right)
\end{array}\right) U
\]</span></p>
<h4 id="summarize">summarize</h4>
<p>To summarize the above results, the solution to the vector differential equation <span class="math display">\[
\dot{ x }=A x
\]</span> is <span class="math display">\[
x (t)=e^{A t} x (0)
\]</span> where <span class="math display">\[
e^{A t} \equiv \sum_{n}^{\infty} \frac{(A t)^{n}}{n !}=U^{\dagger} e^{D t} U
\]</span> We can now also solve any linear vector differential equation with driving, just as we did for the single-variable linear equation above. The solution to <span class="math display">\[
\dot{ x }=A x + f (t)
\]</span> where <span class="math inline">\(f\)</span> is now a vector of driving terms, is <span class="math display">\[
x (t)=e^{A t} x (0)+\int_{0}^{t} e^{A(t-s)} f (s) d s
\]</span></p>
<h2 id="包含高斯噪声的随机方程">包含高斯噪声的随机方程</h2>
<p>We have seen that a differential equation for <span class="math inline">\(x\)</span> is an equation that tells us how <span class="math inline">\(x\)</span> changes in each infinitesimal time-step <span class="math inline">\(d t .\)</span> In the differential equations we have considered so far, this increment in <span class="math inline">\(x\)</span> is deterministic <span class="math inline">\(-\)</span> that is, it is completely determined at each time by the differential equation. Now we are going to consider a situation in which this increment is not completely determined, but is a random variable. This is the subject of stochastic differential equations.</p>
<ul>
<li>deterministic</li>
<li>random variable: not completely determined</li>
</ul>
<h3 id="discrete-stochastic-difference-equation-sde">discrete stochastic difference equation (SDE)</h3>
<p>The difference equation for <span class="math inline">\(x\)</span> is <span class="math display">\[
\Delta x\left(t_{n}\right)=x\left(t_{n}\right) \Delta t+f\left(t_{n}\right) \Delta t
\]</span> where</p>
<ul>
<li><span class="math inline">\(f\left(t_{n}\right) \Delta t\)</span> is the driving term.</li>
<li>discrete-time type of <span class="math inline">\(\frac{d x}{d t}=-\gamma x+f(t)\)</span></li>
</ul>
<p>Given the value of <span class="math inline">\(x\)</span> at time <span class="math inline">\(t_{n},\)</span> the value at time <span class="math inline">\(t_{n+1}\)</span> is then <span class="math display">\[
\begin{aligned}
x\left(t_{n}+\Delta t\right)&amp;=x\left(t_{n}\right)+\Delta x\left(t_{n}\right)\\&amp;=x\left(t_{n}\right)+x\left(t_{n}\right) \Delta t+f\left(t_{n}\right) \Delta t
\end{aligned}
\]</span> If we know the value of <span class="math inline">\(x\)</span> at <span class="math inline">\(t=0,\)</span> then <span class="math display">\[
x(\Delta t)=x(0)(1+\Delta t)+f(0) \Delta t
\]</span> Now, what we are really interested in is what happens if the driving term, <span class="math inline">\(f\left(t_{n}\right) \Delta t\)</span> is random at each time <span class="math inline">\(t_{n} ?\)</span> This means replacing <span class="math inline">\(f\left(t_{n}\right)\)</span> with a random variable, <span class="math inline">\(y_{n},\)</span> at each time <span class="math inline">\(t_{n} .\)</span> Now the difference equation for <span class="math inline">\(x\)</span> becomes <span class="math display">\[
\begin{aligned}
x\left(t_{n}+\Delta t\right)&amp;=x\left(t_{n}\right)+x\left(t_{n}\right) \Delta t+f\left(t_{n}\right) \Delta t\\
x\left(t_{n}+\Delta t\right)-x\left(t_{n}\right)&amp;=x\left(t_{n}\right) \Delta t+f\left(t_{n}\right) \Delta t\\
\Delta x\left(t_{n}\right)&amp;=x\left(t_{n}\right) \Delta t+y_{n} \Delta t
\end{aligned}
\]</span> and this is called a "stochastic difference equation".</p>
<p>This equation says that at each time <span class="math inline">\(t_{n}\)</span></p>
<ul>
<li>we pick a value for the random variable <span class="math inline">\(y_{n}\)</span> (sampled from its probability density)</li>
<li>and add <span class="math inline">\(y_{n} \Delta t\)</span> to <span class="math inline">\(x\left(t_{n}\right)\)</span>.</li>
</ul>
<p>This means that we can <strong>no longer predict</strong> exactly what <span class="math inline">\(x\)</span> will be at some future time, <span class="math inline">\(T,\)</span> <strong>until we arrive at that time,</strong> and all the values of the random increments up until <span class="math inline">\(T\)</span> have been determined.</p>
<p>The solution for <span class="math inline">\(x\)</span> at time <span class="math inline">\(\Delta t\)</span> is <span class="math display">\[
x(\Delta t)=x(0)(1+\Delta t)+y_{0} \Delta t
\]</span> So <span class="math inline">\(x(\Delta t)\)</span> is now a random variable.</p>
<ul>
<li>If <span class="math inline">\(x(0)\)</span> is fixed (that is, not random), then <span class="math inline">\(x(\Delta t)\)</span> is just a linear transformation of the random variable <span class="math inline">\(y_{0}\)</span>.</li>
<li>If <span class="math inline">\(x(0)\)</span> is also random, then <span class="math inline">\(x(\Delta t)\)</span> is a linear combination of the two random variables <span class="math inline">\(x(0)\)</span> and <span class="math inline">\(y_{0} .\)</span></li>
</ul>
<p>Similarly, if we go to the next time-step, and calculate <span class="math inline">\(x(2 \Delta t),\)</span> then this is a function of <span class="math inline">\(x(0)\)</span> <span class="math inline">\(y_{0}\)</span> and <span class="math inline">\(y_{1} .\)</span> We see that at each time then, the solution to a stochastic difference equation, <span class="math inline">\(x\left(t_{n}\right),\)</span> is a random variable, and this random variable changes at each time-step. To solve a stochastic difference equation, we must therefore determine the probability density for <span class="math inline">\(x\)</span> at all future times. Since <span class="math inline">\(x\left(t_{n}\right)\)</span> is a function of all the noise increments <span class="math inline">\(y_{n},\)</span> as well as <span class="math inline">\(x(0),\)</span> this means calculating the probability density for <span class="math inline">\(x\)</span> from the probability densities for the noise increments (and from the probability density for <span class="math inline">\(x(0)\)</span> if it is also random <span class="math inline">\() .\)</span> That is why we discussed summing random variables, and making transformations of random variables in Chapter <span class="math inline">\(1 .\)</span> Stochastic differential equations are obtained by taking the limit <span class="math inline">\(\Delta t \rightarrow 0\)</span> of stochastic difference equations. Thus the solution of a stochastic differential equation is also a probability density for the value of <span class="math inline">\(x\)</span> at each future time <span class="math inline">\(t .\)</span> Just as in the case of ordinary (deterministic) differential equations, it is not always possible to find a closed-form expression for the solution of a stochastic differential equation. But for a number of simple cases it is possible. For stochastic equations that cannot be solved analytically, we can solve them numerically using a computer, and we describe this later in Chapter 6 .</p>
<p>The reason that differential equations that are driven by random increments at each time-step are called stochastic differential equations is because a process that fluctuates randomly in time is called a stochastic process. Stochastic differential equations (SDEs) are thus driven by stochastic processes. The random increments that drive an SDE are also referred to as noise.</p>
<p>In addition to obtaining the probability density for <span class="math inline">\(x\)</span> at the future times <span class="math inline">\(t_{n},\)</span> we can also ask how <span class="math inline">\(x\)</span> evolves with time given a specific set of values for the random increments <span class="math inline">\(y_{n}\)</span>. A set of values of the random increments (values sampled from their probability densities) is called a realization of the noise. A particular evolution for <span class="math inline">\(x\)</span> given a specific noise realization is called a sample path for <span class="math inline">\(x .\)</span> In addition to wanting to know the probability density for <span class="math inline">\(x\)</span> at future times, it can also be useful to know what properties the sample paths of <span class="math inline">\(x\)</span> have. The full solution to an SDE is therefore really the complete set of all possible sample paths, and the probabilities for all these paths, but we don't usually need to know all this information. Usually all we need to know is the probability density for <span class="math inline">\(x\)</span> at each time, and how <span class="math inline">\(x\)</span> at one time is correlated with <span class="math inline">\(x\)</span> at another time. How we calculate the latter will be discussed in Chapter <span class="math inline">\(4 .\)</span> Note that since the solution to an <span class="math inline">\(\operatorname{SDE}, x(t),\)</span> varies randomly in time, it is a stochastic process. If one wishes, one can therefore view an SDE as something that takes as an input one stochastic process (the driving noise , and produces another.</p>
<blockquote>
<p>通过离散情况引出随机微分方程(SDE)，但是并没有给出求解方程的方法。下面一节以高斯噪声(Gaussian noise)为例子，求解随机微分方程(SDE)</p>
</blockquote>
<h3 id="gaussian-increments-and-the-continuum-limit">Gaussian increments and the continuum limit</h3>
<p>In this chapter we are going to study stochastic differential equations driven by Gaussian noise. By "Gaussian noise" we mean that each of the random increments has a Gaussian probability density.</p>
<h4 id="wiener-increment">1 Wiener increment</h4>
<p>First we consider the simplest stochastic difference equation <span class="math display">\[
\Delta x\left(t_{n}\right)=x\left(t_{n}\right) \Delta t+y_{n} \Delta t
\]</span> in which the increment of <span class="math inline">\(x, \Delta x,\)</span> consists solely of the random increment <span class="math inline">\(y_{n} \Delta t .\)</span> Since Gaussian noise is usually called <strong>Wiener noise</strong>, we will call the random increment <span class="math inline">\(\Delta W_{n}=y_{n} \Delta t .\)</span> The discrete differential equation for <span class="math inline">\(x\)</span> is thus <span class="math display">\[
\Delta x\left(t_{n}\right)=\Delta W_{n}
\]</span> Each Wiener increment is completely independent of all the others, and all have the same probability density, given by <span class="math display">\[
P(\Delta W)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-(\Delta W)^{2} /\left(2 \sigma^{2}\right)}
\]</span> This density is a Gaussian with zero mean, and we choose the variance to be <span class="math inline">\(\sigma^{2}=\)</span> <span class="math inline">\(V=\Delta t .\)</span> This choice for the variance of the Wiener increment is very important, and we will see why shortly. We will often denote a Wiener increment in some time-step <span class="math inline">\(\Delta t\)</span> simply as <span class="math inline">\(\Delta W\)</span>, without reference to the subscript <span class="math inline">\(n .\)</span> This notation is convenient because all the random increments have the same probability density, and even though independent of each other, are in that sense identical.</p>
<h4 id="initial">2 initial</h4>
<p>We can easily solve the difference equation for <span class="math inline">\(x\)</span> simply by starting with <span class="math inline">\(x(0)=\)</span> <span class="math inline">\(0,\)</span> and repeatedly adding <span class="math inline">\(\Delta x .\)</span> This gives the solution <span class="math display">\[
x_{n} \equiv x(n \Delta t)=\sum_{i=0}^{n-1} \Delta W_{i}
\]</span> So now we need to calculate the probability density for <span class="math inline">\(x_{n} .\)</span> Since we know from Exercise 5 in Chapter 1 that the <strong>sum of</strong> two Gaussian random variables is also a Gaussian, we know that the probability density for <span class="math inline">\(x_{n}\)</span> is Gaussian. We also know that the mean and variance of <span class="math inline">\(x_{n}\)</span> is the sum of the means and variances of the <span class="math inline">\(\Delta W_{i}\)</span> because the <span class="math inline">\(\Delta W_{i}\)</span> are all independent (see Section <span class="math inline">\(\left.1.5\right)\)</span>. We therefore have <span class="math display">\[
\begin{array}{c}
\left\langle x_{n}\right\rangle=0 \\
V\left(x_{n}\right)=n \Delta t
\end{array}
\]</span> and so <span class="math display">\[
P\left(x_{n}\right)=\frac{1}{\sqrt{2 \pi V}} e^{-x_{n}^{2} /(2 V)}=\frac{1}{\sqrt{2 \pi n \Delta t}} e^{-x_{n}^{2} /(2 n \Delta t)}
\]</span> #### 3 continue limit</p>
<p>We now need to move from difference equations to differential equations. To do so we will consider solving the difference equation above, Eq. (3.6), for a given future time <span class="math inline">\(T\)</span>, with <span class="math inline">\(N\)</span> discrete time-steps. (So each time-step is <span class="math inline">\(\Delta t=T / N .\)</span> ) We then take the limit as <span class="math inline">\(N \rightarrow \infty\)</span>. Proceeding in the same way as above, the solution <span class="math inline">\(x(T)\)</span> is now <span class="math display">\[
x(T)=\lim _{N \rightarrow \infty} \sum_{i=0}^{N-1} \Delta W_{i} \equiv \int_{0}^{T} d W(t) \equiv W(T)
\]</span> Here we define a stochastic integral, <span class="math inline">\(W(T)=\int_{0}^{T} d W(t),\)</span> as the limit of the sum of all the increments of the Wiener process. A stochastic integral, being the sum of a bunch of random variables, is therefore a random variable. The important thing is that in many cases we can calculate the probability density for this random variable. We can easily do this in the case above because we know that the probability density for <span class="math inline">\(x(T)\)</span> is Gaussian (since it is merely the sum of many independent Gaussian variables). Because of this, we need only to calculate its mean and variance. The mean is zero, because all the random variables in the sum have zero mean. To calculate the variance of <span class="math inline">\(x(T)\)</span> it turns out that we don't even need to take the limit as <span class="math inline">\(N \rightarrow \infty\)</span> because <span class="math inline">\(N\)</span> factors out of the expression: <span class="math display">\[
\begin{aligned}
V(x(T))&amp;=\sum_{i=0}^{N-1} V\left[\Delta W_{i}\right]\\&amp;=\sum_{i=0}^{N-1} \Delta t\\&amp;=N \Delta t\\&amp;=N\left(\frac{T}{N}\right)\\&amp;=T
\end{aligned}
\]</span> The probability density for <span class="math inline">\(W(T)\)</span> is therefore <span class="math display">\[
P(W(T))=P(x(T)) \equiv P(x, T)=\frac{1}{\sqrt{2 \pi T}} e^{-x^{2} /(2 T)}
\]</span> Note that when writing the integral over the Wiener increments, we have included explicitly as an argument to <span class="math inline">\(d W,\)</span> just to indicate that <span class="math inline">\(d W\)</span> changes with time. We will often drop the explicit dependence of <span class="math inline">\(d W\)</span> on <span class="math inline">\(t,\)</span> and just write the stochastic integral as <span class="math display">\[
W(T)=\int_{0}^{T} d W
\]</span> While we often loosely refer to the increments <span class="math inline">\(d W\)</span> as the "Wiener process", the Wiener process is actually defined as <span class="math inline">\(W(T),\)</span> and <span class="math inline">\(d W\)</span> is, strictly speaking, an increment of the Wiener process.</p>
<p>The fact that the variance of <span class="math inline">\(x(T)\)</span> is proportional to <span class="math inline">\(T\)</span> is a result of the fact that we chose each of the Wiener increments to have variance <span class="math inline">\(\Delta t .\)</span> Since there is one Wiener increment in each time step <span class="math inline">\(\Delta t,\)</span> the variance of <span class="math inline">\(x\)</span> grows by precisely <span class="math inline">\(\Delta t\)</span> in each interval <span class="math inline">\(\Delta t,\)</span> and is thus proportional to <span class="math inline">\(t .\)</span> So what would happen if we chose <span class="math inline">\(V[\Delta W(\Delta t)]\)</span> to be some other power of <span class="math inline">\(\Delta t ?\)</span> To find this out we set <span class="math inline">\(V[\Delta W(\Delta t)]=\Delta t^{\alpha}\)</span> and calculate once again the variance of <span class="math inline">\(x(T)\)</span> (before taking the limit as <span class="math inline">\(N \rightarrow \infty\)</span> ). This gives <span class="math display">\[
\begin{aligned}
V(x(T))&amp;=\sum_{i=0}^{N-1} V\left[\Delta W_{i}\right]\\&amp;=N(\Delta t)^{\alpha}
\\&amp;=N\left(\frac{T}{N}\right)^{\alpha}
\\&amp;=N^{(1-\alpha)} T^{\alpha}
\end{aligned}
\]</span> Now we take the continuum limit <span class="math inline">\(N \rightarrow \infty\)</span> so as to obtain a stochastic differential equation. When <span class="math inline">\(\alpha\)</span> is greater than one we have <span class="math display">\[
\lim _{N \rightarrow \infty} V(x(T))=T^{\alpha} \lim _{N \rightarrow \infty} N^{(1-\alpha)}=0
\]</span> and when <span class="math inline">\(\alpha\)</span> is less than one we have <span class="math display">\[
\lim _{N \rightarrow \infty} V(x(T))=T^{\alpha} \lim _{N \rightarrow \infty} N^{(1-\alpha)} \rightarrow \infty
\]</span> Neither of these make sense for the purposes of obtaining a stochastic differential equation that describes real systems driven by noise. Thus we are forced to choose <span class="math inline">\(\alpha=1\)</span> and hence <span class="math inline">\(V\left(\Delta W_{n}\right) \propto \Delta t\)</span></p>
<p>When we are working in the continuum limit the Gaussian increments, <span class="math inline">\(d W,\)</span> are referred to as being infinitesimal. A general SDE for a single variable <span class="math inline">\(x(t)\)</span> is then written as <span class="math display">\[
d x=f(x, t) d t+g(x, t) d W
\]</span> Since the variance of <span class="math inline">\(d W\)</span> must be proportional to <span class="math inline">\(d t,\)</span> and since any constant of proportionality can always be absorbed into <span class="math inline">\(g(x, t),\)</span> the variance of <span class="math inline">\(d W\)</span> is defined to be equal to <span class="math inline">\(d t .\)</span> We can therefore write the probability density for <span class="math inline">\(d W\)</span> as <span class="math display">\[
P(d W)=\frac{e^{-(d W)^{2} /(2 d t)}}{\sqrt{2 \pi d t}}
\]</span></p>
<p>To summarize the main result of this section: if we have an SDE driven by an infinitesimal increment <span class="math inline">\(d W\)</span> in each infinitesimal interval <span class="math inline">\(d t,\)</span> then if all the increments are Gaussian, independent and identical, they must have variance proportional to <span class="math inline">\(d t\)</span></p>
<h3 id="why-gaussian-noise">why Gaussian noise?</h3>
<p>In this chapter we consider only noise that is Gaussian. This kind of noise is important because it is very common in physical systems. The reason for this is a result known as the central limit theorem. The central limit theorem says that if one sums together many independent random variables, then the probability density of the sum will be close to a Gaussian. As the number of variables in the sum tends to infinity, the resulting probability density tends exactly to a Gaussian. The only condition on the random variables is they all have a finite variance.</p>
<p>Now consider noise in a physical system. This noise is usually the result of many random events happening at the microscopic level. This could be the impacts of individual molecules, the electric force from many electrons moving randomly in a conductor, or the thermal jiggling of atoms in a solid. The total force applied by these microscopic particles is the sum of the random forces applied by each. Because the total force is the sum over many random variables, it has a Gaussian probability density. Since the microscopic fluctuations are usually fast compared to the motion of the system, we can model the noise as having a Gaussian probability density in each time-step <span class="math inline">\(\Delta t,\)</span> where <span class="math inline">\(\Delta t\)</span> is small compared to the time-scale on which the system moves. In fact, assuming that the noise increments are completely independent of each other from one infinitesimal time-step to the next is really an idealization (an approximation) which is not true in practice. Nevertheless, this approximation works very well, and we will explain why at the end of Section 4.6</p>
<p>From a mathematical point of view, simple noise processes in which the random increment in each time interval <span class="math inline">\(d t\)</span> is independent of all the previous random increments will usually be Gaussian for the same reason. This is because the random increment in each small but finite time interval <span class="math inline">\(\Delta t\)</span> is the sum over the infinite number of increments for the infinitesimal intervals <span class="math inline">\(d t\)</span> that make up that finite interval. There are two exceptions to this. One is processes in which the random increment in an infinitesimal time-step <span class="math inline">\(d t\)</span> is not necessarily infinitesimal. The sample paths of such processes make instant and discrete jumps from time to time, and are thus not continuous. These are called jump or point processes, and we consider them in Chapter <span class="math inline">\(8 .\)</span> Jump processes are actually quite common and have many applications. The other exception, which is much rarer in nature, happens when the noise increments remain infinitesimal, as in Gaussian noise, but are drawn from a probability density with an infinite variance (one that avoids the central limit theorem). These processes will be discussed in Chapter 9</p>
<h3 id="ito-calculus">Ito calculus</h3>
<p>We saw in the previous section that the increments of Wiener noise, <span class="math inline">\(d W,\)</span> have a variance proportional to <span class="math inline">\(d t .\)</span> In this section we are going to discover a very surprising consequence of this fact, and the most unusual aspect of Wiener noise and stochastic equations. To set the stage, consider how we obtained the solution of the simple differential equation <span class="math display">\[
d x=-\gamma x d t
\]</span> We solved this equation in Section 2.4 using the relation <span class="math inline">\(e^{\alpha d t} \approx 1+\alpha d t .\)</span> This relation is true because <span class="math inline">\(d t\)</span> is a differential <span class="math inline">\(-\)</span> that is, it is understood that when calculating the solution, <span class="math inline">\(x(t),\)</span> we will always take the limit in which <span class="math inline">\(d t \rightarrow 0,\)</span> just as we did in solving the simple stochastic equation in the previous section (in fact, we can regard the use of the symbol <span class="math inline">\(d t\)</span> as a shorthand notation for the fact that this limit will be taken). The approximation <span class="math inline">\(e^{\alpha d t} \approx 1+\alpha d t\)</span> works because the terms in the power series expansion for <span class="math inline">\(e^{\alpha d t}\)</span> that are second-order or higher in <span class="math inline">\(d t\left(d t^{2},\right.\)</span> <span class="math inline">\(d t^{3},\)</span> etc. ) will vanish in comparison to <span class="math inline">\(d t\)</span> as <span class="math inline">\(d t \rightarrow 0 .\)</span></p>
<p>The result of being able to ignore terms that are second-order and higher in the infinitesimal increment leads to the usual rules for differential equations. (It also means that any equation we write in terms of differentials <span class="math inline">\(d x\)</span> and <span class="math inline">\(d t\)</span> can alternatively be written in terms of derivatives.) However, we will now show that the second power of the stochastic differential <span class="math inline">\(d W\)</span> does not vanish with respect to <span class="math inline">\(d t,\)</span> and we must therefore learn a new rule for the manipulation of stochastic differential equations. It will also mean that we cannot write stochastic differential equations in terms of derivatives - we must use differentials.</p>
<p>Solving a differential equation involves summing the infinitesimal increments over all the time-steps <span class="math inline">\(d t .\)</span> To examine whether <span class="math inline">\((d W)^{2}\)</span> makes a non-zero contribution to the solution, we must therefore sum <span class="math inline">\((d W)^{2}\)</span> over all the time-steps for a finite time <span class="math inline">\(T .\)</span> To do this we will return to a discrete description so that we can explicitly write down the sum and then take the continuum limit.</p>
<p>The first thing to note is that the expectation value of <span class="math inline">\((\Delta W)^{2}\)</span> is equal to the variance of <span class="math inline">\(\Delta W,\)</span> because <span class="math inline">\(\langle\Delta W\rangle=0 .\)</span> Thus <span class="math inline">\(\left\langle(\Delta W)^{2}\right\rangle=\Delta t .\)</span> This tells us immediately that the expectation value of <span class="math inline">\((\Delta W)^{2}\)</span> does not vanish with respect to the time-step, <span class="math inline">\(\Delta t,\)</span> and so the sum of these increments will not vanish when we sum over all the time-steps and take the infinitesimal limit. In fact, the expectation value of the sum</p>
<h2 id="解一些随机微分方程">解一些随机微分方程</h2>
<h3 id="the-ornstein-uhlenbeck-process">The Ornstein-Uhlenbeck process</h3>
<p>The Ornstein-Uhlenbeck process <span class="math display">\[
d x=-\gamma x d t+g d W
\]</span> We now change variables in the SDE to <span class="math inline">\(y=x e^{y t}\)</span>. The result is <span class="math display">\[
y(t)=y_{0}+g \int_{0}^{t} e^{\gamma s} d W(s)
\]</span> discrete time-steps <span class="math display">\[
y(t)=y(N \Delta t)=y_{0}+\lim _{N \rightarrow \infty} g \sum_{n=0}^{N-1} e^{\gamma n \Delta t} \Delta W_{n}
\]</span> limit <span class="math display">\[
\begin{aligned}
V[y(t)] &amp;=\lim _{N \rightarrow \infty} \sum_{n=0}^{N-1} g^{2} e^{2 \gamma n \Delta t} \Delta t \\
&amp;=g^{2} \int_{0}^{t} e^{2 \gamma s} d s=\frac{g^{2}}{\gamma}\left(e^{2 \gamma t}-1\right)
\end{aligned}
\]</span> transform back to <span class="math inline">\(x\)</span> using <span class="math inline">\(x=y e^{-\gamma t} .\)</span> The solution is <span class="math display">\[
\begin{aligned}
x(t)&amp;=x_{0} e^{-\gamma t}+g e^{-\gamma t} \int_{0}^{t} e^{\gamma s} d W(s)\\&amp;=x_{0} e^{-\gamma t}+g \int_{0}^{t} e^{\gamma(s-t)} d W(s)
\end{aligned}
\]</span> It is worth noting that the solution we have obtained is exactly the same solution that we would get if we replaced the driving noise by a deterministic function of time, <span class="math inline">\(f(t) .\)</span></p>
<blockquote>
<p>This would mean replacing <span class="math inline">\(d W\)</span> with <span class="math inline">\(f(t) d t .\)</span> The solution has exactly the same form because the method we used above to solve the stochastic equation was exactly the same method that we used in Chapter 2 to solve the same equation but with deterministic driving.</p>
</blockquote>
<p>That is, to solve this stochastic equation we do not need to use Ito calculus - normal calculus is sufficient. This can be seen immediately by noting that nowhere did <span class="math inline">\((d W)^{2}\)</span> appear in the analysis, because <span class="math inline">\(d^{2} y / d x^{2}=0 .\)</span> This is always true if the term giving the driving noise in the equation (the term containing <span class="math inline">\(d W\)</span> ) does not contain <span class="math inline">\(x .\)</span> When the driving noise does depend on <span class="math inline">\(x,\)</span> then we cannot obtain the solution by assuming that <span class="math inline">\(d W=f(t) d t\)</span> for some deterministic function <span class="math inline">\(f(t),\)</span> and must use Ito calculus to get the solution. In the next section we show how to solve the simplest equation of this type.</p>
<p>We can also solve the Ornstein-Uhlenbeck stochastic equation when <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(g\)</span> are functions of time, using essentially the same method. That is, we change to a new variable that has the deterministic part of the dynamics removed, and proceed as before. We leave the details of the derivation as an exercise. The solution in this case is <span class="math display">\[
x(t)=x_{0} e^{-\Gamma(t)}+\int_{0}^{t} e^{\Gamma(s)-\Gamma(t)} g(s) d W(s)
\]</span> where <span class="math display">\[
\Gamma(t)=\int_{0}^{t} \gamma(s) d s
\]</span></p>
<h3 id="the-full-linear-stochastic-equation">The full linear stochastic equation</h3>
<h3 id="multiple-variables-and-multiple-noise-sources">Multiple variables and multiple noise sources</h3>
<p>Stochastic equations can, of course, be driven by more than one Gaussian noise source. These noise sources can be independent, or mutually correlated, but as we now explain, all correlated Gaussian noise sources can be obtained in a simple way from independent sources. We can solve stochastic differential equations driven by two (or more) independent Wiener noises using the same methods as those described above for a single noise source, along with the additional rule that the product of the increments of different noise sources are zero. As an example, to solve the equation <span class="math display">\[
d x=f(x, t) d t+g_{1}(x, t) d W_{1}+g_{2}(x, t) d W_{2}
\]</span> the Ito rules are <span class="math display">\[
\begin{aligned}
\left(d W_{1}\right)^{2}=\left(d W_{2}\right)^{2} &amp;=d t \\
d W_{1} d W_{2} &amp;=0
\end{aligned}
\]</span> To obtain two Wiener noise processes that are correlated, all we need to do is to form linear combinations of independent Wiener processes. If we define noise sources <span class="math inline">\(d V_{1}\)</span> and <span class="math inline">\(d V_{2}\)</span> by <span class="math display">\[
\left(\begin{array}{l}
d V_{1} \\
d V_{2}
\end{array}\right)=M\left(\begin{array}{l}
d W_{1} \\
d W_{2}
\end{array}\right)=\left(\begin{array}{cc}
\sqrt{1-\eta^{2}} &amp; \eta \\
\eta &amp; \sqrt{1-\eta^{2}}
\end{array}\right)\left(\begin{array}{l}
d W_{1} \\
d W_{2}
\end{array}\right)
\]</span> with <span class="math inline">\(-1 \leq \eta \leq 1,\)</span> then <span class="math inline">\(d V_{1}\)</span> and <span class="math inline">\(d V_{2}\)</span> are correlated even though <span class="math inline">\(d W_{1}\)</span> and <span class="math inline">\(d W_{2}\)</span> are not. The covariance matrix for <span class="math inline">\(d W_{1}\)</span> and <span class="math inline">\(d W_{2}\)</span> is <span class="math inline">\(I d t\)</span> (where <span class="math inline">\(I\)</span> is the two-by-two identity matrix), and that for <span class="math inline">\(d V_{1}\)</span> and <span class="math inline">\(d V_{2}\)</span> is <span class="math display">\[
\begin{aligned}
\left(\begin{array}{cc}
\left\langle\left(d V_{1}\right)^{2}\right\rangle &amp; \left\langle d V_{1} d V_{2}\right\rangle \\
\left\langle d V_{1} d V_{2}\right\rangle &amp; \left\langle\left(d V_{2}\right)^{2}\right\rangle
\end{array}\right) &amp;=\left\langle\left(\begin{array}{c}
d V_{1} \\
d V_{2}
\end{array}\right)\left(d V_{1}, d V_{2}\right)\right\rangle \\
&amp;=\left\langle\left(\begin{array}{c}
d W_{1} \\
d W_{2}
\end{array}\right) M M^{ T }\left(d W_{1}, d W_{2}\right)\right.\\
&amp;=\left(\begin{array}{cc}
1 &amp; C \\
C &amp; 1
\end{array}\right) d t
\end{aligned}
\]</span> where <span class="math display">\[
C=\eta \sqrt{1-\eta^{2}}
\]</span> Since the means of <span class="math inline">\(d V_{1}\)</span> and <span class="math inline">\(d V_{2}\)</span> are zero, and because we have pulled out the factor of <span class="math inline">\(d t\)</span> in Eq. <span class="math inline">\((3.74), C\)</span> is in fact the correlation coefficient of <span class="math inline">\(d V_{1}\)</span> and <span class="math inline">\(d V_{2}\)</span>.</p>
<p>The set of Ito calculus relations for <span class="math inline">\(d V_{1}\)</span> and <span class="math inline">\(d V_{2}\)</span> are given by essentially the same calculation: <span class="math display">\[
\begin{aligned}
\left(\begin{array}{cc}
\left(d V_{1}\right)^{2} &amp; d V_{1} d V_{2} \\
d V_{1} d V_{2} &amp; \left(d V_{2}\right)^{2}
\end{array}\right) &amp;=\left(\begin{array}{c}
d W_{1} \\
d W_{2}
\end{array}\right) M M^{ T }\left(d W_{1}, d W_{2}\right) \\
&amp;=\left(\begin{array}{cc}
1 &amp; C \\
C &amp; 1
\end{array}\right) d t
\end{aligned}
\]</span> If we have a stochastic equation driven by two correlated noise sources, such as <span class="math display">\[
d x=f(x, t) d t+g_{1}(x, t) d V_{1}+g_{2}(x, t) d V_{2}=f(x, t) d t+ g \cdot d V
\]</span> with <span class="math inline">\(d V \equiv\left(d V_{1}, d V_{2}\right)^{ T },\)</span> then we can always rewrite this in terms of the uncorrelated noises: <span class="math display">\[
d x=f(x, t) d t+ g \cdot d V =f(x, t) d t+ g ^{ T } M d W
\]</span> where we have defined <span class="math inline">\(d W =\left(d W_{1}, d W_{2}\right)^{ T }\)</span> More generally, we can always write <span class="math inline">\(N\)</span> correlated Gaussian noise processes, <span class="math inline">\(d V ,\)</span> in terms of <span class="math inline">\(N\)</span> independent Wiener processes, <span class="math inline">\(d W\)</span>. If we want the processes <span class="math inline">\(d V\)</span> to have the covariance matrix <span class="math inline">\(C,\)</span> so that <span class="math display">\[
d V d V ^{ T }=C d t
\]</span> then we define <span class="math display">\[
d V =M d W
\]</span> where <span class="math inline">\(M\)</span> is the square root of <span class="math inline">\(C .\)</span> One can calculate the symmetric matrix <span class="math inline">\(M\)</span> from <span class="math inline">\(C\)</span> by using the definition of a function of a matrix given in Chapter 2 : one diagonalizes <span class="math inline">\(C\)</span> and then takes the square root of all the eigenvalues to construct <span class="math inline">\(M .\)</span> (Note. Actually <span class="math inline">\(M\)</span> does not have to be symmetric. If <span class="math inline">\(M\)</span> is not symmetric, then <span class="math inline">\(\left.C=M M^{ T } .\right)\)</span> To summarize, when considering stochastic equations driven by multiple Gaussian noise sources, we only ever need to consider equations driven by independent Wiener processes.</p>
<h3 id="itos-formula-for-multiple-variables">Ito's formula for multiple variables</h3>
<p>A general Ito stochastic differential equation that has multiple variables can be written in the vector form <span class="math display">\[
d x = f ( x , t) d t+G( x , t) d W
\]</span> Here <span class="math inline">\(x , f\)</span>, and <span class="math inline">\(d W\)</span> are the vectors <span class="math display">\[
x =\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{N}
\end{array}\right) \quad f =\left(\begin{array}{c}
f_{1}( x , t) \\
f_{2}( x , t) \\
\vdots \\
f_{N}( x , t)
\end{array}\right) \quad d W =\left(\begin{array}{c}
d W_{1} \\
d W_{2} \\
\vdots \\
d W_{M}
\end{array}\right)
\]</span> where the <span class="math inline">\(d W_{i}\)</span> are a set of mutually independent noise sources. They satisfy <span class="math display">\[
d W_{i} d W_{j}=\delta_{i j} d t
\]</span> The symbol <span class="math inline">\(G\)</span> is the <span class="math inline">\(N \times M\)</span> matrix <span class="math display">\[
G( x , t)=\left(\begin{array}{cccc}
G_{11}( x , t) &amp; G_{12}( x , t) &amp; \ldots &amp; G_{1 M}( x , t) \\
G_{21}( x , t) &amp; G_{22}( x , t) &amp; \ldots &amp; G_{2 M}( x , t) \\
\vdots &amp; \vdots &amp; \ddots &amp; \\
G_{N 1}( x , t) &amp; G_{N 2}( x , t) &amp; \ldots &amp; G_{N M}( x , t)
\end{array}\right)
\]</span> To determine Ito's formula for transforming stochastic equations involving multiple variables, all we have to do is use the multi-variable Taylor expansion. Let us say that we wish to transform from a set of variables <span class="math inline">\(x =\left(x_{1}, \ldots, x_{N}\right)^{ T },\)</span> to a set of variables <span class="math inline">\(y =\left(y_{1}, \ldots, y_{L}\right)^{ T },\)</span> where each of the <span class="math inline">\(y_{i}\)</span> is a function of some or all of the</p>
<h2 id="小结">小结</h2>
<p>微分方程和对应的解</p>
<ul>
<li><span class="math inline">\(d x=-\gamma x d t\)</span> 一个时间步</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
x(t+d t)=e^{-\gamma d t} x(t)
\end{aligned}
\]</span></p>
<ul>
<li><span class="math inline">\(d x=-\gamma(t) x d t\)</span> 多个时间步，<span class="math inline">\(\gamma(t)\)</span> 包含时间</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
x(t+\tau)=e^{-\int_{t}^{t+\tau} \gamma(t) d t} x(t)
\end{aligned}
\]</span></p>
<ul>
<li><span class="math inline">\(\frac{d x}{d t}=-\gamma x+f(t)\)</span> 加上驱动项</li>
</ul>
<p><span class="math display">\[
x(t)=x_{0} e^{-\Gamma(t)}+e^{-\Gamma(t)} \int_{0}^{t} e^{\Gamma(s)} f(s) d s
\]</span></p>
<ul>
<li><span class="math inline">\(\dot{x}=A x\)</span> 一个变量换成一组变量</li>
</ul>
<p><span class="math display">\[
x (t)=e^{A t} x (0)+\int_{0}^{t} e^{A(t-s)} f (s) d s
\]</span></p>
<p>The Ornstein-Uhlenbeck process <span class="math display">\[
d x=-\gamma x d t+g d W
\]</span> We now change variables in the SDE to <span class="math inline">\(y=x e^{y t}\)</span>. The result is <span class="math display">\[
y(t)=y_{0}+g \int_{0}^{t} e^{\gamma s} d W(s)
\]</span> discrete time-steps <span class="math display">\[
y(t)=y(N \Delta t)=y_{0}+\lim _{N \rightarrow \infty} g \sum_{n=0}^{N-1} e^{\gamma n \Delta t} \Delta W_{n}
\]</span> limit <span class="math display">\[
\begin{aligned}
V[y(t)] &amp;=\lim _{N \rightarrow \infty} \sum_{n=0}^{N-1} g^{2} e^{2 \gamma n \Delta t} \Delta t \\
&amp;=g^{2} \int_{0}^{t} e^{2 \gamma s} d s=\frac{g^{2}}{\gamma}\left(e^{2 \gamma t}-1\right)
\end{aligned}
\]</span> transform back to <span class="math inline">\(x\)</span> using <span class="math inline">\(x=y e^{-\gamma t} .\)</span> The solution is</p>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Mathematics/">Mathematics</a>
                    
                      <a class="hover-with-bg" href="/categories/Mathematics/%E9%9A%8F%E6%9C%BA/">随机</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Stochastic/">Stochastic</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2021/02/24/Ftmb_07_Kardar_02/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">统计场</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2021/02/15/QC02_QML_CN01/">
                        <span class="hidden-mobile">笔记- HHL算法</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div id="tocbot"></div>
</div>


      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>







  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 1,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "随机过程基础&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js" ></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  
















</body>
</html>
