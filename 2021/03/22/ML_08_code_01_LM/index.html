<!DOCTYPE html>
<html lang="en">





<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="JPZhuang">
  <meta name="keywords" content="">
  <title>放码过来 - JPZ</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
<meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Physics</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner intro-2" id="background" parallax=true
         style="background: url('/img/tag-bg.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fade-in-up">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2021-03-22 13:23">
      March 22, 2021 pm
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      5.7k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      78
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <p>[TOC]</p>
<h2 id="参考文献">参考文献</h2>
<ol type="1">
<li><a href="https://zh-v2.d2l.ai/index.html" target="_blank" rel="noopener">《动手学深度学习》第二版</a> 3种框架</li>
<li><a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/" target="_blank" rel="noopener">《动手学深度学习》PyTorch版</a></li>
</ol>
<h2 id="线性回归">1 线性回归</h2>
<blockquote>
<p>这个模型简单到好理解，也是构成神经网络的基本构件</p>
</blockquote>
<p>特征两个</p>
<ul>
<li>房屋的面积为 <span class="math inline">\(x_{1}\)</span></li>
<li>房龄为 <span class="math inline">\(x_{2}\)</span></li>
</ul>
<p>模型 <span class="math display">\[
\hat{y}=x_{1} w_{1}+x_{2} w_{2}+b
\]</span> 其中</p>
<ul>
<li><span class="math inline">\(w_{1}\)</span> 和 <span class="math inline">\(w_{2}\)</span> 是权重 (weight)</li>
<li><span class="math inline">\(b\)</span> 是偏差 (bias)<br />
</li>
<li>它们是线性回归模型的参数</li>
<li><span class="math inline">\(\hat{y}\)</span> 是模型预测出的房价</li>
</ul>
<h3 id="训练数据">(1) 训练数据</h3>
<p>通常收集一系列的真实数据</p>
<ol type="1">
<li>该数据集被称为训练数据集（training data set）或训练集（training set），</li>
<li>一栋房屋被称为一个样本（sample）</li>
<li>其真实售出价格叫作标签（label）</li>
<li>用来预测标签的两个因素叫作特征（feature）</li>
</ol>
<p>对应的符号</p>
<ul>
<li>采集的样本数为 <span class="math inline">\(n,\)</span></li>
<li>样本的索引为 <span class="math inline">\(i\)</span> ，表示第几个样本</li>
<li>索引为 <span class="math inline">\(i\)</span> 的样本的特征为 <span class="math inline">\(x_{1}^{(i)}\)</span> 和 <span class="math inline">\(x_{2}^{(i)}\)</span></li>
<li>索引为 <span class="math inline">\(i\)</span> 的样本的标签为 <span class="math inline">\(y^{(i)}\)</span></li>
</ul>
<p>有时候真实数据不方便收集，所以我们自己产生数据。</p>
<p>具体数据</p>
<ol type="1">
<li>训练数据集样本数为 <span class="math inline">\(n=1000\)</span></li>
<li>每个样本有2个特征</li>
<li>训练数据 <span class="math inline">\(X \in R ^{1000 \times 2}\)</span> 随机产生</li>
<li>标签的生成：线性回归模型+随机噪声项 <span class="math inline">\(\epsilon\)</span></li>
</ol>
<p><span class="math display">\[
y = X w +b+\epsilon
\]</span></p>
<h4 id="代码">代码</h4>
<pre><code class="hljs python"><span class="hljs-comment">## 定义基本参数</span>
num_inputs = <span class="hljs-number">2</span>
num_examples = <span class="hljs-number">1000</span>
true_w = [<span class="hljs-number">2</span>, <span class="hljs-number">-3.4</span>]
true_b = <span class="hljs-number">4.2</span>
<span class="hljs-comment"># 随机产生一个符合正态分布的数据</span>
features = torch.randn(num_examples, num_inputs,
                       dtype=torch.float32)
<span class="hljs-comment"># 线性回归模型：权重(weight)与特征数据(X)的线性函数</span>
<span class="hljs-comment"># python技巧，通过features[:, 0]的`：`来历遍所有样本中的第1个特征</span>
labels = true_w[<span class="hljs-number">0</span>] * features[:, <span class="hljs-number">0</span>] + true_w[<span class="hljs-number">1</span>] * features[:, <span class="hljs-number">1</span>] + true_b
<span class="hljs-comment"># 标签的生成：线性回归模型+随机噪声项</span>
labels += torch.tensor(np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>, size=labels.size()),
                       dtype=torch.float32)

<span class="hljs-comment">## 把数据画出来</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">use_svg_display</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-comment"># 用矢量图显示</span>
    display.set_matplotlib_formats(<span class="hljs-string">'svg'</span>)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">set_figsize</span><span class="hljs-params">(figsize=<span class="hljs-params">(<span class="hljs-number">3.5</span>, <span class="hljs-number">2.5</span>)</span>)</span>:</span>
    use_svg_display()
    <span class="hljs-comment"># 设置图的尺寸</span>
    plt.rcParams[<span class="hljs-string">'figure.figsize'</span>] = figsize

<span class="hljs-comment"># # 在../d2lzh_pytorch里面添加上面两个函数后就可以这样导入</span>
<span class="hljs-comment"># import sys</span>
<span class="hljs-comment"># sys.path.append("..")</span>
<span class="hljs-comment"># from d2lzh_pytorch import * </span>

set_figsize()
plt.scatter(features[:, <span class="hljs-number">1</span>].numpy(), labels.numpy(), <span class="hljs-number">1</span>);</code></pre>
<p>随机数的产生</p>
<pre><code class="hljs python">features = torch.randn(<span class="hljs-number">4</span>, <span class="hljs-number">2</span>, dtype=torch.float32)
<span class="hljs-comment"># 返回一个张量，包含了从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取的一组随机数。</span>
print(features)
<span class="hljs-comment"># 输出结果</span>
tensor([[ <span class="hljs-number">2.3289</span>,  <span class="hljs-number">1.0474</span>],
        [<span class="hljs-number">-1.7138</span>, <span class="hljs-number">-1.4371</span>],
        [<span class="hljs-number">-0.8568</span>, <span class="hljs-number">-0.1856</span>],
        [ <span class="hljs-number">0.2729</span>,  <span class="hljs-number">1.4254</span>]])
<span class="hljs-comment">#拓展</span>
torch.rand(*sizes, out=<span class="hljs-literal">None</span>) 
<span class="hljs-comment"># 返回一个张量，包含了从区间[0, 1)的均匀分布中抽取的一组随机数。张量的形状由参数sizes定义。</span></code></pre>
<p>作图</p>
<p>我们将上面的<code>plt</code>作图函数以及<code>use_svg_display</code>函数和<code>set_figsize</code>函数定义在<code>d2lzh_pytorch</code>包里。以后在作图时，我们将直接调用<code>d2lzh_pytorch.plt</code>。由于<code>plt</code>在<code>d2lzh_pytorch</code>包中是一个全局变量，我们在作图前只需要调用<code>d2lzh_pytorch.set_figsize()</code>即可打印矢量图并设置图的尺寸。</p>
<p>读取数据</p>
<p>在训练模型的时候，我们需要遍历数据集并不断读取小批量数据样本。这里我们定义一个函数：它每次返回<code>batch_size</code>（批量大小）个随机样本的特征和标签。</p>
<pre><code class="hljs python"><span class="hljs-comment"># 本函数已保存在d2lzh包中方便以后使用</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">data_iter</span><span class="hljs-params">(batch_size, features, labels)</span>:</span>
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)  <span class="hljs-comment"># 样本的读取顺序是随机的</span>
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, num_examples, batch_size):
        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) <span class="hljs-comment"># 最后一次可能不足一个batch</span>
        <span class="hljs-keyword">yield</span>  features.index_select(<span class="hljs-number">0</span>, j), labels.index_select(<span class="hljs-number">0</span>, j)</code></pre>
<h3 id="损失函数">(2) 损失函数</h3>
<blockquote>
<p>评价一个函数与数据拟合的好坏程度</p>
</blockquote>
<p>在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。第 <span class="math inline">\(i\)</span> 的样本误差函数 <span class="math display">\[
\ell^{(i)}\left(w_{1}, w_{2}, b\right)=\frac{1}{2}\left(\hat{y}^{(i)}-y^{(i)}\right)^{2}
\]</span> 其中</p>
<ul>
<li>预测值 <span class="math inline">\(\hat{y}^{(i)}=x_{1}^{(i)} w_{1}+x_{2}^{(i)} w_{2}+b\)</span></li>
<li>常数 <span class="math inline">\(\frac{1}{2}\)</span> 使对平方项求导后的常数系数为1，这样在形式上稍微简单一些。</li>
<li>显然，误差越小表示预测价格与真实价格越相近，且当二者相等时误差为0。</li>
</ul>
<p>在机器学习里，将衡量误差的函数称为损失函数（loss function）</p>
<p>用训练数据集中所有样本误差的平均来衡量模型预测的质量 <span class="math display">\[
\begin{aligned}
\ell\left(w_{1}, w_{2}, b\right)
&amp;=\frac{1}{n} \sum_{i=1}^{n} \ell^{(i)}\left(w_{1}, w_{2}, b\right)\\
&amp;=\frac{1}{n} \sum_{i=1}^{n} \frac{1}{2}\left(x_{1}^{(i)} w_{1}+x_{2}^{(i)} w_{2}+b-y^{(i)}\right)^{2} 
\end{aligned}
\]</span> 在模型训练中，我们希望找出一组模型参数, 记为 <span class="math inline">\(w_{1}^{*}, w_{2}^{*}, b^{*},\)</span> 来使训练样本平均损失最小: <span class="math display">\[
w_{1}^{*}, w_{2}^{*}, b^{*}=\underset{w_{1}, w_{2}, b}{\arg \min } \ell\left(w_{1}, w_{2}, b\right)
\]</span></p>
<p><span class="math display">\[
w_{1}^{*}, w_{2}^{*}, b^{*}=\underset{w_{1}, w_{2}, b}{\arg \min } \ell\left(w_{1}, w_{2}, b\right)
\]</span></p>
<h4 id="代码-初始">代码 初始</h4>
<p>寻找一组模型参数，一开始的时候我们不知道参数多少，就随机化初始值</p>
<pre><code class="hljs python"><span class="hljs-comment"># 权重初始化成均值为0、标准差为0.01的正态随机数，偏差则初始化成0</span>
w = torch.tensor(np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>, (num_inputs, <span class="hljs-number">1</span>)), dtype=torch.float32)
b = torch.zeros(<span class="hljs-number">1</span>, dtype=torch.float32)
<span class="hljs-comment"># 之后的模型训练中，需要对这些参数求梯度来迭代参数的值，因此我们要让它们的requires_grad=True。</span>
w.requires_grad_(requires_grad=<span class="hljs-literal">True</span>)
b.requires_grad_(requires_grad=<span class="hljs-literal">True</span>)</code></pre>
<p>补充</p>
<ul>
<li>参考 <a href="https://pytorch-cn.readthedocs.io/zh/latest/notes/autograd/" target="_blank" rel="noopener">自动求导机制</a></li>
</ul>
<p><code>requires_grad</code> 如果有一个单一的输入操作需要梯度，它的输出也需要梯度。相反，只有所有输入都不需要梯度，输出才不需要。如果其中所有的变量都不需要梯度进行，后向计算不会在子图中执行。</p>
<pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.autograd <span class="hljs-keyword">import</span> Variable
x = Variable(torch.randn(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>))
y = Variable(torch.randn(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>))
z = Variable(torch.randn(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), requires_grad=<span class="hljs-literal">True</span>)
a = x + y
a.requires_grad
<span class="hljs-comment"># 输出结果 False</span>
b = a + z
b.requires_grad
<span class="hljs-comment"># 输出结果 True</span></code></pre>
<p>这个标志特别有用，当您想要冻结部分模型时，或者您事先知道不会使用某些参数的梯度。</p>
<h4 id="代码-定义模型">代码 定义模型</h4>
<pre><code class="hljs python"><span class="hljs-comment"># 线性回归的矢量计算表达式</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">linreg</span><span class="hljs-params">(X, w, b)</span>:</span>  
    <span class="hljs-keyword">return</span> torch.mm(X, w) + b</code></pre>
<p>补充</p>
<p><code>torch.mm(input, mat2, *, out=None) → Tensor</code></p>
<p>If input is a <span class="math inline">\((n \times m)\)</span> tensor, mat2 is a <span class="math inline">\((m \times p)\)</span> tensor, out will be a $(n p) $ tensor.</p>
<pre><code class="hljs python"><span class="hljs-comment"># torch.mm的使用方法：两个矩阵相乘 </span>
<span class="hljs-comment"># torch.mm(input, mat2, *, out=None) → Tensor</span>
mat1 = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)
mat2 = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)
torch.mm(mat1, mat2)
<span class="hljs-comment"># 输出</span>
tensor([[ <span class="hljs-number">0.8022</span>,  <span class="hljs-number">0.1761</span>, <span class="hljs-number">-0.0657</span>, <span class="hljs-number">-0.9012</span>],
        [ <span class="hljs-number">1.6339</span>,  <span class="hljs-number">0.6334</span>, <span class="hljs-number">-1.2002</span>,  <span class="hljs-number">2.5095</span>]])</code></pre>
<h4 id="代码-损失函数">代码 损失函数</h4>
<pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">squared_loss</span><span class="hljs-params">(y_hat, y)</span>:</span>  <span class="hljs-comment"># 本函数已保存在d2lzh_pytorch包中方便以后使用</span>
    <span class="hljs-comment"># 注意这里返回的是向量, 另外, pytorch里的MSELoss并没有除以 2</span>
    <span class="hljs-keyword">return</span> (y_hat - y.view(y_hat.size())) ** <span class="hljs-number">2</span> / <span class="hljs-number">2</span></code></pre>
<h3 id="优化算法">(3) 优化算法</h3>
<blockquote>
<p>找出一组模型参数的方法, 目标是使训练样本平均损失尽量最小</p>
</blockquote>
<ul>
<li>误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。</li>
<li>大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。</li>
</ul>
<p>在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用</p>
<ol type="1">
<li>选取一组模型参数的初始值，如随机选取；</li>
<li>对参数进行多次迭代，使每次迭代都可能降低损失函数的值</li>
<li>每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）<span class="math inline">\(B\)</span></li>
<li>然后求小批量中数据样本的平均损失有关模型参数的导数（梯度）</li>
<li>最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。</li>
</ol>
<p>以线性模型为例子：更新参数 <span class="math display">\[
\begin{aligned}
w_{1} &amp; \leftarrow w_{1}-\frac{\eta}{| B |} \sum_{i \in B } \frac{\partial \ell^{(i)}\left(w_{1}, w_{2}, b\right)}{\partial w_{1}} \\
w_{2} &amp; \leftarrow w_{2}-\frac{\eta}{| B |} \sum_{i \in B } \frac{\partial \ell^{(i)}\left(w_{1}, w_{2}, b\right)}{\partial w_{2}} \\
b &amp; \leftarrow b-\frac{\eta}{| B |} \sum_{i \in B } \frac{\partial \ell^{(i)}\left(w_{1}, w_{2}, b\right)}{\partial b}
\end{aligned}
\]</span> 其中 <span class="math display">\[
\begin{aligned}
&amp; w_{1}-\frac{\eta}{| B |} \sum_{i \in B } \frac{\partial \ell^{(i)}\left(w_{1}, w_{2}, b\right)}{\partial w_{1}}=w_{1}-\frac{\eta}{| B |} \sum_{i \in B } x_{1}^{(i)}\left(x_{1}^{(i)} w_{1}+x_{2}^{(i)} w_{2}+b-y^{(i)}\right) \\
&amp; w_{2}-\frac{\eta}{| B |} \sum_{i \in B } \frac{\partial \ell^{(i)}\left(w_{1}, w_{2}, b\right)}{\partial w_{2}}=w_{2}-\frac{\eta}{| B |} \sum_{i \in B } x_{2}^{(i)}\left(x_{1}^{(i)} w_{1}+x_{2}^{(i)} w_{2}+b-y^{(i)}\right) \\
&amp; b-\frac{\eta}{| B |} \sum_{i \in B } \frac{\partial \ell^{(i)}\left(w_{1}, w_{2}, b\right)}{\partial b}=b-\frac{\eta}{| B |} \sum_{i \in B }\left(x_{1}^{(i)} w_{1}+x_{2}^{(i)} w_{2}+b-y^{(i)}\right)
\end{aligned}
\]</span> 上式中</p>
<ul>
<li><span class="math inline">\(| B |\)</span> 代表每个小批量中的样本个数 (批量大小, batch size)</li>
<li><span class="math inline">\(\eta\)</span> 称作学习率 (learning rate) 并取正数。</li>
</ul>
<p>需要强调的是, 这里的批量大小和学习率的值是人为设定的, 并不是通过模型 训练学出的, 因此叫作超参数（hyperparameter) 。我们通常所说的“调参”指的正是调节超参数，例如通过反复试错来找到超参数合适的值。在少数情况下，超参数也可以通过模型训练学出。</p>
<h4 id="代码-1">代码</h4>
<p><code>sgd</code>函数实现了上一节中介绍的小批量随机梯度下降算法。它通过不断迭代模型参数来优化损失函数。这里自动求梯度模块计算得来的梯度是一个批量样本的梯度和。我们将它除以批量大小来得到平均值。</p>
<pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sgd</span><span class="hljs-params">(params, lr, batch_size)</span>:</span> 
    <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> params:
        param.data -= lr * param.grad / batch_size <span class="hljs-comment"># 注意这里更改param时用的param.data</span></code></pre>
<h3 id="用pytorch简洁实现模型">用PyTorch简洁实现模型</h3>
<pre><code class="hljs python"><span class="hljs-comment">#### 生成数据</span>
num_inputs = <span class="hljs-number">2</span>
num_examples = <span class="hljs-number">1000</span>
true_w = [<span class="hljs-number">2</span>, <span class="hljs-number">-3.4</span>]
true_b = <span class="hljs-number">4.2</span>
features = torch.tensor(np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, (num_examples, num_inputs)), dtype=torch.float)
labels = true_w[<span class="hljs-number">0</span>] * features[:, <span class="hljs-number">0</span>] + true_w[<span class="hljs-number">1</span>] * features[:, <span class="hljs-number">1</span>] + true_b
labels += torch.tensor(np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>, size=labels.size()), dtype=torch.float)

<span class="hljs-comment">#### 读取数据</span>
 
<span class="hljs-keyword">import</span> torch.utils.data <span class="hljs-keyword">as</span> Data
batch_size = <span class="hljs-number">10</span>
<span class="hljs-comment"># 将训练数据的特征和标签组合</span>
dataset = Data.TensorDataset(features, labels)
<span class="hljs-comment"># 随机读取小批量</span>
data_iter = Data.DataLoader(dataset, batch_size, shuffle=<span class="hljs-literal">True</span>)</code></pre>
<h3 id="用tensorflow实现模型">用Tensorflow实现模型</h3>
<h3 id="向量化">向量化</h3>
<p>处理大量数据的时候</p>
<ol type="1">
<li>逐一做标量加乘法，循环</li>
<li>向量加乘法</li>
</ol>
<p>向量方式计算速度快很多，公式表达也比较简洁</p>
<h4 id="公式对比">公式对比</h4>
<p><span class="math display">\[
\begin{array}{l}
\hat{y}^{(1)}=x_{1}^{(1)} w_{1}+x_{2}^{(1)} w_{2}+b \\
\hat{y}^{(2)}=x_{1}^{(2)} w_{1}+x_{2}^{(2)} w_{2}+b \\
\hat{y}^{(3)}=x_{1}^{(3)} w_{1}+x_{2}^{(3)} w_{2}+b
\end{array}
\]</span></p>
<p>将上面3个等式转化成矢量计算 <span class="math display">\[
\hat{y}= X w +b
\]</span> 其中 <span class="math display">\[
\hat{ y }=\left[\begin{array}{c}
\hat{y}^{(1)} \\
\hat{y}^{(2)} \\
\hat{y}^{(3)}
\end{array}\right], \quad X =\left[\begin{array}{cc}
x_{1}^{(1)} &amp; x_{2}^{(1)} \\
x_{1}^{(2)} &amp; x_{2}^{(2)} \\
x_{1}^{(3)} &amp; x_{2}^{(3)}
\end{array}\right], \quad w =\left[\begin{array}{c}
w_{1} \\
w_{2}
\end{array}\right]
\]</span> 重写损失函数</p>
<ul>
<li>设模型参数 <span class="math inline">\(\theta =\left[w_{1}, w_{2}, b\right]\)</span></li>
<li>数据样本标签 <span class="math inline">\(\hat{ y } \in R ^{n \times 1}\)</span></li>
<li>数据样本特征 <span class="math inline">\(X \in R ^{n \times d}\)</span></li>
<li>权重 <span class="math inline">\(w \in R ^{d \times 1}\)</span></li>
<li>偏差 <span class="math inline">\(b \in R\)</span></li>
</ul>
<p><span class="math display">\[
\ell^{(i)}\left(w_{1}, w_{2}, b\right)=\frac{1}{2}\left(\hat{y}^{(i)}-y^{(i)}\right)^{2}\\
\downarrow
\]</span></p>
<p><span class="math display">\[
\ell( \theta )=\frac{1}{2 n}(\hat{ y }- y )^{\top}(\hat{ y }- y )
\]</span></p>
<p>迭代步骤将相应地改写 <span class="math display">\[
\theta \leftarrow \theta -\frac{\eta}{| B |} \sum_{i \in B } \nabla_{ \theta } \ell^{(i)}( \theta )
\]</span> 其中梯度是损失有关3个为标量的模型参数的偏导数组成的向量 <span class="math display">\[
\begin{aligned}
\nabla_{ \theta } \ell^{(i)}( \theta )&amp;=\left[\begin{array}{l}
\frac{\partial \ell^{(i)}\left(w_{1}, w_{2}, b\right)}{\partial w_{1}} \\
\frac{\partial \ell^{(i)}{ }_{\left(w 1, w_{2}, b\right)}}{\partial w_{2}} \\
\frac{\partial \ell^{(i)}{ }_{\left(w_{1}, w_{2}, b\right)}}{\partial b}
\end{array}\right]\\&amp;
=\left[\begin{array}{c}
x_{1}^{(i)}\left(x_{1}^{(i)} w_{1}+x_{2}^{(i)} w_{2}+b-y^{(i)}\right) \\
x_{2}^{(i)}\left(x_{1}^{(i)} w_{1}+x_{2}^{(i)} w_{2}+b-y^{(i)}\right) \\
x_{1}^{(i)} w_{1}+x_{2}^{(i)} w_{2}+b-y^{(i)}
\end{array}\right]\\&amp;= 
\left[\begin{array}{c}
x_{1}^{(i)} \\
x_{2}^{(i)} \\
1
\end{array}\right]\left(\hat{y}^{(i)}-y^{(i)}\right)
\end{aligned}
\]</span></p>
<h4 id="代码对比">代码对比</h4>
<pre><code class="hljs python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> time

a = torch.ones(<span class="hljs-number">1000</span>)
b = torch.ones(<span class="hljs-number">1000</span>)

<span class="hljs-comment">## 两个向量按元素逐一做标量加法</span>
start = time()
c = torch.zeros(<span class="hljs-number">1000</span>)
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1000</span>):
    c[i] = a[i] + b[i]
print(time() - start)

<span class="hljs-comment">## 两个向量直接做矢量加法</span>
start = time()
d = a + b
print(time() - start)</code></pre>
<h3 id="从线性回归到深度网络">从线性回归到深度网络</h3>
<p>线性回归可以作为神经网络的结构 <span class="math display">\[
\hat{o}=x_{1} w_{1}+x_{2} w_{2}+b
\]</span> 这个线性回归是一个单层神经网络</p>
<ul>
<li>输入层：输入个数为2</li>
<li>输出层：输出个数为1</li>
<li>由于输入层并不涉及计算，神经网络的层数为1</li>
</ul>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210324205434685.png" srcset="/img/loading.gif" /></p>
<p>如果是一个分类问题，输出单元从一个变成了多个，这就是softmax回归</p>
<h2 id="softmax回归">2 softmax回归</h2>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210324205951773.png" srcset="/img/loading.gif" /> <span class="math display">\[
\begin{array}{l}
o_{1}=x_{1} w_{11}+x_{2} w_{21}+x_{3} w_{31}+x_{4} w_{41}+b_{1} \\
o_{2}=x_{1} w_{12}+x_{2} w_{22}+x_{3} w_{32}+x_{4} w_{42}+b_{2}, \\
o_{3}=x_{1} w_{13}+x_{2} w_{23}+x_{3} w_{33}+x_{4} w_{43}+b_{3} .
\end{array}
\]</span> 其中</p>
<ul>
<li>4种特征 <span class="math inline">\(x_{1}, x_{2}, x_{3}, x_{4}\)</span> 也就是输入层</li>
<li>3种输出 <span class="math inline">\(o_{1}, o_{2}, o_{3}\)</span> 也就是输出层</li>
<li>12个标量 <span class="math inline">\(w\)</span></li>
</ul>
<p>分类问题需要输出预测，直接使用输出层的输出有两个问题</p>
<ol type="1">
<li>由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。</li>
<li>由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。</li>
</ol>
<p>softmax运算符（softmax operator）解决了以上两个问题。它通过下式将输出值变换成值为正且和为1的概率分布 <span class="math display">\[
\hat{y}_{1}, \hat{y}_{2}, \hat{y}_{3}=\operatorname{softmax}\left(o_{1}, o_{2}, o_{3}\right)
\]</span> 其中</p>
<ul>
<li><span class="math inline">\(\hat{y}_{1}=\frac{\exp \left(o_{1}\right)}{\sum_{i=1}^{3} \exp (o_i)}\)</span></li>
<li><span class="math inline">\(\hat{y}_{2}=\frac{\exp \left(o_{2}\right)}{\sum_{i=1}^{3} \exp (o_i)}\)</span></li>
<li><span class="math inline">\(\hat{y}_{3}=\frac{\exp \left(o_{3}\right)}{\sum_{i=1}^{3} \exp \left(o_{i}\right)}\)</span></li>
</ul>
<p>容易看出</p>
<ol type="1">
<li><span class="math inline">\(\hat{y}_{1}+\hat{y}_{2}+\hat{y}_{3}=1\)</span></li>
<li><span class="math inline">\(0 \leq \hat{y}_{1}, \hat{y}_{2}, \hat{y}_{3} \leq 1\)</span></li>
<li>softmax运算不改变预测类别输出 <span class="math inline">\(\underset{i}{\arg \max } o_{i}=\underset{i}{\arg \max } \hat{y}_{i}\)</span></li>
</ol>
<h3 id="单样本分类的矢量表达式">单样本分类的矢量表达式</h3>
<p>为了提高计算效率，我们可以将单样本分类通过矢量计算来表达</p>
<ul>
<li>softmax回归的权重</li>
</ul>
<p><span class="math display">\[
W =\left[\begin{array}{lll}
w_{11} &amp; w_{12} &amp; w_{13} \\
w_{21} &amp; w_{22} &amp; w_{23} \\
w_{31} &amp; w_{32} &amp; w_{33} \\
w_{41} &amp; w_{42} &amp; w_{43}
\end{array}\right]
\]</span></p>
<ul>
<li>softmax回归的偏差参数</li>
</ul>
<p><span class="math display">\[
b =\left[\begin{array}{lll}
b_{1} &amp; b_{2} &amp; b_{3}
\end{array}\right]
\]</span></p>
<ul>
<li>高和宽分别为2个像素的图像样本 <span class="math inline">\(i\)</span> 的特征</li>
</ul>
<p><span class="math display">\[
x ^{(i)}=\left[\begin{array}{llll}
x_{1}^{(i)} &amp; x_{2}^{(i)} &amp; x_{3}^{(i)} &amp; x_{4}^{(i)}
\end{array}\right]
\]</span></p>
<ul>
<li>输出层</li>
</ul>
<p><span class="math display">\[
o ^{(i)}=\left[\begin{array}{lll}
o_{1}^{(i)} &amp; o_{2}^{(i)} &amp; o_{3}^{(i)}
\end{array}\right]
\]</span></p>
<ul>
<li>预测为狗、猫或鸡的概率分布</li>
</ul>
<p><span class="math display">\[
\hat{ y }^{(i)}=\left[\begin{array}{lll}
\hat{y}_{1}^{(i)} &amp; \hat{y}_{2}^{(i)} &amp; \hat{y}_{3}^{(i)}
\end{array}\right]
\]</span></p>
<p>softmax回归对样本 <em>i</em> 分类的矢量计算表达式为 <span class="math display">\[
\begin{array}{l} 
o ^{(i)}= x ^{(i)} W + b \\
\hat{ y }^{(i)}=\operatorname{softmax}\left( o ^{(i)}\right)
\end{array}
\]</span></p>
<h3 id="小批量样本分类的矢量表达式">小批量样本分类的矢量表达式</h3>
<p>为了进一步提升计算效率，我们通常对小批量数据做矢量计算。广义上讲，给定一个小批量样 本，</p>
<ul>
<li>批量大小为 <span class="math inline">\(n,\)</span></li>
<li>输入个数 <span class="math inline">\((\)</span> 特征数 <span class="math inline">\()\)</span> 为 <span class="math inline">\(d\)</span></li>
<li>输出个数 (类别数) 为 <span class="math inline">\(q\)</span></li>
<li>设批量特征为 <span class="math inline">\(X \in\)</span> <span class="math inline">\(R ^{n \times d}\)</span></li>
</ul>
<p>假设softmax回归的</p>
<ul>
<li>权重参数 <span class="math inline">\(W \in R ^{d \times q}\)</span></li>
<li>偏差参数 <span class="math inline">\(b \in R ^{1 \times q}\)</span></li>
</ul>
<p>softmax回归的矢量计算表达式为 <span class="math display">\[
\begin{array}{l} 
O = X W + b \\
\hat{ Y }=\operatorname{softmax}( O )
\end{array}
\]</span> 输出</p>
<ul>
<li><span class="math inline">\(O , \hat{ Y } \in R ^{n \times q}\)</span> 矩阵</li>
<li>矩阵的第 <span class="math inline">\(i\)</span> 行分别为样本 <span class="math inline">\(i\)</span> 的输出 <span class="math inline">\(o ^{(i)}\)</span> 和概率分布 <span class="math inline">\(\hat{ y }^{(i)}\)</span></li>
</ul>
<h3 id="损失函数交叉熵">损失函数：交叉熵</h3>
<p>我们可以像线性回归那样使用平方损失函数 <span class="math inline">\(\left\|\hat{ y }^{(i)}- y ^{(i)}\right\|^{2} / 2\)</span> 但是</p>
<ol type="1">
<li>想要预测分类结果正确，我们其实并不需要预测概率完全等于标签概率。平方损失则过于严格。</li>
<li>例如，在图像分类的例子里，我们只需要 <span class="math inline">\(\hat{y}_{3}^{(i)}\)</span> 比其他两个预测值 <span class="math inline">\(\hat{y}_{1}^{(i)}\)</span> 和 <span class="math inline">\(\hat{y}_{2}^{(i)}\)</span> 大就行了。</li>
<li>softmax运算将输出变换成一个合法的类别预测分布。</li>
</ol>
<p>使用更适合衡量两个概率分布差异的测量函数。其中，交叉熵（cross entropy）是一个常用的衡量方法： <span class="math display">\[
H\left( y ^{(i)}, \hat{ y }^{(i)}\right)=-\sum_{j=1}^{q} y_{j}^{(i)} \log \hat{y}_{j}^{(i)}
\]</span> 其中</p>
<ul>
<li>带下标的 <span class="math inline">\(y_{j}^{(i)}\)</span> 是向量 <span class="math inline">\(y ^{(i)}\)</span> 中非0即1的元素</li>
</ul>
<p>练数据集的样本数为 <span class="math inline">\(n,\)</span> 交叉熵损失函数定义为 <span class="math display">\[
\ell( \Theta )=\frac{1}{n} \sum_{i=1}^{n} H\left( y ^{(i)}, \hat{ y }^{(i)}\right)
\]</span> 其中</p>
<ul>
<li><span class="math inline">\(\Theta\)</span> 代表模型参数。</li>
<li>同样地, 如果每个样本只有一个标签, 那么交叉熵损失可以简写成 <span class="math inline">\(\ell( \Theta )=-(1 / n) \sum_{i=1}^{n} \log \hat{y}_{y^{(i)}}^{(i)}\)</span></li>
<li>从另一个角度来看, 我们知道最小化 <span class="math inline">\(\ell(\Theta)\)</span> 等价于最大化 <span class="math inline">\(\exp (-n \ell( \Theta ))=\prod_{i=1}^{n} \hat{y}_{y}^{(i)},\)</span> 即最小化交叉商损失函数等价于最大化训练数据集所有标签类别的联合预测概率。</li>
</ul>
<h3 id="code图像分类数据集fashion-mnist">code：图像分类数据集（Fashion-MNIST）</h3>
<p>动机</p>
<ol type="1">
<li>图像分类数据集中最常用的是手写数字识别数据集MNIST。但大部分模型在MNIST上的分类精度都超过了95%。</li>
<li>为了更直观地观察算法之间的差异，我们将使用一个图像内容更加复杂的数据集Fashion-MNIST</li>
<li>这个数据集也比较小，只有几十M，没有GPU的电脑也能吃得消</li>
</ol>
<p>Fashion-MNIST中一共包括了10个类别，分别为t-shirt（T恤）、trouser（裤子）、pullover（套衫）、dress（连衣裙）、coat（外套）、sandal（凉鞋）、shirt（衬衫）、sneaker（运动鞋）、bag（包）和ankle boot（短靴）</p>
<h4 id="torchvision包">torchvision包</h4>
<p>本节我们将使用torchvision包，它是服务于PyTorch深度学习框架的，主要用来构建计算机视觉模型。torchvision主要由以下几部分构成：</p>
<ol type="1">
<li><code>torchvision.datasets</code>: 一些加载数据的函数及常用的数据集接口；</li>
<li><code>torchvision.models</code>: 包含常用的模型结构（含预训练模型），例如AlexNet、VGG、ResNet等；</li>
<li><code>torchvision.transforms</code>: 常用的图片变换，例如裁剪、旋转等；</li>
<li><code>torchvision.utils</code>: 其他的一些有用的方法。</li>
</ol>
<h4 id="获取数据集">获取数据集</h4>
<pre><code class="hljs python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> sys
sys.path.append(<span class="hljs-string">".."</span>) <span class="hljs-comment"># 为了导入上层目录的d2lzh_pytorch</span>
<span class="hljs-keyword">import</span> d2lzh_pytorch <span class="hljs-keyword">as</span> d2l</code></pre>
<h3 id="简洁版本">简洁版本</h3>
<p>通过深度学习框架的高级API能够使实现线性回归变得更加容易</p>
<pre><code class="hljs python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l

batch_size = <span class="hljs-number">256</span>
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

<span class="hljs-comment"># PyTorch不会隐式地调整输入的形状。</span>
<span class="hljs-comment"># 因此，我们定义了展平层（flatten）在线性层前调整网络输入的形状</span>
net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="hljs-number">784</span>, <span class="hljs-number">10</span>))

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init_weights</span><span class="hljs-params">(m)</span>:</span>
    <span class="hljs-keyword">if</span> type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=<span class="hljs-number">0.01</span>)

net.apply(init_weights);

loss = nn.CrossEntropyLoss()

trainer = torch.optim.SGD(net.parameters(), lr=<span class="hljs-number">0.1</span>)

num_epochs = <span class="hljs-number">10</span>
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</code></pre>
<h2 id="多层感知机">3 多层感知机</h2>
<p>我们已经介绍了包括线性回归和softmax回归在内的单层神经网络。然而深度学习主要关注多层模型。在本节中，我们将以多层感知机（multilayer perceptron，MLP）为例，介绍多层神经网络的概念。</p>
<h4 id="引入激活函数的动机">引入激活函数的动机</h4>
<blockquote>
<p>添加更多的隐藏层，以上设计依然只能与仅含输出层的单层神经网络等价</p>
</blockquote>
<p>证明</p>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210325004812564.png" srcset="/img/loading.gif" /></p>
<ol type="1">
<li>输入层到隐藏层的映射 <span class="math inline">\(H = X W _{h}+ b _{h}\)</span></li>
<li>隐藏层到输出层的映射 <span class="math inline">\(O = H W _{o}+ b _{o}\)</span></li>
</ol>
<p>将以上两个式子联立起来，也就是将隐藏层的输出直接作为输出层的输入 <span class="math display">\[
\begin{aligned}
O &amp;=\left( X W _{h}+ b _{h}\right) W _{o}+ b _{o}\\&amp;= X W _{h} W _{o}+ b _{h} W _{o}+ b _{o}
\end{aligned}
\]</span> 从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络</p>
<ul>
<li>单层神经网络权重参数 <span class="math inline">\(W _{h} W _{o}\)</span></li>
<li>单层神经网络偏差参数 <span class="math inline">\(b _{h} W _{o}+ b _{o}\)</span></li>
</ul>
<p>全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换。</p>
<ol type="1">
<li>输入层到隐藏层的映射 $ X W_{h}+b_{h}$ 经过非线性函数进行变换 <span class="math inline">\(H =\phi\left( X W _{h}+ b _{h}\right)\)</span></li>
<li>个非线性函数被称为激活函数（activation function）</li>
</ol>
<h3 id="常用的激活函数">常用的激活函数</h3>
<p>下面我们介绍几个常用的激活函数</p>
<ol type="1">
<li>ReLU（rectified linear unit）函数</li>
<li>sigmoid函数</li>
<li><span class="math inline">\(\tanh\)</span> 函数</li>
</ol>
<h4 id="relurectified-linear-unit函数">1. ReLU（rectified linear unit）函数</h4>
<p><span class="math display">\[
\operatorname{ReLU}(x)=\max (x, 0)
\]</span></p>
<p>可以看出</p>
<ul>
<li>ReLU函数只保留正数元素，并将负数元素清零。</li>
</ul>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210325005526618.png" srcset="/img/loading.gif" /></p>
<p>ReLU函数的导数</p>
<ul>
<li>当输入为负数时，ReLU函数的导数为0；</li>
<li>当输入为正数时，ReLU函数的导数为1</li>
</ul>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210325005749202.png" srcset="/img/loading.gif" /></p>
<h4 id="sigmoid函数">2 sigmoid函数</h4>
<p>sigmoid函数可以将元素的值变换到0和1之间： <span class="math display">\[
\operatorname{sigmoid}(x)=\frac{1}{1+\exp (-x)}
\]</span> <img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210412012438968.png" srcset="/img/loading.gif" /></p>
<p>sigmoid函数的导数 <span class="math display">\[
\operatorname{sigmoid}^{\prime}(x)=\operatorname{sigmoid}(x)(1-\operatorname{sigmoid}(x))
\]</span></p>
<h4 id="tanh-函数">3 <span class="math inline">\(\tanh\)</span> 函数</h4>
<p>tanh（双曲正切）函数可以将元素的值变换到-1和1之间： <span class="math display">\[
\tanh (x)=\frac{1-\exp (-2 x)}{1+\exp (-2 x)}
\]</span> <img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210412012512862.png" srcset="/img/loading.gif" /></p>
<p>tanh函数的导数 <span class="math display">\[
\tanh ^{\prime}(x)=1-\tanh ^{2}(x)
\]</span></p>
<h3 id="多层感知机-1">多层感知机</h3>
<p>多层感知机在输出层与输入层之间加入了一个或多个全连接隐藏层，并通过激活函数对隐藏层输出进行变换 <span class="math display">\[
\begin{aligned}
H &amp;=\phi\left( X W _{h}+ b _{h}\right)  \\
O &amp;= H W _{o}+ b _{o}
\end{aligned}
\]</span> 其中</p>
<ul>
<li><span class="math inline">\(\phi\)</span> 表示激活函数。</li>
<li>在分类问题中，我们可以对输出O做softmax运算, 并使用softmax回归中的 交叉熵损失函数。</li>
<li>在回归问题中，我们将输出层的输出个数设为1，并将输出 <span class="math inline">\(O\)</span> 直接提供给线性回 归中使用的平方损失函数。</li>
</ul>
<h3 id="code">code</h3>
<h2 id="网络训练技巧">网络训练技巧</h2>
<h2 id="遇到的问题">遇到的问题</h2>
<ol type="1">
<li><a href="https://blog.csdn.net/qq_44025027/article/details/105546202" target="_blank" rel="noopener">《动手学深度学习》d2lzh_pytorch （使用Google colab）导入Colab问题</a></li>
</ol>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Machine-learning/">Machine learning</a>
                    
                      <a class="hover-with-bg" href="/categories/Machine-learning/%E4%BB%A3%E7%A0%81/">代码</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Machine-learning/">Machine learning</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2021/03/22/ML_08_code_02_LM/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">放码过来 2 卷积神经网络</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2021/03/13/Ftmb_12_Fradkir_02_entanglement/">
                        <span class="hidden-mobile">量子纠缠</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div id="tocbot"></div>
</div>


      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>







  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 1,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "放码过来&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js" ></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  
















</body>
</html>
