<!DOCTYPE html>
<html lang="en">





<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="JPZhuang">
  <meta name="keywords" content="">
  <title>Python量子机器学习 - JPZ</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
<meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Physics</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner intro-2" id="background" parallax=true
         style="background: url('https://jptanjing.oss-cn-beijing.aliyuncs.com/blog_QuantumAI/QC02_QML02_python.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fade-in-up">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2021-06-06 14:23">
      June 6, 2021 pm
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      4.9k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      97
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <h2 id="前言">前言</h2>
<p>这章涉及到的算法</p>
<ol type="1">
<li>HHL Algorithm</li>
<li>Quantum Swap Test Subroutine</li>
<li>Quantum Euclidean Distance Calculation</li>
<li>Quantum K-Means Clustering</li>
<li>Quantum Principal Component Analysis</li>
<li>Quantum Support Vector Machines</li>
<li>Quantum Least Square SVM</li>
</ol>
<p>[TOC]</p>
<h2 id="参考文献">参考文献</h2>
<ol type="1">
<li><a href="https://github.com/Apress/quantum-machine-learning-python" target="_blank" rel="noopener">Quantum Machine Learning with Python</a> by Santanu Pattanayak</li>
</ol>
<blockquote>
<p>"The distinction between past, present, and future is only a stubbornly persistent illusion." ——Albert Einstein</p>
</blockquote>
<p>In this chapter and the next one, we will explore the exciting areas of quantum machine learning and quantum deep learning. Machine learning and deep learning have seen great success in recent years because of the increase in the computational power at our disposal and because of the high-end research in these fields. Quantum machine learning presents an exciting opportunity to increase the computational efficiency of the existing machine learning algorithms as well as presents a way to tackle some of the more computationally intractable problems.</p>
<h4 id="matrix-inversion-routine">1 matrix inversion routine</h4>
<p>In this chapter, we start with the HarrowHassidim-Lloyd algorithm, popularly known as HHL, which acts as the matrix inversion routine(矩阵求逆程序) in the quantum computing domain. Hence, HHL will be the default choice for algorithms such as</p>
<ul>
<li>linear regression</li>
<li>least square support vector machines.</li>
</ul>
<p>Subsequently, we touch upon quantum linear regression and support vector machines in detail in this chapter.</p>
<h4 id="quantum-routines">2 quantum routines</h4>
<p>We will then move on to implementing quantum routines such as</p>
<ul>
<li>quantum dot product</li>
<li>quantum Euclidean distances</li>
</ul>
<p>since they are integral to several machine learning algorithms such as</p>
<ul>
<li>the k-means clustering<br />
</li>
<li>nearest neighbor methods.</li>
</ul>
<p>In this regard, we will implement the k-means clustering method in detail. Also, we will discuss how Grover's algorithm can be used to optimize quantum objectives by illustrating its usage in the cluster assignment in the k-means algorithm. Principal component analysis is an important machine learning technique, and we will walk through its quantum implementation in great detail.</p>
<h2 id="hhl-algorithm">HHL Algorithm</h2>
<blockquote>
<ul>
<li>解线性方程</li>
<li>矩阵求逆</li>
</ul>
</blockquote>
<p>The Harrow-Hassidim-Lloyd algorithm involves finding a solution to a set of linear equations using a quantum implementation. Finding a solution to a set of linear equations is analogous to solving the matrix inversion problem. Given a matrix <span class="math inline">\(A\)</span> and a vector <span class="math inline">\(b\)</span>, the matrix inversion problem aims to find the vector <span class="math inline">\(x\)</span>. <span class="math display">\[
A x= b
\]</span> Classically, we can solve for <span class="math inline">\(x\)</span> as <span class="math inline">\(A^{-1} b\)</span> given that the inverse of A exists.</p>
<h4 id="计算复杂性">计算复杂性</h4>
<p>However, matrix inversion can be intractable for large matrices. Such inversion problems are hence solved through methods such as Gaussian elimination, which has <span class="math inline">\(O\left(N^{3}\right)\)</span> computational complexity for a matrix of dimension <span class="math inline">\(N \times N\)</span>.</p>
<p>If the matrix A has sparsity <span class="math inline">\(s\)</span></p>
<ul>
<li>where <span class="math inline">\(s\)</span> denotes the proportion of elements in A with 0 values and condition number <span class="math inline">\(\kappa\)</span></li>
<li>where <span class="math inline">\(\kappa\)</span> denotes the ratio of the maximum eigenvalue to the minimum eigenvalue, 最大与最小的比值</li>
</ul>
<p>then algorithms</p>
<ol type="1">
<li>such as conjugate gradient can solve the matrix inversion problem in <span class="math inline">\(O(N s \kappa \log (1 / \epsilon))\)</span> time where epsilon is the desired error bound.</li>
<li>Using HHL, we can achieve a logarithmic reduction in compute by solving the matrix inverse problem in <span class="math inline">\(O\left(\log N s^{2} \kappa^{2} \log \left(\frac{1}{\epsilon}\right)\right)\)</span> time in most cases.</li>
</ol>
<h4 id="与机器学习的联系">与机器学习的联系</h4>
<p>This algorithm is critical for quantum machine learning purposes since several machine learning algorithms <strong>learn their parameter</strong> <span class="math inline">\(\theta\)</span> by solving the matrix inversion problem of the form <span class="math inline">\(A \theta=b\)</span>.</p>
<ol type="1">
<li><span class="math inline">\(A=X^{T} X\)</span> : Generally, the matrix <span class="math inline">\(A\)</span> in such problems is a function of the input features of training data points represented by the <strong>data matrix</strong> <span class="math inline">\(X\)</span>.</li>
<li><span class="math inline">\(b=X^{T} Y\)</span> : The vector <span class="math inline">\(b\)</span> is a function of both data matrix <span class="math inline">\(X\)</span> and the target vector <span class="math inline">\(Y\)</span> for the training data points. For instance, for linear regression where we model output <span class="math inline">\(y =\theta^{T} x\)</span>, finding the <span class="math inline">\(\theta\)</span> boils down to solving the matrix inversion problem given by the following:</li>
</ol>
<p><span class="math display">\[
\left(X^{T} X\right) \theta=X^{T} Y
\]</span> As you can see, <span class="math inline">\(A=X^{T} X\)</span> while <span class="math inline">\(b=X^{T} Y\)</span> for linear regression. We will discuss quantum linear regression in more detail in the subsequent sections.</p>
<h4 id="hermitian">Hermitian</h4>
<blockquote>
<p>矩阵 <span class="math inline">\(A\)</span> 要作为一个量子算符必须是Hermitian的，如果不是，我们可以选择构造成Hermitian的</p>
</blockquote>
<p>In HHL we need to find one or more operators that can transform the state <span class="math inline">\(|b\rangle\)</span> to our solution vector <span class="math inline">\(\theta\)</span>. It is obvious that we would have to factor in <span class="math inline">\(A=X^{T} X\)</span> in one of the operators. We cannot choose <span class="math inline">\(A\)</span> as the quantum operator unless <span class="math inline">\(A\)</span> is unitary. Instead, we can choose <span class="math inline">\(A\)</span> to the Hamiltonian <span class="math inline">\(H\)</span> of the quantum system provided <span class="math inline">\(A\)</span> is Hermitian. Just to refresh your memory, a matrix or linear operator <span class="math inline">\(H\)</span> is Hermitian if it equals its complex conjugate transpose <span class="math inline">\(H^{\dagger}\)</span>. Even if <span class="math inline">\(A\)</span> is not Hermitian, we can define a Hermitian operator <span class="math inline">\(\tilde{A}\)</span> as shown here: <span class="math display">\[
\tilde{A}=\left[\begin{array}{rr}
0 &amp; A^{+} \\
A &amp; 0
\end{array}\right]
\]</span> Now since <span class="math inline">\(\tilde{A}\)</span> is Hermitian, it has an eigenvalue decomposition given by the following: <span class="math display">\[
\tilde{A}=\sum_{i} \lambda_{i}\left|u_{i}\right\rangle\left\langle u_{i}\right|
\]</span> where</p>
<ul>
<li>the eigenvectors <span class="math inline">\(\left|u_{i}\right\rangle\)</span> forms an orthonormal basis.</li>
</ul>
<p>The vector <span class="math inline">\(|b\rangle\)</span> can be represented in the orthonormal basis <span class="math inline">\(\left|u_{i}\right\rangle\)</span> as shown here: <span class="math display">\[
|b\rangle=\sum_{i} \beta_{i}\left|u_{i}\right\rangle
\]</span> The solution to the inverse problem is then given by the following: <span class="math display">\[
|x\rangle=\tilde{A}^{-1}|b\rangle
\]</span> Since <span class="math inline">\(\tilde{A}\)</span> is a Hermitian matrix with spectral decomposition <span class="math inline">\(\tilde{A}=\sum_{i} \lambda_{i}\left|u_{i}\right\rangle\left\langle u_{i}\right|\)</span>, its inverse is given by the following: <span class="math display">\[
\tilde{A}^{-1}=\sum_{i} \frac{1}{\lambda_{i}}\left|u_{i}\right\rangle\left\langle u_{i}\right|
\]</span> Substituting the value of <span class="math inline">\(\tilde{A}^{-1}\)</span> and <span class="math inline">\(|b\rangle\)</span>, we get the solution <span class="math inline">\(|x\rangle\)</span>, as shown here: <span class="math display">\[
\begin{align}
|x\rangle&amp;=\tilde{A}^{-1}|b\rangle \\
&amp; =\sum_{i} \frac{1}{\lambda_{i}}\left|u_{i}\right\rangle \langle u_{i} |\sum_{j} \beta_{j} | u_{j} \rangle \\
&amp;=\sum_{j} \frac{\beta_{j}}{\lambda_{j}}\left|u_{j}\right\rangle
\end{align}
\]</span> #### 相位估计</p>
<p>We can see that if we could go from the eigenstates <span class="math inline">\(\left|u_{i}\right\rangle\)</span> to <span class="math inline">\(\frac{1}{\lambda_{i}}\left|u_{i}\right\rangle\)</span>, we would be closer to the solution. One way to achieve this is to perform <strong>quantum phase estimation</strong> using the unitary operator <span class="math inline">\(U=e^{-i \tilde{A} t}\)</span> on the state <span class="math inline">\(|b\rangle\)</span> expressed as the superposition state of the basis states <span class="math inline">\(\left|u_{i}\right\rangle\)</span> since it would take the eigenstates <span class="math inline">\(\left|u_{i}\right\rangle\)</span> to <span class="math inline">\(\lambda_{i}\left|u_{i}\right\rangle\)</span> Finally, through controlled rotation, we can invert the eigenvalues to take the eigenstates from <span class="math inline">\(\lambda_{i}\left|u_{i}\right\rangle\)</span> to <span class="math inline">\(\frac{1}{\lambda_{i}}\left|u_{i}\right\rangle .\)</span> Please do note that the state <span class="math inline">\(|b\rangle\)</span> needs to be of unit norm before quantum phase estimate can be applied on state <span class="math inline">\(|b\rangle\)</span> Although we now have a high-level understanding of the HHL algorithm, we need to go over each of the steps in a little more detail for an end-to-end implementation. The following are the steps of the HHL algorithm.</p>
<h4 id="initializing-the-registers">Initializing the Registers</h4>
<p>We start HHL with three registers.</p>
<ul>
<li>The ancilla register of one qubit initialized at <span class="math inline">\(|0\rangle_{\text {ANC }}\)</span>.</li>
<li>The work register to hold the eigenvalues from quantum phase estimation. The number of qubits for the work registers depends on the level of accuracy to which we want to measure the eigenvalues. The register starts at the initialized <span class="math inline">\(|0\rangle_{W}\)</span> state.</li>
<li>The final register holds the value of the state <span class="math inline">\(|b\rangle .\)</span> As discussed, for quantum phase estimation to work, <span class="math inline">\(|b\rangle\)</span> should be of unit norm, and hence we load the final register with the following:</li>
</ul>
<p><span class="math display">\[
|\tilde{b}\rangle=\frac{|b\rangle}{\langle b \mid b\rangle^{\frac{1}{2}}}=\frac{1}{\langle b \mid b\rangle^{\frac{1}{2}}} \sum_{j} b_{j}\left|u_{j}\right\rangle=\sum_{j} \tilde{b}_{j}\left|u_{j}\right\rangle
\]</span></p>
<p>Where the normalized coefficient <span class="math inline">\(\tilde{b}_{j}=\frac{b_{j}}{\langle b \mid b\rangle^{\frac{1}{2}}}\)</span></p>
<p>So, the initial state of the three registers is given by the following: <span class="math display">\[
|\psi\rangle_{0}=|0\rangle_{A N C} \otimes|0\rangle_{W} \otimes|\tilde{b}\rangle=|0\rangle_{A N C} \otimes \sum_{j} \tilde{b}_{j}|0\rangle_{W} \otimes\left|u_{j}\right\rangle
\]</span></p>
<h3 id="performing-quantum-phase-estimation">Performing Quantum Phase Estimation</h3>
<p>Apply quantum phase estimate using the unitary operator <span class="math inline">\(e^{-i \lambda t}\)</span>. Since <span class="math inline">\(\tilde{A}\)</span> has a spectral decomposition given by <span class="math inline">\(\tilde{A}=\sum_{i} \lambda_{i}\left|u_{i}\right\rangle\left\langle u_{i}\right|\)</span>, the spectral decomposition of <span class="math inline">\(e^{i A t}\)</span> is given by the following: <span class="math display">\[
e^{-i \lambda t}=\sum_{j} e^{-i \lambda_{j} t}\left|u_{j}\right\rangle\left\langle u_{j}\right|
\]</span> In Equation, <span class="math inline">\(e^{-i \tilde{u}_{j} t}\)</span> is the eigenvalue corresponding to the eigenvector <span class="math inline">\(\left|u_{j}\right\rangle\)</span> of the operator <span class="math inline">\(e^{-i A t} .\)</span> The eigenvalues can be written as follows: <span class="math display">\[
e^{-i \lambda t}=\sum_{j} e^{-i \lambda_{j} t}\left|u_{j}\right\rangle\left\langle u_{j}\right|
\]</span> So, on performing quantum estimation using <span class="math inline">\(e^{-i \tilde{A} t}\)</span> on <span class="math inline">\(|\tilde{b}\rangle\)</span>, we would get phases <span class="math inline">\(\tilde{\lambda}_{j}=\frac{\lambda_{j} t}{2 \pi}\)</span> in the work register.</p>
<p>So, the overall state of the system after quantum phase estimation is given by the following: <span class="math display">\[
\begin{align}
|\psi\rangle_{1}&amp;=|0\rangle_{A N C} \otimes \sum_{j} \tilde{b}_{j}\left|\tilde{\lambda}_{j}\right\rangle_{W} \otimes\left|u_{j}\right\rangle\\&amp;=\sum_{j}|0\rangle_{A N C} \otimes \tilde{b}_{j}\left|\tilde{\lambda}_{j}\right\rangle_{W} \otimes\left|u_{j}\right\rangle
\end{align}
\]</span></p>
<h3 id="inverting-the-eigenvalues">Inverting the Eigenvalues</h3>
<p>We need to invert the normalized eigenvalues <span class="math inline">\(\tilde{\lambda}_{j}\)</span>. This can be done by rotating the ancilla qubit state around the <span class="math inline">\(y\)</span> -axis conditioned on the states <span class="math inline">\(\left|\tilde{\lambda}_{j}\right\rangle\)</span>. The angle of rotation <span class="math inline">\(\theta_{j}\)</span> and the rotational operator pertaining to each of the eigenvectors <span class="math inline">\(\left|u_{j}\right\rangle\)</span> are given by the following: <span class="math display">\[
\theta_{j}=2 \sin ^{-1} \frac{C}{\lambda_{j}}
\]</span> Thus, the rotational operator around the <span class="math inline">\(y\)</span> -axis for each angle <span class="math inline">\(\theta_{j}\)</span> can be expressed as follows: <span class="math display">\[
R_{y}\left(\theta_{j}\right)=e^{-\frac{i Y \theta_{J}}{2}}
\]</span> Since the Pauli matrix <span class="math inline">\(Y=\left[\begin{array}{cc}0 &amp; -i \\ i &amp; 0\end{array}\right]\)</span> is idempotent, i.e., satisfies the relation <span class="math inline">\(Y^{2}=I\)</span>, <span class="math inline">\(R_{y}\left(\theta_{j}\right)\)</span> can be written also as follows: <span class="math display">\[
R_{y}\left(\theta_{j}\right)=\operatorname{Icos}\left(\frac{\theta_{j}}{2}\right)-i Y \sin \left(\frac{\theta_{j}}{2}\right)=\left[\begin{array}{cc}
\cos \left(\frac{\theta_{j}}{2}\right) &amp; -\sin \left(\frac{\theta_{j}}{2}\right) \\
\sin \left(\frac{\theta_{j}}{2}\right) &amp; \cos \left(\frac{\theta_{j}}{2}\right)
\end{array}\right]
\]</span> The rotation matrix would change the state of the ancillary bit at <span class="math inline">\(|0\rangle_{\text {ANC }}\)</span> to the following: <span class="math display">\[
\begin{aligned}
R_{y}\left(\theta_{j}\right)|0\rangle_{A N C}&amp;=\left[\begin{array}{ll}
\cos \left(\frac{\theta_{j}}{2}\right) &amp; -\sin \left(\frac{\theta_{j}}{2}\right) \\
\sin \left(\frac{\theta_{j}}{2}\right) &amp; \cos \left(\frac{\theta_{j}}{2}\right)
\end{array}\right]\left[\begin{array}{l}
1 \\
0
\end{array}\right]\\
&amp;=\left[\begin{array}{c}
\cos \left(\frac{\theta_{j}}{2}\right) \\
\sin \left(\frac{\theta_{j}}{2}\right)
\end{array}\right] \\
&amp;=\cos \left(\frac{\theta_{j}}{2}\right)|0\rangle_{A N C}+\sin \left(\frac{\theta_{j}}{2}\right)|1\rangle_{A N C}
\end{aligned}
\]</span> From Equation, we have <span class="math inline">\(\theta_{j}=2 \sin ^{-1} \frac{C}{\tilde{\lambda}_{j}}\)</span>, which makes <span class="math inline">\(\sin \left(\frac{\theta_{j}}{2}\right)=\frac{C}{\tilde{\lambda}_{j}}\)</span> and <span class="math inline">\(\cos \left(\frac{\theta_{j}}{2}\right)=\sqrt{1-\frac{C^{2}}{\tilde{\lambda}_{j}^{2}}}\)</span>. Hence, Equation simplifies to the following: <span class="math display">\[
R_{y}\left(\theta_{j}\right)|0\rangle_{A N C}=\sqrt{1-\frac{C^{2}}{\tilde{\lambda}_{j}^{2}}}|0\rangle_{A N C}+\frac{C}{\tilde{\lambda}_{j}}|1\rangle_{A N C}
\]</span> So, the combined state <span class="math inline">\(\left|\psi_{3}\right\rangle\)</span> of the three registers after the ancilla qubit rotation is given by the following: <span class="math display">\[
\begin{align}
|\psi_{3}\rangle &amp;=\sum_{j}|0\rangle_{A N C} \otimes \tilde{b}_{j}\left|\tilde{\lambda}_{j}\right\rangle_{W} \otimes\left|u_{j}\right\rangle\\&amp;
=\sum_{j}\left(\sqrt{1-\frac{C}{\tilde{\lambda}_{j}}}|0\rangle_{A N C}+\frac{C}{\tilde{\lambda}_{j}}|1\rangle_{ ANC }\right) \otimes \tilde{b}_{j}\left|\tilde{\lambda}_{j}\right\rangle_{W} \otimes\left|u_{j}\right\rangle
\end{align}
\]</span></p>
<h3 id="uncomputing-the-work-registers">Uncomputing the Work Registers</h3>
<p>Once we have done the conditional rotation based on the eigenvalue states in the work register, we do not really need them. We can apply the inverse of the quantum phase estimation transform to what we have applied earlier to uncompute the work register. Essentially, this uncompute step would change the state of the work register to <span class="math inline">\(|0\rangle_{W}\)</span> for every <span class="math inline">\(\left|\tilde{\lambda}_{j}\right\rangle_{W}\)</span> state.</p>
<p>So, the state of the three registers after the uncompute step is given by the following: <span class="math display">\[
\begin{aligned}
\left|\psi_{4}\right\rangle &amp;=\sum_{j}\left(\sqrt{1-\frac{C^{2}}{\tilde{\lambda}_{j}^{2}}}|0\rangle_{A N C}+\frac{C}{\tilde{\lambda}_{j}}|1\rangle_{A N C}\right) \otimes \tilde{b}_{j}|0\rangle_{W} \otimes\left|u_{j}\right\rangle \\
&amp;=|0\rangle_{W} \otimes \sum_{j}\left(\sqrt{1-\frac{C}{\tilde{\lambda}_{j}}}|0\rangle_{A N C}+\frac{C}{\tilde{\lambda}_{j}}|0\rangle_{A N C}\right) \otimes \tilde{b}_{j}\left|u_{j}\right\rangle
\end{aligned}
\]</span> Now that the work register has been reset to the <span class="math inline">\(|0\rangle_{ w }\)</span> state, we can ignore the work register since it no longer would be entangled unfavorably with the qubit states that matter. Hence, we can concentrate on the combined state of the ancilla qubit and inputoutput register qubits, which is given by the following: <span class="math display">\[
\begin{aligned}
\left|\psi_{5}\right\rangle &amp;=\sum_{j}\left(\sqrt{1-\frac{C^{2}}{\tilde{\lambda}_{j}^{2}}}|0\rangle_{A N C}+\frac{C}{\tilde{\lambda}_{j}}|1\rangle_{A N C}\right) \otimes \tilde{b}_{j}\left|u_{j}\right\rangle \\
&amp;=\sum_{j}\left(\tilde{b}_{j} \sqrt{1-\frac{C^{2}}{\tilde{\lambda}_{j}^{2}}}|0\rangle_{A N C} \otimes\left| u _{ j }\right\rangle+ C \frac{\tilde{b}_{j}}{\tilde{\lambda}_{j}}|1\rangle_{A N C} \otimes\left|u_{j}\right\rangle\right.
\end{aligned}
\]</span></p>
<h3 id="measuring-the-ancilla-oubit">Measuring the Ancilla Oubit</h3>
<p>In the final step, we measure the ancilla qubit. When the ancilla qubit measures the state <span class="math inline">\(|1\rangle\)</span>, the post-measurement input-output register state is given by the following: <span class="math display">\[
\left|\psi_{6}\right\rangle=C \sum_{j} \frac{\tilde{b}_{j}}{\tilde{\lambda}_{j}}\left|u_{j}\right\rangle
\]</span> Now <span class="math inline">\(\tilde{b}_{j}=\frac{b_{j}}{\langle b \mid b\rangle^{1 / 2}}\)</span> and <span class="math inline">\(\tilde{\lambda}_{j}=\frac{\lambda_{j} t}{2 \pi}\)</span>, and hence we can rewrite Equation 5-22 as follows: <span class="math display">\[
\left|\psi_{6}\right\rangle=C \times \frac{2 \pi}{t\langle b \mid b\rangle^{1 / 2}} \sum_{j} \frac{b_{j}}{\lambda_{j}}\left|u_{j}\right\rangle
\]</span> The state <span class="math inline">\(\left|\psi_{6}\right\rangle\)</span> is nothing but the solution state <span class="math inline">\(|x\rangle=\sum_{j} \frac{b_{j}}{\lambda_{j}}\left|u_{j}\right\rangle\)</span> up to some proportionality factor given by <span class="math inline">\(C \times \frac{2 \pi}{t\langle b \mid b\rangle^{1 / 2}}\)</span>. The proportionality constant <span class="math inline">\(C\)</span> and <span class="math inline">\(t\)</span> can be chosen appropriately to reduce the factor <span class="math inline">\(C \times \frac{2 \pi}{t\langle b \mid b\rangle^{1 / 2}}\)</span> to 1 .</p>
<h3 id="cirq代码">cirq代码</h3>
<p>例子 <span class="math display">\[
\begin{align}
A x &amp;= b \\
\left[\begin{array}{ll}
4.30213466   &amp; 0.23531802+i0.934386156   \\
0.23531802-i0.934386156 &amp; 0.58386534
\end{array}\right] \left[\begin{array}{l}
x_{1} \\
x_{2} 
\end{array}\right] &amp; =\left[\begin{array}{l}
1.276359 \\
1.276359
\end{array}\right]
\end{align}
\]</span></p>
<h2 id="quantum-linear-regression">Quantum Linear Regression</h2>
<p>In any regression problem, we try to predict the continuous value of a variable <span class="math inline">\(y_{i} \in R\)</span> given a set of input features <span class="math inline">\(x^{(1)}, x^{(2)} \ldots x^{(N)}\)</span> that can be represented as an <span class="math inline">\(N\)</span> -dimensional input vector <span class="math inline">\(x \in R ^{N} .\)</span> In linear regression, we consider the output to be a linear combination of the input features with some irreducible error component <span class="math inline">\(e_{i}\)</span>, as follows: <span class="math display">\[
\begin{aligned}
y_{i}&amp;=\theta_{1} x^{(1)}+\theta_{2} x^{(2)}+\ldots \theta_{N} x^{(N)}+b+e_{i} \\
&amp;=\sum_{i=1}^{N} \theta_{i} x^{(i)}+b+e_{i}
\end{aligned}
\]</span> The <span class="math inline">\(\theta_{i}\)</span> corresponding to each feature and the intercept <span class="math inline">\(b\)</span> are the parameters to the model that we want to learn. If we represent the parameters <span class="math inline">\(\theta_{i} ; i \in\{1,2, . . N\}\)</span> by the vector <span class="math inline">\(\theta \in R ^{N}\)</span>, then we can simplify the linear relationship in Equation <span class="math inline">\(5-24\)</span> as follows: <span class="math display">\[
y_{i} / x_{i}=\theta^{T} x_{i}+b+e_{i}
\]</span> The expression <span class="math inline">\(y_{i} / x_{i}\)</span> stands for the value of <span class="math inline">\(y_{i}\)</span> conditioned on <span class="math inline">\(x_{i}\)</span>. Now <span class="math inline">\(e_{i}\)</span> is the irreducible component that shares zero correlation with the input features and hence is not learnable. We can, however, given <span class="math inline">\(x_{i}\)</span>, completely determine the term <span class="math inline">\(\theta^{T} x_{i}+b\)</span>. The error <span class="math inline">\(e_{i}\)</span> is assumed to follow a normal distribution with zero mean and finite standard deviation <span class="math inline">\(\sigma\)</span>, and hence we can write the following: <span class="math display">\[
e_{i} \sim N\left(0, \sigma^{2}\right)
\]</span> The term <span class="math inline">\(\theta^{T} x_{i}+b\)</span> is constant given the value of feature vector <span class="math inline">\(x_{i}\)</span>, and we can say this: <span class="math display">\[
\begin{aligned}
\theta^{T} x_{i}+b+e_{i} \sim N\left(\theta^{T} x_{i}+b, \sigma^{2}\right) \\
\rightarrow y_{i} / x_{i} \sim N\left(\theta^{T} x_{i}+b, \sigma^{2}\right)
\end{aligned}
\]</span> So, the target label <span class="math inline">\(y_{i}\)</span> given the input feature follows a normal distribution with mean <span class="math inline">\(\theta^{T} x_{i}+b\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. In linear regression, we take the conditional mean of the distribution as our prediction, as shown here: <span class="math display">\[
\hat{y}_{i}= E \left[y_{i} / x_{i}\right]=\theta^{T} x_{i}+b
\]</span> The parameters of the model, <span class="math inline">\(\theta\)</span> and <span class="math inline">\(b\)</span>, can be determined by minimizing the sum of the square of the error term <span class="math inline">\(e_{i}\)</span> for each data point. For the ease of notation, we can consume the bias term <span class="math inline">\(b\)</span> as a parameter within the <span class="math inline">\(\theta\)</span> parameter vector corresponding to the constant feature of 1 . This makes the prediction <span class="math inline">\(\hat{y}_{i}=\theta^{T} x_{i}\)</span> where both <span class="math inline">\(\theta\)</span> and <span class="math inline">\(x_{i}\)</span> are <span class="math inline">\(N+1\)</span> dimensional vectors. With this simplification, the system of equations for the <span class="math inline">\(M\)</span> data points can be written in matrix notation, as shown here: <span class="math display">\[
\left[\begin{array}{c}
x_{1}^{T} \rightarrow \\
x_{2}^{T} \rightarrow \\
\cdot \cdot \\
x_{i}^{T} \rightarrow \\
. . \\
x_{M \rightarrow}^{T}
\end{array}\right] \theta=\left[\begin{array}{c}
\hat{y}_{1} \\
\hat{y}_{2} \\
. \cdot \\
\hat{y}_{i} \\
. . \\
\hat{y}_{M}
\end{array}\right]
\]</span> If we represent the matrix with the input feature vectors in Equation <span class="math inline">\(5-29\)</span> as <span class="math inline">\(X \in R ^{M \times(N+1)}\)</span> and the prediction vector as <span class="math inline">\(\hat{Y}_{i} \in R ^{M}\)</span>, then <span class="math inline">\(5-29\)</span> can be written as follows: <span class="math display">\[
X \theta=\hat{Y}
\]</span> Now if we let the actual targets <span class="math inline">\(y_{i}\)</span> for all the <span class="math inline">\(M\)</span> data points be represented by vector <span class="math inline">\(Y \in E ^{M}\)</span>, then we have the error vector <span class="math inline">\(e \in R ^{M}\)</span> as follows: <span class="math display">\[
e=Y-\hat{Y}=Y-X \theta
\]</span> The loss objective can be written as the mean of the squared errors in prediction for each data point. <span class="math display">\[
L(\theta)=\frac{1}{M} \sum_{i=1}^{M} e_{i}^{2}
\]</span> The previous loss is nothing but the average of the dot product of the error vector <span class="math inline">\(e \in R ^{M}\)</span> with itself. This allows us to write the loss completely in matrix notations, as shown here: <span class="math display">\[
\begin{aligned}
L(\theta)&amp;=\frac{1}{M} \sum_{i=1}^{M} e_{i}^{2}\\&amp;=\frac{1}{M} e ^{T} e \\
&amp;=\frac{1}{M}(Y-X \theta)^{T}(Y-X \theta)
\end{aligned}
\]</span> To determine the parameter <span class="math inline">\(\theta\)</span>, we need to minimize the loss <span class="math inline">\(L(\theta)\)</span> with respect to <span class="math inline">\(\theta\)</span>. To determine the minima, we can take the gradient of the loss <span class="math inline">\(L(\theta)\)</span> with respect to <span class="math inline">\(\theta\)</span> and set it to zero vector as shown here: <span class="math display">\[
\begin{aligned}
\nabla_{\theta} &amp;=-\frac{2}{M} X^{T}(Y-X \theta)=0 \\
&amp; \rightarrow\left(X^{T} X\right) \theta=X^{T} Y
\end{aligned}
\]</span> The matrix <span class="math inline">\(\left(X^{T} X\right)\)</span> is Hermitian in nature and hence can be treated as Hamiltonian for a quantum system. We can solve the matrix inversion problem in Equation <span class="math inline">\(5-34\)</span> to find the model parameter <span class="math inline">\(\theta\)</span> by using the HHL algorithm that we discussed earlier.</p>
<h2 id="quantum-swap-test-subroutine">Quantum Swap Test Subroutine</h2>
<p>The quantum swap test is an effective subroutine that computes the dot product of two quantum states in terms of the probability of measuring an ancilla qubit in state <span class="math inline">\(|0\rangle\)</span>. Since computing the dot product is an essential requirement in all machine learning algorithms, the swap test subroutine will be central in the implementation of their quantum machine learning counterparts. We take two unit-norm vectors <span class="math inline">\(|a\rangle\)</span> and <span class="math inline">\(|b\rangle\)</span> and illustrate how we can use the circuit in Figure <span class="math inline">\(5-1\)</span> to compute the dot product between them.</p>
<p><img src="https://jptanjing.oss-cn-beijing.aliyuncs.com/img/image-20210605171356357.png" srcset="/img/loading.gif" style="zoom:50%;" /></p>
<blockquote>
<p>Figure 5-1. Swap test to compute dot product</p>
</blockquote>
<p>The circuit also has an ancilla qubit initialized at state <span class="math inline">\(|0\rangle .\)</span> The state vectors <span class="math inline">\(|a\rangle\)</span> and <span class="math inline">\(|b\rangle\)</span> can be represented by <span class="math inline">\(\log _{2} n\)</span> qubits where <span class="math inline">\(n\)</span> is the dimension of these state vectors.</p>
<p>We will look at the combined state of the qubits at each stage of the swap test subroutine to understand the series of transformations involved in computing the dot product.</p>
<h4 id="initial-state">Initial State</h4>
<p>The initial state of the system is given by the following: <span class="math display">\[
\left|\psi_{o}\right\rangle=|0\rangle \otimes|a\rangle \otimes|b\rangle
\]</span> #### Hadamard Gate on the Ancilla Qubit</p>
<p>After the application of the Hadamard gate on the ancilla qubit, the combined state of the system changes to the following: <span class="math display">\[
\left|\psi_{1}\right\rangle=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle) \otimes|a\rangle \otimes|b\rangle
\]</span> #### Controlled Swap Operation</p>
<p>In this step, the two state vectors are swapped conditioned on the ancilla qubit. If the ancilla qubit is in state <span class="math inline">\(|0\rangle\)</span>, the states <span class="math inline">\(|a\rangle\)</span> and <span class="math inline">\(|b\rangle\)</span> are left unchanged, while if the ancilla qubit is in state <span class="math inline">\(|1\rangle\)</span>, then the two states are swapped. Hence, the combined state of the system <span class="math inline">\(\left|\psi_{2}\right\rangle\)</span> after the controlled SWAP operation is as follows: <span class="math display">\[
\left|\psi_{2}\right\rangle=\frac{1}{\sqrt{2}}(|0\rangle \otimes|a\rangle \otimes|b\rangle+|1\rangle \otimes|b\rangle \otimes|a\rangle)
\]</span> #### Hadamard Gate on the Control Qubit</p>
<p>The Hadamard gate on the control qubit after the Controlled SWAP Operation changes the combined state to <span class="math inline">\(\left|\psi_{3}\right\rangle\)</span>, as shown here: <span class="math display">\[
\begin{aligned}
\left|\psi_{3}\right\rangle &amp;=\frac{1}{2}(|0\rangle+|1\rangle) \otimes|a\rangle \otimes|b\rangle+\frac{1}{2}(|0\rangle-|1\rangle) \otimes|b\rangle \otimes|a\rangle \\
&amp;=\frac{1}{2}|0\rangle(|a\rangle \otimes|b\rangle+|b\rangle \otimes|a\rangle)+\frac{1}{2}|1\rangle(|a\rangle \otimes|b\rangle-|b\rangle \otimes|a\rangle)
\end{aligned}
\]</span> In the state <span class="math inline">\(\left|\psi_{3}\right\rangle\)</span>, the probability of the ancilla qubit in the state <span class="math inline">\(|0\rangle\)</span> is given by the square of the <span class="math inline">\(l^{2}\)</span> norm of the state <span class="math inline">\(\left|\phi_{0}\right\rangle=\frac{1}{2}(|a\rangle \otimes|b\rangle+|b\rangle \otimes|a\rangle)\)</span> attached to it. <span class="math display">\[
\begin{gathered}
P(|0\rangle)=\left\langle\phi_{0} \mid \phi_{0}\right\rangle \\
=\frac{1}{4}(\langle b|\otimes\langle a|+\langle a|\otimes\langle b|)(|a\rangle \otimes|b\rangle+|b\rangle \otimes|a\rangle) \\
=\frac{1}{4}(\langle b|\langle a \mid a\rangle| b\rangle+\langle b|\langle a \mid b\rangle| a\rangle+\langle a|\langle b \mid a\rangle| b\rangle+\langle a|\langle b \mid b\rangle| a\rangle) \\
=\frac{1}{4}\left(1+2\langle a \mid b\rangle^{2}+1\right)=\frac{1}{2}+\frac{1}{2}\langle a \mid b\rangle^{2}
\end{gathered}
\]</span> If we measure the ancilla qubit to be in the state <span class="math inline">\(|0\rangle\)</span> with probability <span class="math inline">\(0.5\)</span>, then the states <span class="math inline">\(|a\rangle\)</span> and <span class="math inline">\(|b\rangle\)</span> are mutually orthogonal to each other since their dot product <span class="math inline">\(\langle a \mid b\rangle\)</span> in this case turns out to be 0 as per Equation 5-39. Similarly, when the states <span class="math inline">\(|a\rangle\)</span> and <span class="math inline">\(|b\rangle\)</span> are the same, the dot product <span class="math inline">\(\langle a \mid b\rangle=1\)</span> and the probability of the ancilla qubit in state <span class="math inline">\(|0\rangle\)</span> turns out to be 1 . The good thing about the swap test approach of computing the dot product over classical methods is the time complexity does not scale with the number of qubits required to represent each state.</p>
<h3 id="swap-test-implementation">Swap Test Implementation</h3>
<h2 id="quantum-euclidean-distance-calculation">Quantum Euclidean Distance Calculation</h2>
<p>Much like the dot product, the Euclidean distance is a core component of several machine learning algorithms such as k-means clustering and <span class="math inline">\(K\)</span> nearest neighbors. Classical data represented by vector <span class="math inline">\(\vec{a}\)</span> is generally encoded as a quantum state by unit vector <span class="math inline">\(|a\rangle\)</span>, as shown here: <span class="math display">\[
|a\rangle=\frac{\vec{a}}{\|\vec{a}\|}=\|\vec{a}\|^{-1} \sum_{i=0}^{N-1} a_{i}|i\rangle
\]</span> In machine learning, we are interested in finding out the Euclidean distance between vectors that are not unit vectors in general. Let's try to compute the Euclidean distance between two general vectors represented by <span class="math inline">\(\vec{a}\)</span> and <span class="math inline">\(\vec{b}\)</span> whose <span class="math inline">\(l^{2}\)</span> norms are not necessarily <span class="math inline">\(1 .\)</span> As it turns out, we can use the swap test intelligently to compute the Euclidean distance between <span class="math inline">\(\vec{a}\)</span> and <span class="math inline">\(\vec{b}\)</span>.</p>
<p>We create the two quantum states <span class="math inline">\(|a\rangle\)</span> and <span class="math inline">\(|b\rangle\)</span> by normalizing <span class="math inline">\(\vec{a}\)</span> and <span class="math inline">\(\vec{b}\)</span>, as illustrated in Equation <span class="math inline">\(5-40\)</span>. Now using <span class="math inline">\(|a\rangle\)</span> and <span class="math inline">\(|b\rangle\)</span> and another qubit, we can create two states <span class="math inline">\(|\psi\rangle\)</span> and <span class="math inline">\(|\phi\rangle\)</span> as shown here: <span class="math display">\[
\begin{array}{r}
|\psi\rangle=\frac{1}{\sqrt{2}}(|0\rangle \otimes|a\rangle+|1\rangle \otimes|b\rangle) \\
|\phi\rangle=\frac{1}{\sqrt{Z}}(\| \vec{a}|||0\rangle-\|\vec{b}\||1\rangle)
\end{array}
\]</span> In Equation <span class="math inline">\(5-41, Z\)</span> is the sum of the square of the <span class="math inline">\(l^{2}\)</span> norm of <span class="math inline">\(\vec{a}\)</span> and <span class="math inline">\(\vec{b}\)</span>. In other words, <span class="math inline">\(Z=\|\vec{a}\|^{2}+\|\vec{b}\|^{2}\)</span>. Performing a swap test with <span class="math inline">\(|\psi\rangle\)</span> and <span class="math inline">\(|\phi\rangle\)</span>, we will get the dot product <span class="math inline">\(\langle\psi \mid \phi\rangle\)</span> from Equation <span class="math inline">\(5-39\)</span> in terms of the probability of measuring the ancilla qubit in state <span class="math inline">\(|0\rangle\)</span>, as shown here: <span class="math display">\[
P(|0\rangle)=\frac{1}{2}+\frac{1}{2}\langle\psi \mid \phi\rangle^{2}
\]</span> Now the dot product <span class="math inline">\(\langle\psi \mid \phi\rangle\)</span> can be simplified as follows: <span class="math display">\[
\begin{array}{c}
\langle\psi \mid \phi\rangle=\frac{1}{\sqrt{2}}\left(\langle a|\langle 0|+\left\langle b|\langle 1|) \frac{1}{\sqrt{Z}}(\|\vec{a}|\| 0\rangle-\| \vec{b} \||1\rangle)\right.\right. \\
=\frac{1}{\sqrt{2 Z}}(\|\vec{a}\| \mid\langle a|\langle 0 \mid 0\rangle-\|\vec{b}\|\langle a|\langle 0 \mid 1\rangle+\|\vec{a}\|\langle b|\langle 1 \mid 0\rangle-\|\vec{b}|\| b\rangle\langle 1 \mid 1\rangle \\
=\frac{1}{\sqrt{2 Z}}(\|\vec{a}\|\langle a|-\|\vec{b}\|\langle b|)=\frac{1}{\sqrt{2 Z}}(\vec{a}-\vec{b})^{T}
\end{array}
\]</span> Substituting the expression for <span class="math inline">\(\langle\psi \mid \phi\rangle\)</span> in Equation <span class="math inline">\(5-42\)</span>, we get the following: <span class="math display">\[
\begin{array}{c}
P(|0\rangle)=\frac{1}{2}+\frac{1}{2}\left\|\frac{1}{\sqrt{2} Z}(\vec{a}-\vec{b})\right\|^{2} \\
=\frac{1}{2}+\frac{1}{4} \frac{1}{Z}\|(\vec{a}-\vec{b})\|^{2}
\end{array}
\]</span></p>
<h2 id="quantum-k-means-clustering">Quantum K-Means Clustering</h2>
<h2 id="quantum-principal-component-analysis">Quantum Principal Component Analysis</h2>
<h2 id="quantum-support-vector-machines">Quantum Support Vector Machines</h2>
<h2 id="quantum-least-square-svm">Quantum Least Square SVM</h2>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Quantum-Computation/">Quantum Computation</a>
                    
                      <a class="hover-with-bg" href="/categories/Quantum-Computation/%E9%87%8F%E5%AD%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">量子机器学习</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/AI/">AI</a>
                    
                      <a class="hover-with-bg" href="/tags/Quantum-Computation/">Quantum Computation</a>
                    
                      <a class="hover-with-bg" href="/tags/%E9%87%8F%E5%AD%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">量子机器学习</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2021/06/04/QC06_Qprogramming_IBM01/">
                        <span class="hidden-mobile">Qiskit - IBM量子计算实践</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div id="tocbot"></div>
</div>


      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>







  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 1,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "Python量子机器学习&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js" ></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  
















</body>
</html>
